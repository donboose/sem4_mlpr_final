{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7294e37c-aae2-4365-80d0-358276483f89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T03:38:11.840398Z",
     "iopub.status.busy": "2025-05-14T03:38:11.839917Z",
     "iopub.status.idle": "2025-05-14T03:38:11.866913Z",
     "shell.execute_reply": "2025-05-14T03:38:11.866521Z",
     "shell.execute_reply.started": "2025-05-14T03:38:11.840230Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import glob\n",
    "import gc\n",
    "import h5py\n",
    "\n",
    "base_dir = \"../storage/transform/edaicwoz_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f4649e-a005-4512-8b04-0e62eea4c115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:54:36.797407Z",
     "iopub.status.busy": "2025-05-13T19:54:36.797063Z",
     "iopub.status.idle": "2025-05-13T19:54:36.800896Z",
     "shell.execute_reply": "2025-05-13T19:54:36.800542Z",
     "shell.execute_reply.started": "2025-05-13T19:54:36.797388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, h5_path):\n",
    "        h5_path = base_dir + h5_path\n",
    "        self.h5_path = h5_path\n",
    "        with h5py.File(h5_path, 'r', libver='latest', swmr=True) as f:\n",
    "            self.participants = list(f.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.participants)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_path, 'r', libver='latest', swmr=True) as f:\n",
    "            grp = f[self.participants[idx]]\n",
    "            features = {k: torch.from_numpy(grp[k][:]) for k in grp.keys() if k != 'info'}\n",
    "            label = torch.tensor(grp['info'][1][()])\n",
    "            return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb62910d-dae1-4efe-98ff-eb352bce99e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:54:37.028678Z",
     "iopub.status.busy": "2025-05-13T19:54:37.028489Z",
     "iopub.status.idle": "2025-05-13T19:54:37.032019Z",
     "shell.execute_reply": "2025-05-13T19:54:37.031583Z",
     "shell.execute_reply.started": "2025-05-13T19:54:37.028663Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "    \n",
    "    padded_batch = {}\n",
    "    for modality in features[0].keys():\n",
    "        sequences = [item[modality] for item in features]\n",
    "        padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "        padded_batch[modality] = padded\n",
    "        \n",
    "        masks = torch.ones_like(padded[:, :, 0])\n",
    "        for i, seq in enumerate(sequences):\n",
    "            masks[i, len(seq):] = 0\n",
    "        padded_batch[f'{modality}_mask'] = masks.bool()\n",
    "    \n",
    "    return padded_batch, torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36aeefa-289f-4e8f-b2f9-4ce42e964b5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:54:37.213081Z",
     "iopub.status.busy": "2025-05-13T19:54:37.212899Z",
     "iopub.status.idle": "2025-05-13T19:54:37.216841Z",
     "shell.execute_reply": "2025-05-13T19:54:37.216466Z",
     "shell.execute_reply.started": "2025-05-13T19:54:37.213067Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PTSDTransformer(nn.Module):\n",
    "    def __init__(self, audio_dim=24, video_dim=2048, d_model=128):\n",
    "        super().__init__()\n",
    "        self.egemaps_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead=8, dim_feedforward=512, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        self.video_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead=16, dim_feedforward=1024, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        self.audio_proj = nn.Linear(audio_dim, d_model)\n",
    "        self.video_proj = nn.Linear(video_dim, d_model)\n",
    "        \n",
    "        self.classifier = nn.Linear(2*d_model, 2)\n",
    "\n",
    "    def forward(self, audio, video, audio_mask=None, video_mask=None):\n",
    "        # audio pathway\n",
    "        audio = self.audio_proj(audio)\n",
    "        audio = self.egemaps_transformer(audio, src_key_padding_mask=audio_mask)\n",
    "        audio_pooled = audio.mean(dim=1)\n",
    "        \n",
    "        # video pathway\n",
    "        video = self.video_proj(video)\n",
    "        video = self.video_transformer(video, src_key_padding_mask=video_mask)\n",
    "        video_pooled = video.mean(dim=1)\n",
    "        \n",
    "        # fusion\n",
    "        fused = torch.cat([audio_pooled, video_pooled], dim=-1)\n",
    "        return self.classifier(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1501578e-fe4a-44a2-ac70-ae6c533a26e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:54:37.397684Z",
     "iopub.status.busy": "2025-05-13T19:54:37.397462Z",
     "iopub.status.idle": "2025-05-13T19:54:37.400219Z",
     "shell.execute_reply": "2025-05-13T19:54:37.399846Z",
     "shell.execute_reply.started": "2025-05-13T19:54:37.397667Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataloader(h5_path, batch_size=32, pin_memory=False, num_workers=0, shuffle=True):\n",
    "    dataset = HDF5Dataset(h5_path)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "802e504b-090c-4d1c-99b7-aaaa90df8b1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:59:23.484545Z",
     "iopub.status.busy": "2025-05-13T19:59:23.484354Z",
     "iopub.status.idle": "2025-05-13T19:59:23.487687Z",
     "shell.execute_reply": "2025-05-13T19:59:23.487287Z",
     "shell.execute_reply.started": "2025-05-13T19:59:23.484530Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_balanced_dataloader(h5_path, batch_size, collate_fn,  pin_memory=False, num_workers=0):\n",
    "    dataset = HDF5Dataset(h5_path)\n",
    "    \n",
    "    all_labels = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        all_labels.append(label.item())\n",
    "    \n",
    "    class_counts = torch.bincount(torch.tensor(all_labels))\n",
    "    \n",
    "    weights_per_class = 1.0 / class_counts.float()\n",
    "    \n",
    "    sample_weights = [weights_per_class[label] for label in all_labels]\n",
    "    \n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f36ead-3cc0-4d66-8023-fdebbaacb066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:54:37.756599Z",
     "iopub.status.busy": "2025-05-13T19:54:37.756370Z",
     "iopub.status.idle": "2025-05-13T19:54:37.760889Z",
     "shell.execute_reply": "2025-05-13T19:54:37.760537Z",
     "shell.execute_reply.started": "2025-05-13T19:54:37.756581Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in loader:\n",
    "            inputs = {\n",
    "                mod: features[mod].float().to(device, non_blocking=True)\n",
    "                for mod in ['audio_egemaps', 'visual_resnet']\n",
    "            }\n",
    "            masks = {\n",
    "                mod: features[f'{mod}_mask'].to(device, non_blocking=True)\n",
    "                for mod in ['audio_egemaps', 'visual_resnet']\n",
    "            }\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(\n",
    "                inputs['audio_egemaps'],\n",
    "                inputs['visual_resnet'],\n",
    "                audio_mask=~masks['audio_egemaps'],\n",
    "                video_mask=~masks['visual_resnet']\n",
    "            )\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': total_loss / len(loader),\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds),\n",
    "        'recall': recall_score(all_labels, all_preds),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        metrics['auc'] = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        metrics['auc'] = float('nan')\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20be981a-23eb-4f8f-99eb-02ffc9801817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:54:37.941828Z",
     "iopub.status.busy": "2025-05-13T19:54:37.941443Z",
     "iopub.status.idle": "2025-05-13T19:54:37.949065Z",
     "shell.execute_reply": "2025-05-13T19:54:37.948636Z",
     "shell.execute_reply.started": "2025-05-13T19:54:37.941808Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, test_loader, optimizer, scheduler, criterion, num_epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "    history = {\n",
    "        'train': {'loss': [], 'accuracy': [], 'f1': [], 'recall': [], 'auc': []},\n",
    "        'val': {'loss': [], 'accuracy': [], 'f1': [], 'recall': [], 'auc': []},\n",
    "        'test': {'loss': None, 'accuracy': None, 'f1': None, 'recall': None, 'auc': None}\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            inputs = {\n",
    "                mod: features[mod].float().to(device, non_blocking=True)\n",
    "                for mod in ['audio_egemaps', 'visual_resnet']\n",
    "            }\n",
    "            masks = {\n",
    "                mod: features[f'{mod}_mask'].to(device, non_blocking=True)\n",
    "                for mod in ['audio_egemaps', 'visual_resnet']\n",
    "            }\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                inputs['audio_egemaps'],\n",
    "                inputs['visual_resnet'],\n",
    "                audio_mask=~masks['audio_egemaps'],\n",
    "                video_mask=~masks['visual_resnet']\n",
    "            )\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            print(f\"batch idx: {batch_idx} done; train loss: {loss.item()}; avg train loss: {train_loss/(batch_idx+1)}\")\n",
    "            \n",
    "\n",
    "        train_metrics = evaluate(model, train_loader, device)\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        \n",
    "        scheduler.step(val_metrics['loss'])\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current learning rate: {current_lr}\")\n",
    "\n",
    "        for metric in ['loss', 'accuracy', 'f1', 'recall', 'auc']:\n",
    "            history['train'][metric].append(train_metrics[metric])\n",
    "            history['val'][metric].append(val_metrics[metric])\n",
    "\n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, \" +\n",
    "              f\"F1: {train_metrics['f1']:.4f}, Recall: {train_metrics['recall']:.4f}, AUC: {train_metrics['auc']:.4f}\")\n",
    "        print(f\"Val   - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, \" +\n",
    "              f\"F1: {val_metrics['f1']:.4f}, Recall: {val_metrics['recall']:.4f}, AUC: {val_metrics['auc']:.4f}\")\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "    history['test'] = test_metrics\n",
    "\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_metrics['loss']:.4f}, Acc: {test_metrics['accuracy']:.4f}, \" +\n",
    "          f\"F1: {test_metrics['f1']:.4f}, Recall: {test_metrics['recall']:.4f}, AUC: {test_metrics['auc']:.4f}\")\n",
    "\n",
    "    plot_metrics(history['train'], history['val'], history['test'])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5978adc-2262-49c4-9b6b-ab763c7bda23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:54:38.132548Z",
     "iopub.status.busy": "2025-05-13T19:54:38.132357Z",
     "iopub.status.idle": "2025-05-13T19:54:38.136044Z",
     "shell.execute_reply": "2025-05-13T19:54:38.135554Z",
     "shell.execute_reply.started": "2025-05-13T19:54:38.132529Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics(train_metrics, val_metrics, test_metrics):\n",
    "    epochs = range(1, len(train_metrics['loss']) + 1)\n",
    "    metrics = ['loss', 'accuracy', 'f1', 'recall', 'auc']\n",
    "\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        plt.subplot(2, 3, i)\n",
    "        plt.plot(epochs, train_metrics[metric], label='Train')\n",
    "        plt.plot(epochs, val_metrics[metric], label='Validation')\n",
    "        \n",
    "        if test_metrics[metric] is not None:\n",
    "            plt.axhline(y=test_metrics[metric], color='r', linestyle='--', label='Test')\n",
    "        \n",
    "        plt.title(metric.capitalize())\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "855ff108-7600-41a7-be93-ac6cf234d338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:59:26.196920Z",
     "iopub.status.busy": "2025-05-13T19:59:26.196717Z",
     "iopub.status.idle": "2025-05-14T03:16:46.737549Z",
     "shell.execute_reply": "2025-05-14T03:16:46.727783Z",
     "shell.execute_reply.started": "2025-05-13T19:59:26.196902Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx: 0 done; train loss: 0.6348230242729187; avg train loss: 0.6348230242729187\n",
      "batch idx: 1 done; train loss: 0.9704708456993103; avg train loss: 0.8026469349861145\n",
      "batch idx: 2 done; train loss: 0.08553390204906464; avg train loss: 0.5636092573404312\n",
      "batch idx: 3 done; train loss: 0.013515986502170563; avg train loss: 0.42608593963086605\n",
      "batch idx: 4 done; train loss: 4.5282063484191895; avg train loss: 1.2465100213885307\n",
      "batch idx: 5 done; train loss: 0.4734707176685333; avg train loss: 1.1176701374351978\n",
      "batch idx: 6 done; train loss: 0.1527927815914154; avg train loss: 0.9798305151718003\n",
      "batch idx: 7 done; train loss: 5.227176189422607; avg train loss: 1.5107487244531512\n",
      "batch idx: 8 done; train loss: 0.06630849093198776; avg train loss: 1.350255365173022\n",
      "batch idx: 9 done; train loss: 3.452159881591797; avg train loss: 1.5604458168148994\n",
      "batch idx: 10 done; train loss: 4.129878520965576; avg train loss: 1.7940306081013246\n",
      "batch idx: 11 done; train loss: 2.0784449577331543; avg train loss: 1.8177318039039772\n",
      "batch idx: 12 done; train loss: 1.8044949769973755; avg train loss: 1.816713586449623\n",
      "batch idx: 13 done; train loss: 1.0253369808197021; avg train loss: 1.760186686047486\n",
      "batch idx: 14 done; train loss: 0.5960577726364136; avg train loss: 1.682578091820081\n",
      "batch idx: 15 done; train loss: 1.81344735622406; avg train loss: 1.6907574208453298\n",
      "batch idx: 16 done; train loss: 1.7068041563034058; avg train loss: 1.6917013464605106\n",
      "batch idx: 17 done; train loss: 0.860750675201416; avg train loss: 1.64553742027945\n",
      "batch idx: 18 done; train loss: 0.0955340713262558; avg train loss: 1.5639582966503345\n",
      "batch idx: 19 done; train loss: 1.6040302515029907; avg train loss: 1.5659618943929672\n",
      "batch idx: 20 done; train loss: 0.2996116578578949; avg train loss: 1.5056595021770114\n",
      "batch idx: 21 done; train loss: 1.1889970302581787; avg train loss: 1.4912657534534282\n",
      "batch idx: 22 done; train loss: 0.04126382991671562; avg train loss: 1.4282221915605275\n",
      "batch idx: 23 done; train loss: 0.38959476351737976; avg train loss: 1.3849460487253964\n",
      "batch idx: 24 done; train loss: 0.012113929726183414; avg train loss: 1.330032763965428\n",
      "batch idx: 25 done; train loss: 0.275612473487854; avg train loss: 1.2894781374085982\n",
      "batch idx: 26 done; train loss: 0.002205917378887534; avg train loss: 1.2418013885186088\n",
      "batch idx: 27 done; train loss: 3.5094223022460938; avg train loss: 1.3227878497231618\n",
      "batch idx: 28 done; train loss: 0.06580913066864014; avg train loss: 1.279443755962661\n",
      "batch idx: 29 done; train loss: 0.04912294074892998; avg train loss: 1.2384330621222035\n",
      "batch idx: 30 done; train loss: 1.2990717887878418; avg train loss: 1.2403891500791595\n",
      "batch idx: 31 done; train loss: 0.031670019030570984; avg train loss: 1.202616677233891\n",
      "batch idx: 32 done; train loss: 4.383428573608398; avg train loss: 1.2990049165179671\n",
      "batch idx: 33 done; train loss: 0.0059450226835906506; avg train loss: 1.260973743169897\n",
      "batch idx: 34 done; train loss: 6.315752029418945; avg train loss: 1.40539597991987\n",
      "batch idx: 35 done; train loss: 0.00402010977268219; avg train loss: 1.3664688724157814\n",
      "batch idx: 36 done; train loss: 0.015752287581562996; avg train loss: 1.3299630187716134\n",
      "batch idx: 37 done; train loss: 0.17555798590183258; avg train loss: 1.2995839389592507\n",
      "batch idx: 38 done; train loss: 5.080785751342773; avg train loss: 1.3965378315844692\n",
      "batch idx: 39 done; train loss: 0.020157836377620697; avg train loss: 1.362128331704298\n",
      "batch idx: 40 done; train loss: 4.0256476402282715; avg train loss: 1.4270922172780536\n",
      "batch idx: 41 done; train loss: 0.1497744470834732; avg train loss: 1.3966798894162777\n",
      "batch idx: 42 done; train loss: 2.441525459289551; avg train loss: 1.420978623599377\n",
      "batch idx: 43 done; train loss: 0.9900267124176025; avg train loss: 1.4111842619816095\n",
      "batch idx: 44 done; train loss: 3.916626214981079; avg train loss: 1.4668607498260422\n",
      "batch idx: 45 done; train loss: 0.018527541309595108; avg train loss: 1.435375245293076\n",
      "batch idx: 46 done; train loss: 0.41689401865005493; avg train loss: 1.4137054319602458\n",
      "batch idx: 47 done; train loss: 0.03143519535660744; avg train loss: 1.3849081353643367\n",
      "batch idx: 48 done; train loss: 3.4471912384033203; avg train loss: 1.4269955456304384\n",
      "batch idx: 49 done; train loss: 2.008453130722046; avg train loss: 1.4386246973322705\n",
      "batch idx: 50 done; train loss: 3.7922568321228027; avg train loss: 1.4847743470340455\n",
      "batch idx: 51 done; train loss: 1.840453863143921; avg train loss: 1.4916143377284663\n",
      "batch idx: 52 done; train loss: 1.9572347402572632; avg train loss: 1.5003996283422172\n",
      "batch idx: 53 done; train loss: 1.1596126556396484; avg train loss: 1.4940887584773548\n",
      "batch idx: 54 done; train loss: 0.6061023473739624; avg train loss: 1.4779435510027477\n",
      "batch idx: 55 done; train loss: 5.13306188583374; avg train loss: 1.5432135212675868\n",
      "batch idx: 56 done; train loss: 0.8446152806282043; avg train loss: 1.5309574117826854\n",
      "batch idx: 57 done; train loss: 0.011636242270469666; avg train loss: 1.5047622192048886\n",
      "batch idx: 58 done; train loss: 0.007054422050714493; avg train loss: 1.4793773412870213\n",
      "batch idx: 59 done; train loss: 0.030291354283690453; avg train loss: 1.455225908170299\n",
      "batch idx: 60 done; train loss: 0.1537272334098816; avg train loss: 1.4338898643217675\n",
      "batch idx: 61 done; train loss: 5.404608726501465; avg train loss: 1.497933716937569\n",
      "batch idx: 62 done; train loss: 0.005690327845513821; avg train loss: 1.4742473139361079\n",
      "batch idx: 63 done; train loss: 0.008257297798991203; avg train loss: 1.4513412199339655\n",
      "batch idx: 64 done; train loss: 4.289564609527588; avg train loss: 1.4950061951584828\n",
      "batch idx: 65 done; train loss: 0.014650856144726276; avg train loss: 1.4725765688097894\n",
      "batch idx: 66 done; train loss: 2.9678502082824707; avg train loss: 1.4948940858168445\n",
      "batch idx: 67 done; train loss: 0.009683186188340187; avg train loss: 1.4730527490576018\n",
      "batch idx: 68 done; train loss: 0.010931958444416523; avg train loss: 1.4518625926719033\n",
      "batch idx: 69 done; train loss: 4.289258003234863; avg train loss: 1.492396812822803\n",
      "batch idx: 70 done; train loss: 2.7414026260375977; avg train loss: 1.5099884439948421\n",
      "batch idx: 71 done; train loss: 3.2506022453308105; avg train loss: 1.5341636356800639\n",
      "batch idx: 72 done; train loss: 2.7139689922332764; avg train loss: 1.5503253528931216\n",
      "batch idx: 73 done; train loss: 0.047548096626996994; avg train loss: 1.5300175521327686\n",
      "batch idx: 74 done; train loss: 0.06393086910247803; avg train loss: 1.5104697296923648\n",
      "batch idx: 75 done; train loss: 0.36899811029434204; avg train loss: 1.495450366279233\n",
      "batch idx: 76 done; train loss: 1.5863158702850342; avg train loss: 1.4966304377598276\n",
      "batch idx: 77 done; train loss: 0.1573725789785385; avg train loss: 1.4794604652113497\n",
      "batch idx: 78 done; train loss: 1.176578164100647; avg train loss: 1.4756265120327332\n",
      "batch idx: 79 done; train loss: 0.14826838672161102; avg train loss: 1.459034535466344\n",
      "batch idx: 80 done; train loss: 3.508423089981079; avg train loss: 1.484335628731958\n",
      "batch idx: 81 done; train loss: 0.31468307971954346; avg train loss: 1.4700715732561969\n",
      "batch idx: 82 done; train loss: 0.4122406840324402; avg train loss: 1.4573266227836215\n",
      "batch idx: 83 done; train loss: 0.6329423189163208; avg train loss: 1.4475125239280584\n",
      "batch idx: 84 done; train loss: 2.841947555541992; avg train loss: 1.4639176419470459\n",
      "batch idx: 85 done; train loss: 0.3243354856967926; avg train loss: 1.4506666866418103\n",
      "batch idx: 86 done; train loss: 1.8277006149291992; avg train loss: 1.4550004099554585\n",
      "batch idx: 87 done; train loss: 3.9931912422180176; avg train loss: 1.4838434875948059\n",
      "batch idx: 88 done; train loss: 1.7344857454299927; avg train loss: 1.4866596927390214\n",
      "batch idx: 89 done; train loss: 3.0784595012664795; avg train loss: 1.5043463572782154\n",
      "batch idx: 90 done; train loss: 0.06132291629910469; avg train loss: 1.4884889568278954\n",
      "batch idx: 91 done; train loss: 0.3456805944442749; avg train loss: 1.4760671268019865\n",
      "batch idx: 92 done; train loss: 0.4583202600479126; avg train loss: 1.4651236121057063\n",
      "batch idx: 93 done; train loss: 1.3966113328933716; avg train loss: 1.4643947580715324\n",
      "batch idx: 94 done; train loss: 1.6427217721939087; avg train loss: 1.4662718845359786\n",
      "batch idx: 95 done; train loss: 2.9449901580810547; avg train loss: 1.4816751998854063\n",
      "batch idx: 96 done; train loss: 0.4982960522174835; avg train loss: 1.4715372705280052\n",
      "batch idx: 97 done; train loss: 0.11449062079191208; avg train loss: 1.4576898557347797\n",
      "batch idx: 98 done; train loss: 2.057882785797119; avg train loss: 1.4637524105838942\n",
      "batch idx: 99 done; train loss: 2.4866080284118652; avg train loss: 1.473980966762174\n",
      "batch idx: 100 done; train loss: 0.08536841720342636; avg train loss: 1.460232327657632\n",
      "batch idx: 101 done; train loss: 0.11017435789108276; avg train loss: 1.4469964652089402\n",
      "batch idx: 102 done; train loss: 0.8032615780830383; avg train loss: 1.4407466119358732\n",
      "batch idx: 103 done; train loss: 0.40180426836013794; avg train loss: 1.4307567817091835\n",
      "batch idx: 104 done; train loss: 0.22892941534519196; avg train loss: 1.419310806791431\n",
      "batch idx: 105 done; train loss: 1.1161848306655884; avg train loss: 1.416451127771376\n",
      "batch idx: 106 done; train loss: 0.916256844997406; avg train loss: 1.4117764148482548\n",
      "batch idx: 107 done; train loss: 0.5382511615753174; avg train loss: 1.4036882180586905\n",
      "batch idx: 108 done; train loss: 2.281203508377075; avg train loss: 1.4117388170524372\n",
      "batch idx: 109 done; train loss: 0.8795867562294006; avg train loss: 1.406901071044955\n",
      "batch idx: 110 done; train loss: 0.7176311016082764; avg train loss: 1.4006914316806607\n",
      "batch idx: 111 done; train loss: 1.4626530408859253; avg train loss: 1.4012446603342792\n",
      "batch idx: 112 done; train loss: 0.8419125080108643; avg train loss: 1.3962948182783197\n",
      "batch idx: 113 done; train loss: 0.14581464231014252; avg train loss: 1.3853256939277216\n",
      "batch idx: 114 done; train loss: 1.0917243957519531; avg train loss: 1.3827726391609758\n",
      "batch idx: 115 done; train loss: 1.3484556674957275; avg train loss: 1.3824768031983443\n",
      "batch idx: 116 done; train loss: 0.3009500205516815; avg train loss: 1.3732329845432447\n",
      "batch idx: 117 done; train loss: 1.0701993703842163; avg train loss: 1.3706649030673208\n",
      "batch idx: 118 done; train loss: 0.5275092720985413; avg train loss: 1.3635795616306083\n",
      "batch idx: 119 done; train loss: 0.5963900089263916; avg train loss: 1.3571863153580732\n",
      "batch idx: 120 done; train loss: 1.8058810234069824; avg train loss: 1.36089453608575\n",
      "batch idx: 121 done; train loss: 0.2701011598110199; avg train loss: 1.3519536067720228\n",
      "batch idx: 122 done; train loss: 1.210081696510315; avg train loss: 1.3508001766072935\n",
      "batch idx: 123 done; train loss: 0.6095471382141113; avg train loss: 1.3448223295234774\n",
      "batch idx: 124 done; train loss: 0.4867093861103058; avg train loss: 1.3379574259761722\n",
      "batch idx: 125 done; train loss: 0.46814239025115967; avg train loss: 1.3310541320418465\n",
      "batch idx: 126 done; train loss: 1.5498358011245728; avg train loss: 1.3327768223495846\n",
      "batch idx: 127 done; train loss: 0.1483287215232849; avg train loss: 1.3235233215618791\n",
      "batch idx: 128 done; train loss: 0.7623087763786316; avg train loss: 1.3191728212116214\n",
      "batch idx: 129 done; train loss: 1.6563429832458496; avg train loss: 1.321766437842654\n",
      "batch idx: 130 done; train loss: 1.9310479164123535; avg train loss: 1.3264174414958578\n",
      "batch idx: 131 done; train loss: 1.7736252546310425; avg train loss: 1.3298053794741547\n",
      "batch idx: 132 done; train loss: 0.4654916226863861; avg train loss: 1.3233067797990585\n",
      "batch idx: 133 done; train loss: 1.256203532218933; avg train loss: 1.3228060092947294\n",
      "batch idx: 134 done; train loss: 1.7403309345245361; avg train loss: 1.3258987865186538\n",
      "batch idx: 135 done; train loss: 2.61877179145813; avg train loss: 1.3354052056726204\n",
      "batch idx: 136 done; train loss: 2.528393507003784; avg train loss: 1.3441131494779575\n",
      "batch idx: 137 done; train loss: 0.36809393763542175; avg train loss: 1.3370405464935913\n",
      "batch idx: 138 done; train loss: 0.5286749601364136; avg train loss: 1.331224966735626\n",
      "batch idx: 139 done; train loss: 0.28616541624069214; avg train loss: 1.3237602556606622\n",
      "batch idx: 140 done; train loss: 0.32291337847709656; avg train loss: 1.3166620508579419\n",
      "batch idx: 141 done; train loss: 1.4136085510253906; avg train loss: 1.317344772690107\n",
      "batch idx: 142 done; train loss: 0.3744312822818756; avg train loss: 1.3107509720578816\n",
      "batch idx: 143 done; train loss: 0.29446887969970703; avg train loss: 1.3036934575276165\n",
      "batch idx: 144 done; train loss: 1.973114252090454; avg train loss: 1.3083101526625327\n",
      "batch idx: 145 done; train loss: 1.0761052370071411; avg train loss: 1.306719708034756\n",
      "batch idx: 146 done; train loss: 0.8661422729492188; avg train loss: 1.3037225826260108\n",
      "batch idx: 147 done; train loss: 1.089727520942688; avg train loss: 1.3022766700470694\n",
      "batch idx: 148 done; train loss: 0.2930907905101776; avg train loss: 1.295503610452862\n",
      "batch idx: 149 done; train loss: 1.5121666193008423; avg train loss: 1.2969480305118486\n",
      "batch idx: 150 done; train loss: 1.5334022045135498; avg train loss: 1.298513952193979\n",
      "batch idx: 151 done; train loss: 1.0916897058486938; avg train loss: 1.29715326636276\n",
      "batch idx: 152 done; train loss: 0.34578031301498413; avg train loss: 1.29093514248467\n",
      "batch idx: 153 done; train loss: 1.285388708114624; avg train loss: 1.2908991266770724\n",
      "batch idx: 154 done; train loss: 0.5256722569465637; avg train loss: 1.2859621791304239\n",
      "batch idx: 155 done; train loss: 1.4151688814163208; avg train loss: 1.286790427222\n",
      "batch idx: 156 done; train loss: 0.2196355164051056; avg train loss: 1.27999326218495\n",
      "batch idx: 157 done; train loss: 0.25961700081825256; avg train loss: 1.2735351845813632\n",
      "batch idx: 158 done; train loss: 0.22911818325519562; avg train loss: 1.266966524195664\n",
      "batch idx: 159 done; train loss: 1.6669726371765137; avg train loss: 1.2694665624017945\n",
      "batch idx: 160 done; train loss: 0.8133710622787476; avg train loss: 1.2666336710966823\n",
      "batch idx: 161 done; train loss: 0.8768675327301025; avg train loss: 1.2642277072796047\n",
      "batch idx: 162 done; train loss: 0.9934206008911133; avg train loss: 1.2625663139888776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate: 0.0005\n",
      "\n",
      "Epoch 1/10\n",
      "Train - Loss: 0.7379, Acc: 0.5951, F1: 0.6887, Recall: 0.8588, AUC: 0.6605\n",
      "Val   - Loss: 0.9711, Acc: 0.4727, F1: 0.5085, Recall: 0.8824, AUC: 0.6037\n",
      "batch idx: 0 done; train loss: 0.4728243947029114; avg train loss: 0.4728243947029114\n",
      "batch idx: 1 done; train loss: 0.22106553614139557; avg train loss: 0.3469449654221535\n",
      "batch idx: 2 done; train loss: 0.16680647432804108; avg train loss: 0.286898801724116\n",
      "batch idx: 3 done; train loss: 1.3201426267623901; avg train loss: 0.5452097579836845\n",
      "batch idx: 4 done; train loss: 2.14467191696167; avg train loss: 0.8651021897792817\n",
      "batch idx: 5 done; train loss: 0.04506317153573036; avg train loss: 0.728429020072023\n",
      "batch idx: 6 done; train loss: 2.852041244506836; avg train loss: 1.0318021949912821\n",
      "batch idx: 7 done; train loss: 0.11675974726676941; avg train loss: 0.917421889025718\n",
      "batch idx: 8 done; train loss: 0.09845616668462753; avg train loss: 0.8264256976544857\n",
      "batch idx: 9 done; train loss: 0.015867631882429123; avg train loss: 0.74536989107728\n",
      "batch idx: 10 done; train loss: 0.016763299703598022; avg train loss: 0.6791329282251272\n",
      "batch idx: 11 done; train loss: 4.361998081207275; avg train loss: 0.9860383576403061\n",
      "batch idx: 12 done; train loss: 3.1678733825683594; avg train loss: 1.1538718210963101\n",
      "batch idx: 13 done; train loss: 3.6293938159942627; avg train loss: 1.3306948207318783\n",
      "batch idx: 14 done; train loss: 3.829303741455078; avg train loss: 1.4972687487800915\n",
      "batch idx: 15 done; train loss: 3.448232889175415; avg train loss: 1.6192040075547993\n",
      "batch idx: 16 done; train loss: 0.014880494214594364; avg train loss: 1.5248320361818462\n",
      "batch idx: 17 done; train loss: 2.691634178161621; avg train loss: 1.5896543774029448\n",
      "batch idx: 18 done; train loss: 0.07018307596445084; avg train loss: 1.509682203643024\n",
      "batch idx: 19 done; train loss: 0.08027061074972153; avg train loss: 1.438211623998359\n",
      "batch idx: 20 done; train loss: 2.321336269378662; avg train loss: 1.480265178540278\n",
      "batch idx: 21 done; train loss: 1.8916202783584595; avg train loss: 1.4989631376229227\n",
      "batch idx: 22 done; train loss: 2.2913753986358643; avg train loss: 1.5334158446234853\n",
      "batch idx: 23 done; train loss: 0.06485413014888763; avg train loss: 1.4722257731870438\n",
      "batch idx: 24 done; train loss: 2.265458822250366; avg train loss: 1.5039550951495766\n",
      "batch idx: 25 done; train loss: 0.35791778564453125; avg train loss: 1.4598767370916903\n",
      "batch idx: 26 done; train loss: 0.46340763568878174; avg train loss: 1.4229704740767677\n",
      "batch idx: 27 done; train loss: 0.12380290776491165; avg train loss: 1.3765716324227728\n",
      "batch idx: 28 done; train loss: 0.25705671310424805; avg train loss: 1.3379676696876515\n",
      "batch idx: 29 done; train loss: 0.765734076499939; avg train loss: 1.3188932165813942\n",
      "batch idx: 30 done; train loss: 0.4137590527534485; avg train loss: 1.28969534032888\n",
      "batch idx: 31 done; train loss: 0.08108614385128021; avg train loss: 1.251926302938955\n",
      "batch idx: 32 done; train loss: 0.6462261080741882; avg train loss: 1.2335717515794165\n",
      "batch idx: 33 done; train loss: 0.031455181539058685; avg train loss: 1.198215381872347\n",
      "batch idx: 34 done; train loss: 2.546701669692993; avg train loss: 1.2367435615243656\n",
      "batch idx: 35 done; train loss: 2.820786237716675; avg train loss: 1.280744746974152\n",
      "batch idx: 36 done; train loss: 0.15534737706184387; avg train loss: 1.2503286018413868\n",
      "batch idx: 37 done; train loss: 0.3169780969619751; avg train loss: 1.2257667464498234\n",
      "batch idx: 38 done; train loss: 2.3950765132904053; avg train loss: 1.2557490481636846\n",
      "batch idx: 39 done; train loss: 0.06190285086631775; avg train loss: 1.2259028932312503\n",
      "batch idx: 40 done; train loss: 0.08662252873182297; avg train loss: 1.1981155672678496\n",
      "batch idx: 41 done; train loss: 0.017316695302724838; avg train loss: 1.1700013084115373\n",
      "batch idx: 42 done; train loss: 0.06416768580675125; avg train loss: 1.1442842474207282\n",
      "batch idx: 43 done; train loss: 0.030051231384277344; avg train loss: 1.1189607697835362\n",
      "batch idx: 44 done; train loss: 3.9664034843444824; avg train loss: 1.1822372745515572\n",
      "batch idx: 45 done; train loss: 0.006151551380753517; avg train loss: 1.1566701936130614\n",
      "batch idx: 46 done; train loss: 0.045637346804142; avg train loss: 1.133031196872446\n",
      "batch idx: 47 done; train loss: 4.9386515617370605; avg train loss: 1.2123149544737923\n",
      "batch idx: 48 done; train loss: 0.004441871773451567; avg train loss: 1.187664483398275\n",
      "batch idx: 49 done; train loss: 3.1101176738739014; avg train loss: 1.2261135472077875\n",
      "batch idx: 50 done; train loss: 0.02703692391514778; avg train loss: 1.2026022408687163\n",
      "batch idx: 51 done; train loss: 5.075803756713867; avg train loss: 1.2770868854042\n",
      "batch idx: 52 done; train loss: 5.487715721130371; avg train loss: 1.356532712493373\n",
      "batch idx: 53 done; train loss: 2.254009962081909; avg train loss: 1.3731526615598273\n",
      "batch idx: 54 done; train loss: 0.005641017109155655; avg train loss: 1.348288813478906\n",
      "batch idx: 55 done; train loss: 0.010986787267029285; avg train loss: 1.3244084201536939\n",
      "batch idx: 56 done; train loss: 0.016897985711693764; avg train loss: 1.3014696406020798\n",
      "batch idx: 57 done; train loss: 0.04033397510647774; avg train loss: 1.279725922231466\n",
      "batch idx: 58 done; train loss: 4.39981746673584; avg train loss: 1.3326088297654386\n",
      "batch idx: 59 done; train loss: 0.025173846632242203; avg train loss: 1.3108182467132186\n",
      "batch idx: 60 done; train loss: 2.3220603466033936; avg train loss: 1.3273959860556805\n",
      "batch idx: 61 done; train loss: 0.040264930576086044; avg train loss: 1.3066358077414935\n",
      "batch idx: 62 done; train loss: 0.03044770285487175; avg train loss: 1.2863788536956742\n",
      "batch idx: 63 done; train loss: 2.4407835006713867; avg train loss: 1.3044164263046696\n",
      "batch idx: 64 done; train loss: 0.091841921210289; avg train loss: 1.285761433918602\n",
      "batch idx: 65 done; train loss: 0.016606906428933144; avg train loss: 1.2665318198657285\n",
      "batch idx: 66 done; train loss: 0.1665259450674057; avg train loss: 1.25011382173441\n",
      "batch idx: 67 done; train loss: 0.025702856481075287; avg train loss: 1.2321077781277434\n",
      "batch idx: 68 done; train loss: 0.02987237274646759; avg train loss: 1.2146840766004785\n",
      "batch idx: 69 done; train loss: 0.03304355964064598; avg train loss: 1.1978034977867666\n",
      "batch idx: 70 done; train loss: 2.754080057144165; avg train loss: 1.2197228859467302\n",
      "batch idx: 71 done; train loss: 0.03739798069000244; avg train loss: 1.2033017067070533\n",
      "batch idx: 72 done; train loss: 4.586427688598633; avg train loss: 1.2496458982398146\n",
      "batch idx: 73 done; train loss: 0.025962378829717636; avg train loss: 1.2331096344640025\n",
      "batch idx: 74 done; train loss: 0.18922851979732513; avg train loss: 1.2191912196017802\n",
      "batch idx: 75 done; train loss: 3.4589717388153076; avg train loss: 1.2486620159072213\n",
      "batch idx: 76 done; train loss: 0.00737965339794755; avg train loss: 1.2325414657447633\n",
      "batch idx: 77 done; train loss: 0.054720476269721985; avg train loss: 1.2174411966489294\n",
      "batch idx: 78 done; train loss: 3.934403896331787; avg train loss: 1.2518331295563072\n",
      "batch idx: 79 done; train loss: 2.470120668411255; avg train loss: 1.267061723791994\n",
      "batch idx: 80 done; train loss: 3.070810079574585; avg train loss: 1.2893302220115324\n",
      "batch idx: 81 done; train loss: 0.025781279429793358; avg train loss: 1.2739210885654135\n",
      "batch idx: 82 done; train loss: 0.02226218767464161; avg train loss: 1.258840860843838\n",
      "batch idx: 83 done; train loss: 0.024293826892971992; avg train loss: 1.244143872344423\n",
      "batch idx: 84 done; train loss: 4.861988544464111; avg train loss: 1.2867067508399486\n",
      "batch idx: 85 done; train loss: 0.03910352289676666; avg train loss: 1.2721997365615396\n",
      "batch idx: 86 done; train loss: 3.0458264350891113; avg train loss: 1.2925862503377186\n",
      "batch idx: 87 done; train loss: 0.054147541522979736; avg train loss: 1.2785130831920966\n",
      "batch idx: 88 done; train loss: 3.4925854206085205; avg train loss: 1.3033903004664384\n",
      "batch idx: 89 done; train loss: 5.050484657287598; avg train loss: 1.3450246822088956\n",
      "batch idx: 90 done; train loss: 0.02673996053636074; avg train loss: 1.3305380369157909\n",
      "batch idx: 91 done; train loss: 3.0438318252563477; avg train loss: 1.34916079548471\n",
      "batch idx: 92 done; train loss: 3.956566095352173; avg train loss: 1.3771974116123171\n",
      "batch idx: 93 done; train loss: 3.797530174255371; avg train loss: 1.4029456324914986\n",
      "batch idx: 94 done; train loss: 0.04925050213932991; avg train loss: 1.3886962100667388\n",
      "batch idx: 95 done; train loss: 3.7957265377044678; avg train loss: 1.4137694426462986\n",
      "batch idx: 96 done; train loss: 0.03994884714484215; avg train loss: 1.399606343723603\n",
      "batch idx: 97 done; train loss: 3.1602742671966553; avg train loss: 1.4175723429427158\n",
      "batch idx: 98 done; train loss: 3.6700141429901123; avg train loss: 1.440324280316932\n",
      "batch idx: 99 done; train loss: 0.05357958748936653; avg train loss: 1.4264568333886565\n",
      "batch idx: 100 done; train loss: 2.745619297027588; avg train loss: 1.439517847880131\n",
      "batch idx: 101 done; train loss: 2.535963773727417; avg train loss: 1.450267317741379\n",
      "batch idx: 102 done; train loss: 0.0754380151629448; avg train loss: 1.4369194604347921\n",
      "batch idx: 103 done; train loss: 0.056656431406736374; avg train loss: 1.4236477005402917\n",
      "batch idx: 104 done; train loss: 0.11664433777332306; avg train loss: 1.4112000494663204\n",
      "batch idx: 105 done; train loss: 2.7794129848480225; avg train loss: 1.4241077186680347\n",
      "batch idx: 106 done; train loss: 0.06363873183727264; avg train loss: 1.4113930552397098\n",
      "batch idx: 107 done; train loss: 2.468251943588257; avg train loss: 1.4211787856873814\n",
      "batch idx: 108 done; train loss: 0.05785592272877693; avg train loss: 1.4086712364859264\n",
      "batch idx: 109 done; train loss: 0.056031517684459686; avg train loss: 1.3963745117695494\n",
      "batch idx: 110 done; train loss: 0.07914386689662933; avg train loss: 1.3845075690229465\n",
      "batch idx: 111 done; train loss: 0.05598631873726845; avg train loss: 1.3726457721453957\n",
      "batch idx: 112 done; train loss: 0.061450161039829254; avg train loss: 1.3610422711621608\n",
      "batch idx: 113 done; train loss: 2.4820196628570557; avg train loss: 1.3708754061770283\n",
      "batch idx: 114 done; train loss: 0.055508051067590714; avg train loss: 1.3594374291760767\n",
      "batch idx: 115 done; train loss: 2.6292431354522705; avg train loss: 1.3703840300922507\n",
      "batch idx: 116 done; train loss: 0.02532472275197506; avg train loss: 1.3588877966961799\n",
      "batch idx: 117 done; train loss: 3.0346381664276123; avg train loss: 1.3730890710159378\n",
      "batch idx: 118 done; train loss: 0.06066546216607094; avg train loss: 1.36206030119367\n",
      "batch idx: 119 done; train loss: 0.04335692524909973; avg train loss: 1.351071106394132\n",
      "batch idx: 120 done; train loss: 0.02571284957230091; avg train loss: 1.340117732370811\n",
      "batch idx: 121 done; train loss: 2.4805986881256104; avg train loss: 1.3494659369261783\n",
      "batch idx: 122 done; train loss: 0.012977663427591324; avg train loss: 1.3386001786050516\n",
      "batch idx: 123 done; train loss: 3.8631317615509033; avg train loss: 1.3589593042739696\n",
      "batch idx: 124 done; train loss: 0.015296058729290962; avg train loss: 1.3482099983096123\n",
      "batch idx: 125 done; train loss: 0.007205688860267401; avg train loss: 1.337567106964776\n",
      "batch idx: 126 done; train loss: 4.22462272644043; avg train loss: 1.3602998283779704\n",
      "batch idx: 127 done; train loss: 0.014888128265738487; avg train loss: 1.3497887994708435\n",
      "batch idx: 128 done; train loss: 0.022115163505077362; avg train loss: 1.3394967557811863\n",
      "batch idx: 129 done; train loss: 0.035438381135463715; avg train loss: 1.3294655375146809\n",
      "batch idx: 130 done; train loss: 3.263942003250122; avg train loss: 1.3442325334363254\n",
      "batch idx: 131 done; train loss: 0.007934125140309334; avg train loss: 1.334109060646204\n",
      "batch idx: 132 done; train loss: 1.7110679149627686; avg train loss: 1.3369433377463287\n",
      "batch idx: 133 done; train loss: 4.291478633880615; avg train loss: 1.3589921086130023\n",
      "batch idx: 134 done; train loss: 3.058894634246826; avg train loss: 1.371583979173253\n",
      "batch idx: 135 done; train loss: 1.9364725351333618; avg train loss: 1.375737571496489\n",
      "batch idx: 136 done; train loss: 2.2292661666870117; avg train loss: 1.3819677072278067\n",
      "batch idx: 137 done; train loss: 0.04885948449373245; avg train loss: 1.372307502715241\n",
      "batch idx: 138 done; train loss: 1.5606869459152222; avg train loss: 1.3736627505080465\n",
      "batch idx: 139 done; train loss: 0.1609025001525879; avg train loss: 1.365000177291222\n",
      "batch idx: 140 done; train loss: 3.274219274520874; avg train loss: 1.3785407382644819\n",
      "batch idx: 141 done; train loss: 1.1232531070709229; avg train loss: 1.3767429380448089\n",
      "batch idx: 142 done; train loss: 0.0684414952993393; avg train loss: 1.3675939769067287\n",
      "batch idx: 143 done; train loss: 0.029258159920573235; avg train loss: 1.3582999781776581\n",
      "batch idx: 144 done; train loss: 0.11214195936918259; avg train loss: 1.3497057849444962\n",
      "batch idx: 145 done; train loss: 0.2120964229106903; avg train loss: 1.3419139399990592\n",
      "batch idx: 146 done; train loss: 1.2324635982513428; avg train loss: 1.3411693798511155\n",
      "batch idx: 147 done; train loss: 3.6033177375793457; avg train loss: 1.356454166051982\n",
      "batch idx: 148 done; train loss: 3.106238842010498; avg train loss: 1.3681976873671398\n",
      "batch idx: 149 done; train loss: 0.07365994155406952; avg train loss: 1.359567435728386\n",
      "batch idx: 150 done; train loss: 0.10621142387390137; avg train loss: 1.3512670647889524\n",
      "batch idx: 151 done; train loss: 0.013991131447255611; avg train loss: 1.3424691968064413\n",
      "batch idx: 152 done; train loss: 3.022758960723877; avg train loss: 1.3534514828451172\n",
      "batch idx: 153 done; train loss: 0.016102386638522148; avg train loss: 1.3447673978048147\n",
      "batch idx: 154 done; train loss: 0.08528106659650803; avg train loss: 1.3366416795389546\n",
      "batch idx: 155 done; train loss: 0.009883160702884197; avg train loss: 1.3281368172387233\n",
      "batch idx: 156 done; train loss: 3.6832401752471924; avg train loss: 1.3431374755699876\n",
      "batch idx: 157 done; train loss: 1.6246649026870728; avg train loss: 1.3449192947289565\n",
      "batch idx: 158 done; train loss: 0.10494902729988098; avg train loss: 1.337120739587893\n",
      "batch idx: 159 done; train loss: 4.57433557510376; avg train loss: 1.3573533323098672\n",
      "batch idx: 160 done; train loss: 3.8032374382019043; avg train loss: 1.3725451590545383\n",
      "batch idx: 161 done; train loss: 1.7491014003753662; avg train loss: 1.3748695802972595\n",
      "batch idx: 162 done; train loss: 0.06530281901359558; avg train loss: 1.3668354283875437\n",
      "Current learning rate: 0.0005\n",
      "\n",
      "Epoch 2/10\n",
      "Train - Loss: 1.2441, Acc: 0.5337, F1: 0.6885, Recall: 1.0000, AUC: 0.6109\n",
      "Val   - Loss: 1.8210, Acc: 0.3273, F1: 0.4789, Recall: 1.0000, AUC: 0.5588\n",
      "batch idx: 0 done; train loss: 0.055838532745838165; avg train loss: 0.055838532745838165\n",
      "batch idx: 1 done; train loss: 3.4885144233703613; avg train loss: 1.7721764780580997\n",
      "batch idx: 2 done; train loss: 0.03134693577885628; avg train loss: 1.1918999639650185\n",
      "batch idx: 3 done; train loss: 2.2045466899871826; avg train loss: 1.4450616454705596\n",
      "batch idx: 4 done; train loss: 3.0913596153259277; avg train loss: 1.7743212394416332\n",
      "batch idx: 5 done; train loss: 0.021054573357105255; avg train loss: 1.4821101284275453\n",
      "batch idx: 6 done; train loss: 0.4161437153816223; avg train loss: 1.3298292122781277\n",
      "batch idx: 7 done; train loss: 0.2128823846578598; avg train loss: 1.1902108588255942\n",
      "batch idx: 8 done; train loss: 1.6495929956436157; avg train loss: 1.2412533184720411\n",
      "batch idx: 9 done; train loss: 0.4406083822250366; avg train loss: 1.1611888248473405\n",
      "batch idx: 10 done; train loss: 2.5562756061553955; avg train loss: 1.2880148958753457\n",
      "batch idx: 11 done; train loss: 2.243692398071289; avg train loss: 1.3676546877250075\n",
      "batch idx: 12 done; train loss: 0.8836358785629272; avg train loss: 1.3304224716356168\n",
      "batch idx: 13 done; train loss: 1.2249538898468018; avg train loss: 1.3228890015078443\n",
      "batch idx: 14 done; train loss: 2.2403922080993652; avg train loss: 1.384055881947279\n",
      "batch idx: 15 done; train loss: 1.303594946861267; avg train loss: 1.3790270735044032\n",
      "batch idx: 16 done; train loss: 0.15923431515693665; avg train loss: 1.3072745583074934\n",
      "batch idx: 17 done; train loss: 0.5384729504585266; avg train loss: 1.2645633578714397\n",
      "batch idx: 18 done; train loss: 0.5283417701721191; avg train loss: 1.225814853255686\n",
      "batch idx: 19 done; train loss: 0.9214107394218445; avg train loss: 1.210594647563994\n",
      "batch idx: 20 done; train loss: 1.2062828540802002; avg train loss: 1.2103893240647656\n",
      "batch idx: 21 done; train loss: 0.6944437026977539; avg train loss: 1.1869372503662652\n",
      "batch idx: 22 done; train loss: 0.235830619931221; avg train loss: 1.1455847881734371\n",
      "batch idx: 23 done; train loss: 0.547088086605072; avg train loss: 1.1206474256080885\n",
      "batch idx: 24 done; train loss: 1.0479615926742554; avg train loss: 1.1177399922907352\n",
      "batch idx: 25 done; train loss: 0.5813138484954834; avg train loss: 1.0971082175293794\n",
      "batch idx: 26 done; train loss: 1.0532987117767334; avg train loss: 1.0954856432422444\n",
      "batch idx: 27 done; train loss: 0.7409366369247437; avg train loss: 1.082823178730905\n",
      "batch idx: 28 done; train loss: 1.0477594137191772; avg train loss: 1.081614083385673\n",
      "batch idx: 29 done; train loss: 0.11677353829145432; avg train loss: 1.0494527318825324\n",
      "batch idx: 30 done; train loss: 1.051103949546814; avg train loss: 1.049505996968477\n",
      "batch idx: 31 done; train loss: 2.0968427658081055; avg train loss: 1.0822352709947154\n",
      "batch idx: 32 done; train loss: 1.7195396423339844; avg train loss: 1.1015475246716628\n",
      "batch idx: 33 done; train loss: 0.06601214408874512; avg train loss: 1.071090601713342\n",
      "batch idx: 34 done; train loss: 1.1523957252502441; avg train loss: 1.0734136052429677\n",
      "batch idx: 35 done; train loss: 0.835099995136261; avg train loss: 1.0667937827400036\n",
      "batch idx: 36 done; train loss: 0.5998477935791016; avg train loss: 1.05417362087079\n",
      "batch idx: 37 done; train loss: 1.1649173498153687; avg train loss: 1.0570879295272262\n",
      "batch idx: 38 done; train loss: 0.6031215786933899; avg train loss: 1.045447766685333\n",
      "batch idx: 39 done; train loss: 0.6170061826705933; avg train loss: 1.0347367270849646\n",
      "batch idx: 40 done; train loss: 1.539255142211914; avg train loss: 1.0470420542831829\n",
      "batch idx: 41 done; train loss: 0.7468873858451843; avg train loss: 1.0398955145584685\n",
      "batch idx: 42 done; train loss: 0.16977229714393616; avg train loss: 1.0196600908976654\n",
      "batch idx: 43 done; train loss: 1.361479640007019; avg train loss: 1.0274287170137872\n",
      "batch idx: 44 done; train loss: 0.26677075028419495; avg train loss: 1.0105252066420185\n",
      "batch idx: 45 done; train loss: 0.4129799008369446; avg train loss: 0.9975350912984299\n",
      "batch idx: 46 done; train loss: 0.13770681619644165; avg train loss: 0.9792408726792386\n",
      "batch idx: 47 done; train loss: 0.6913709044456482; avg train loss: 0.9732435816743722\n",
      "batch idx: 48 done; train loss: 0.18025913834571838; avg train loss: 0.9570602256880731\n",
      "batch idx: 49 done; train loss: 0.5153849124908447; avg train loss: 0.9482267194241285\n",
      "batch idx: 50 done; train loss: 1.02512788772583; avg train loss: 0.9497345854692599\n",
      "batch idx: 51 done; train loss: 0.2128026932477951; avg train loss: 0.9355628183111548\n",
      "batch idx: 52 done; train loss: 1.0116784572601318; avg train loss: 0.9369989624422677\n",
      "batch idx: 53 done; train loss: 1.3298053741455078; avg train loss: 0.9442731552515868\n",
      "batch idx: 54 done; train loss: 1.5597119331359863; avg train loss: 0.9554629512131214\n",
      "batch idx: 55 done; train loss: 2.2890751361846924; avg train loss: 0.9792774545161852\n",
      "batch idx: 56 done; train loss: 1.307037353515625; avg train loss: 0.9850276281828421\n",
      "batch idx: 57 done; train loss: 0.06386622786521912; avg train loss: 0.9691455350739175\n",
      "batch idx: 58 done; train loss: 2.010754108428955; avg train loss: 0.9867999176731554\n",
      "batch idx: 59 done; train loss: 1.9607285261154175; avg train loss: 1.003032061147193\n",
      "batch idx: 60 done; train loss: 0.8823797702789307; avg train loss: 1.0010541547395166\n",
      "batch idx: 61 done; train loss: 0.4679373502731323; avg train loss: 0.9924554966029621\n",
      "batch idx: 62 done; train loss: 0.17867809534072876; avg train loss: 0.979538394995625\n",
      "batch idx: 63 done; train loss: 0.20088963210582733; avg train loss: 0.967372008075472\n",
      "batch idx: 64 done; train loss: 0.5558024048805237; avg train loss: 0.9610401680263189\n",
      "batch idx: 65 done; train loss: 0.08225077390670776; avg train loss: 0.9477251772063248\n",
      "batch idx: 66 done; train loss: 0.05015747994184494; avg train loss: 0.9343286444113326\n",
      "batch idx: 67 done; train loss: 0.21601754426956177; avg train loss: 0.923765245879836\n",
      "batch idx: 68 done; train loss: 0.6847396492958069; avg train loss: 0.920301106798908\n",
      "batch idx: 69 done; train loss: 1.4892455339431763; avg train loss: 0.9284288843295404\n",
      "batch idx: 70 done; train loss: 0.060945454984903336; avg train loss: 0.9162108078598976\n",
      "batch idx: 71 done; train loss: 1.4035511016845703; avg train loss: 0.9229794230519069\n",
      "batch idx: 72 done; train loss: 0.04011010751128197; avg train loss: 0.9108853228390217\n",
      "batch idx: 73 done; train loss: 0.1072557345032692; avg train loss: 0.9000254635371872\n",
      "batch idx: 74 done; train loss: 0.02543036825954914; avg train loss: 0.888364195600152\n",
      "batch idx: 75 done; train loss: 0.6991655230522156; avg train loss: 0.885874739382416\n",
      "batch idx: 76 done; train loss: 0.009876196272671223; avg train loss: 0.8744981349264453\n",
      "batch idx: 77 done; train loss: 0.04717676714062691; avg train loss: 0.863891450724063\n",
      "batch idx: 78 done; train loss: 0.009139250963926315; avg train loss: 0.8530718026258335\n",
      "batch idx: 79 done; train loss: 4.5415425300598145; avg train loss: 0.8991776867187582\n",
      "batch idx: 80 done; train loss: 0.7867704629898071; avg train loss: 0.8977899432159316\n",
      "batch idx: 81 done; train loss: 3.0137546062469482; avg train loss: 0.9235943903260659\n",
      "batch idx: 82 done; train loss: 2.9360806941986084; avg train loss: 0.9478412132642894\n",
      "batch idx: 83 done; train loss: 0.008952478878200054; avg train loss: 0.9366639664263597\n",
      "batch idx: 84 done; train loss: 0.04921668395400047; avg train loss: 0.9262234101619791\n",
      "batch idx: 85 done; train loss: 0.17486204206943512; avg train loss: 0.9174866500678797\n",
      "batch idx: 86 done; train loss: 0.6299454569816589; avg train loss: 0.9141815788829806\n",
      "batch idx: 87 done; train loss: 0.360519140958786; avg train loss: 0.9078899602702056\n",
      "batch idx: 88 done; train loss: 0.7537746429443359; avg train loss: 0.9061583274912633\n",
      "batch idx: 89 done; train loss: 0.01490551047027111; avg train loss: 0.8962555184132524\n",
      "batch idx: 90 done; train loss: 0.007050397340208292; avg train loss: 0.8864840335662958\n",
      "batch idx: 91 done; train loss: 0.5613623857498169; avg train loss: 0.8829501026117689\n",
      "batch idx: 92 done; train loss: 1.4813895225524902; avg train loss: 0.8893849350842497\n",
      "batch idx: 93 done; train loss: 0.3481104075908661; avg train loss: 0.8836266954300648\n",
      "batch idx: 94 done; train loss: 0.0036695541348308325; avg train loss: 0.8743639886795886\n",
      "batch idx: 95 done; train loss: 0.006703271064907312; avg train loss: 0.8653258562044357\n",
      "batch idx: 96 done; train loss: 4.3234944343566895; avg train loss: 0.9009770786596135\n",
      "batch idx: 97 done; train loss: 4.803042411804199; avg train loss: 0.9407940718549664\n",
      "batch idx: 98 done; train loss: 0.001342587056569755; avg train loss: 0.9313046629176089\n",
      "batch idx: 99 done; train loss: 5.360732078552246; avg train loss: 0.9755989370739553\n",
      "batch idx: 100 done; train loss: 0.6907821297645569; avg train loss: 0.9727789686847533\n",
      "batch idx: 101 done; train loss: 0.0015657796757295728; avg train loss: 0.9632572707532924\n",
      "batch idx: 102 done; train loss: 1.9443591833114624; avg train loss: 0.9727825320402649\n",
      "batch idx: 103 done; train loss: 0.41624829173088074; avg train loss: 0.9674312412680592\n",
      "batch idx: 104 done; train loss: 0.0015882747247815132; avg train loss: 0.9582327368247899\n",
      "batch idx: 105 done; train loss: 4.89531946182251; avg train loss: 0.9953750644191081\n",
      "batch idx: 106 done; train loss: 2.1754724979400635; avg train loss: 1.0064040123959395\n",
      "batch idx: 107 done; train loss: 0.02688201330602169; avg train loss: 0.9973343642562179\n",
      "batch idx: 108 done; train loss: 0.025759320706129074; avg train loss: 0.9884208317465841\n",
      "batch idx: 109 done; train loss: 0.012787722051143646; avg train loss: 0.9795514398402619\n",
      "batch idx: 110 done; train loss: 1.8918418884277344; avg train loss: 0.9877702727104193\n",
      "batch idx: 111 done; train loss: 0.06165540590882301; avg train loss: 0.9795013899711194\n",
      "batch idx: 112 done; train loss: 0.005619680043309927; avg train loss: 0.9708829677593688\n",
      "batch idx: 113 done; train loss: 0.6467000246047974; avg train loss: 0.9680392577316972\n",
      "batch idx: 114 done; train loss: 1.4568681716918945; avg train loss: 0.9722899439400466\n",
      "batch idx: 115 done; train loss: 0.05092928186058998; avg train loss: 0.9643471796117755\n",
      "batch idx: 116 done; train loss: 0.007793850731104612; avg train loss: 0.9561715101341629\n",
      "batch idx: 117 done; train loss: 1.3525958061218262; avg train loss: 0.9595310380662618\n",
      "batch idx: 118 done; train loss: 0.902265191078186; avg train loss: 0.9590498124613199\n",
      "batch idx: 119 done; train loss: 0.03226855769753456; avg train loss: 0.9513266353382884\n",
      "batch idx: 120 done; train loss: 0.0054956781677901745; avg train loss: 0.9435098505682843\n",
      "batch idx: 121 done; train loss: 0.12207002937793732; avg train loss: 0.9367767372798388\n",
      "batch idx: 122 done; train loss: 3.4352641105651855; avg train loss: 0.9570896427537035\n",
      "batch idx: 123 done; train loss: 1.5721893310546875; avg train loss: 0.9620501241109695\n",
      "batch idx: 124 done; train loss: 2.70745587348938; avg train loss: 0.9760133701059968\n",
      "batch idx: 125 done; train loss: 1.8183621168136597; avg train loss: 0.9826986776195497\n",
      "batch idx: 126 done; train loss: 0.008274322375655174; avg train loss: 0.9750260449010938\n",
      "batch idx: 127 done; train loss: 1.0793944597244263; avg train loss: 0.975841423141901\n",
      "batch idx: 128 done; train loss: 0.016203029081225395; avg train loss: 0.9684023658236012\n",
      "batch idx: 129 done; train loss: 0.29291629791259766; avg train loss: 0.9632063191473628\n",
      "batch idx: 130 done; train loss: 0.16636395454406738; avg train loss: 0.9571235530053528\n",
      "batch idx: 131 done; train loss: 1.752692699432373; avg train loss: 0.9631505919934363\n",
      "batch idx: 132 done; train loss: 0.0012499623699113727; avg train loss: 0.9559182564323572\n",
      "batch idx: 133 done; train loss: 0.08466097712516785; avg train loss: 0.9494163364375274\n",
      "batch idx: 134 done; train loss: 5.5309576988220215; avg train loss: 0.9833536798625977\n",
      "batch idx: 135 done; train loss: 0.046653080731630325; avg train loss: 0.976466175457223\n",
      "batch idx: 136 done; train loss: 3.9647722244262695; avg train loss: 0.9982786283694058\n",
      "batch idx: 137 done; train loss: 0.07498544454574585; avg train loss: 0.991588098051843\n",
      "batch idx: 138 done; train loss: 0.7444577217102051; avg train loss: 0.9898101816752846\n",
      "batch idx: 139 done; train loss: 0.23959895968437195; avg train loss: 0.9844515300896352\n",
      "batch idx: 140 done; train loss: 0.016430318355560303; avg train loss: 0.9775861314248545\n",
      "batch idx: 141 done; train loss: 0.03223150596022606; avg train loss: 0.9709287044849627\n",
      "batch idx: 142 done; train loss: 0.05646998807787895; avg train loss: 0.9645338882863118\n",
      "batch idx: 143 done; train loss: 0.0050063966773450375; avg train loss: 0.9578705029279162\n",
      "batch idx: 144 done; train loss: 2.226963758468628; avg train loss: 0.9666228702075073\n",
      "batch idx: 145 done; train loss: 0.7083003520965576; avg train loss: 0.9648535378916788\n",
      "batch idx: 146 done; train loss: 0.0508790947496891; avg train loss: 0.9586360246730259\n",
      "batch idx: 147 done; train loss: 0.11984603852033615; avg train loss: 0.9529685247665888\n",
      "batch idx: 148 done; train loss: 0.03623323515057564; avg train loss: 0.9468159389302397\n",
      "batch idx: 149 done; train loss: 4.079107284545898; avg train loss: 0.9676978812343441\n",
      "batch idx: 150 done; train loss: 0.026125097647309303; avg train loss: 0.9614622998860856\n",
      "batch idx: 151 done; train loss: 0.0146742332726717; avg train loss: 0.9552334310267868\n",
      "batch idx: 152 done; train loss: 1.9891597032546997; avg train loss: 0.961991119080564\n",
      "batch idx: 153 done; train loss: 5.066344738006592; avg train loss: 0.9886427659567071\n",
      "batch idx: 154 done; train loss: 4.056647777557373; avg train loss: 1.0084363466767114\n",
      "batch idx: 155 done; train loss: 0.00618330342695117; avg train loss: 1.0020116476815206\n",
      "batch idx: 156 done; train loss: 0.0039429315365850925; avg train loss: 0.9956545221009796\n",
      "batch idx: 157 done; train loss: 0.8987551927566528; avg train loss: 0.9950412352063952\n",
      "batch idx: 158 done; train loss: 6.107823371887207; avg train loss: 1.027197097701243\n",
      "batch idx: 159 done; train loss: 0.07000569999217987; avg train loss: 1.0212146514655616\n",
      "batch idx: 160 done; train loss: 0.020579738542437553; avg train loss: 1.0149995277828092\n",
      "batch idx: 161 done; train loss: 0.10620349645614624; avg train loss: 1.0093896757375829\n",
      "batch idx: 162 done; train loss: 0.013881918042898178; avg train loss: 1.0032822661811738\n",
      "Current learning rate: 0.0005\n",
      "\n",
      "Epoch 3/10\n",
      "Train - Loss: 1.1062, Acc: 0.6748, F1: 0.7415, Recall: 0.9048, AUC: 0.7577\n",
      "Val   - Loss: 1.7563, Acc: 0.5273, F1: 0.5357, Recall: 0.8824, AUC: 0.6486\n",
      "batch idx: 0 done; train loss: 3.7669243812561035; avg train loss: 3.7669243812561035\n",
      "batch idx: 1 done; train loss: 0.004382767248898745; avg train loss: 1.8856535742525011\n",
      "batch idx: 2 done; train loss: 3.1112844944000244; avg train loss: 2.2941972143016756\n",
      "batch idx: 3 done; train loss: 0.008745936676859856; avg train loss: 1.7228343948954716\n",
      "batch idx: 4 done; train loss: 3.6778881549835205; avg train loss: 2.1138451469130812\n",
      "batch idx: 5 done; train loss: 0.1626424789428711; avg train loss: 1.7886447022513796\n",
      "batch idx: 6 done; train loss: 0.1027224063873291; avg train loss: 1.5477986599850868\n",
      "batch idx: 7 done; train loss: 0.012424921616911888; avg train loss: 1.355876942689065\n",
      "batch idx: 8 done; train loss: 3.1038644313812256; avg train loss: 1.5500977747659717\n",
      "batch idx: 9 done; train loss: 0.9285380840301514; avg train loss: 1.4879418056923897\n",
      "batch idx: 10 done; train loss: 0.008886078372597694; avg train loss: 1.353482194117863\n",
      "batch idx: 11 done; train loss: 0.004490292631089687; avg train loss: 1.2410662023272987\n",
      "batch idx: 12 done; train loss: 3.6403141021728516; avg train loss: 1.4256237330846488\n",
      "batch idx: 13 done; train loss: 0.029338965192437172; avg train loss: 1.3258891068066336\n",
      "batch idx: 14 done; train loss: 0.009210360236465931; avg train loss: 1.2381105237019558\n",
      "batch idx: 15 done; train loss: 2.637908458709717; avg train loss: 1.325597894639941\n",
      "batch idx: 16 done; train loss: 3.019122838973999; avg train loss: 1.4252170090125327\n",
      "batch idx: 17 done; train loss: 0.024369101971387863; avg train loss: 1.3473921252880245\n",
      "batch idx: 18 done; train loss: 0.04116030037403107; avg train loss: 1.2786430818714987\n",
      "batch idx: 19 done; train loss: 0.2318144291639328; avg train loss: 1.2263016492361203\n",
      "batch idx: 20 done; train loss: 0.004291966557502747; avg train loss: 1.16811071196571\n",
      "batch idx: 21 done; train loss: 3.664764165878296; avg train loss: 1.2815949598708274\n",
      "batch idx: 22 done; train loss: 5.54301643371582; avg train loss: 1.4668741543858272\n",
      "batch idx: 23 done; train loss: 0.3843810260295868; avg train loss: 1.4217702740376505\n",
      "batch idx: 24 done; train loss: 0.0107884481549263; avg train loss: 1.3653310010023416\n",
      "batch idx: 25 done; train loss: 0.8042521476745605; avg train loss: 1.3437510451051191\n",
      "batch idx: 26 done; train loss: 2.1986546516418457; avg train loss: 1.3754141416435164\n",
      "batch idx: 27 done; train loss: 0.004083272535353899; avg train loss: 1.3264380391753678\n",
      "batch idx: 28 done; train loss: 0.0039049338083714247; avg train loss: 1.2808334493351266\n",
      "batch idx: 29 done; train loss: 2.5238842964172363; avg train loss: 1.3222684775711968\n",
      "batch idx: 30 done; train loss: 3.6385180950164795; avg train loss: 1.396986207166206\n",
      "batch idx: 31 done; train loss: 3.542937994003296; avg train loss: 1.464047200504865\n",
      "batch idx: 32 done; train loss: 0.04606059193611145; avg train loss: 1.421077909336115\n",
      "batch idx: 33 done; train loss: 0.5394126176834106; avg train loss: 1.3951465772286824\n",
      "batch idx: 34 done; train loss: 0.020534658804535866; avg train loss: 1.3558719509879926\n",
      "batch idx: 35 done; train loss: 0.09031490236520767; avg train loss: 1.3207175885262485\n",
      "batch idx: 36 done; train loss: 0.046181369572877884; avg train loss: 1.286270663689671\n",
      "batch idx: 37 done; train loss: 0.37990817427635193; avg train loss: 1.2624190192314257\n",
      "batch idx: 38 done; train loss: 0.009940052404999733; avg train loss: 1.230304173928184\n",
      "batch idx: 39 done; train loss: 0.018108753487467766; avg train loss: 1.1999992884171662\n",
      "batch idx: 40 done; train loss: 2.2065937519073486; avg train loss: 1.2245503728925364\n",
      "batch idx: 41 done; train loss: 0.03321424126625061; avg train loss: 1.1961852269014344\n",
      "batch idx: 42 done; train loss: 1.1519538164138794; avg train loss: 1.1951565894482354\n",
      "batch idx: 43 done; train loss: 0.34823623299598694; avg train loss: 1.1759083995288662\n",
      "batch idx: 44 done; train loss: 0.21967771649360657; avg train loss: 1.1546588287947492\n",
      "batch idx: 45 done; train loss: 0.8574173450469971; avg train loss: 1.1481970574089286\n",
      "batch idx: 46 done; train loss: 0.04262026399374008; avg train loss: 1.124674146910733\n",
      "batch idx: 47 done; train loss: 0.9064039587974548; avg train loss: 1.1201268513250398\n",
      "batch idx: 48 done; train loss: 2.2637386322021484; avg train loss: 1.1434658672613072\n",
      "batch idx: 49 done; train loss: 0.020507797598838806; avg train loss: 1.121006705868058\n",
      "batch idx: 50 done; train loss: 2.4197397232055664; avg train loss: 1.1464720591491855\n",
      "batch idx: 51 done; train loss: 0.004639219027012587; avg train loss: 1.1245137353006822\n",
      "batch idx: 52 done; train loss: 0.0224797111004591; avg train loss: 1.1037206405044515\n",
      "batch idx: 53 done; train loss: 0.02335107885301113; avg train loss: 1.0837137967701655\n",
      "batch idx: 54 done; train loss: 0.06921878457069397; avg train loss: 1.0652684329119935\n",
      "batch idx: 55 done; train loss: 2.6618356704711914; avg train loss: 1.093778562154122\n",
      "batch idx: 56 done; train loss: 0.03505424037575722; avg train loss: 1.0752044512457295\n",
      "batch idx: 57 done; train loss: 2.650378942489624; avg train loss: 1.102362632129245\n",
      "batch idx: 58 done; train loss: 0.0024623333010822535; avg train loss: 1.0837202541830049\n",
      "batch idx: 59 done; train loss: 0.012267730198800564; avg train loss: 1.0658627121166016\n",
      "batch idx: 60 done; train loss: 0.0484808087348938; avg train loss: 1.049184320257885\n",
      "batch idx: 61 done; train loss: 1.541565179824829; avg train loss: 1.0571259470250938\n",
      "batch idx: 62 done; train loss: 3.241157293319702; avg train loss: 1.0917931112519923\n",
      "batch idx: 63 done; train loss: 1.0564543008804321; avg train loss: 1.0912409423399367\n",
      "batch idx: 64 done; train loss: 3.4647183418273926; avg train loss: 1.1277559792551284\n",
      "batch idx: 65 done; train loss: 0.35754844546318054; avg train loss: 1.1160861681370686\n",
      "batch idx: 66 done; train loss: 2.313986301422119; avg train loss: 1.1339652746040096\n",
      "batch idx: 67 done; train loss: 0.09595426917076111; avg train loss: 1.11870040687705\n",
      "batch idx: 68 done; train loss: 0.05209345370531082; avg train loss: 1.1032423350919525\n",
      "batch idx: 69 done; train loss: 0.3273595869541168; avg train loss: 1.0921582958328404\n",
      "batch idx: 70 done; train loss: 0.09206874668598175; avg train loss: 1.0780725275349974\n",
      "batch idx: 71 done; train loss: 0.13309398293495178; avg train loss: 1.0649478255266633\n",
      "batch idx: 72 done; train loss: 0.0035296303685754538; avg train loss: 1.0504078502505252\n",
      "batch idx: 73 done; train loss: 4.557112216949463; avg train loss: 1.0977957470978081\n",
      "batch idx: 74 done; train loss: 1.845897912979126; avg train loss: 1.1077704426428925\n",
      "batch idx: 75 done; train loss: 4.892960071563721; avg train loss: 1.1575755693392191\n",
      "batch idx: 76 done; train loss: 2.2916879653930664; avg train loss: 1.1723043017555028\n",
      "batch idx: 77 done; train loss: 0.010136435739696026; avg train loss: 1.1574047137296593\n",
      "batch idx: 78 done; train loss: 1.7085318565368652; avg train loss: 1.1643810066765858\n",
      "batch idx: 79 done; train loss: 0.0022017541341483593; avg train loss: 1.1498537660198054\n",
      "batch idx: 80 done; train loss: 0.00246649538166821; avg train loss: 1.1356884910736555\n",
      "batch idx: 81 done; train loss: 4.492703437805176; avg train loss: 1.176627697741113\n",
      "batch idx: 82 done; train loss: 0.007658998016268015; avg train loss: 1.1625437375034644\n",
      "batch idx: 83 done; train loss: 0.003630714723840356; avg train loss: 1.1487471538989449\n",
      "batch idx: 84 done; train loss: 0.019334428012371063; avg train loss: 1.135459945359103\n",
      "batch idx: 85 done; train loss: 1.095404863357544; avg train loss: 1.134994188591643\n",
      "batch idx: 86 done; train loss: 0.012945891357958317; avg train loss: 1.122097081726888\n",
      "batch idx: 87 done; train loss: 0.6386380195617676; avg train loss: 1.1166032287477388\n",
      "batch idx: 88 done; train loss: 0.013605482876300812; avg train loss: 1.1042099956480598\n",
      "batch idx: 89 done; train loss: 3.0586211681365967; avg train loss: 1.1259256753423768\n",
      "batch idx: 90 done; train loss: 2.3283486366271973; avg train loss: 1.1391391144773748\n",
      "batch idx: 91 done; train loss: 0.23130999505519867; avg train loss: 1.1292714066575686\n",
      "batch idx: 92 done; train loss: 0.82068932056427; avg train loss: 1.1259533197103289\n",
      "batch idx: 93 done; train loss: 4.005532741546631; avg train loss: 1.1565871433468853\n",
      "batch idx: 94 done; train loss: 1.1675478219985962; avg train loss: 1.1567025189116402\n",
      "batch idx: 95 done; train loss: 1.6887469291687012; avg train loss: 1.1622446481851512\n",
      "batch idx: 96 done; train loss: 5.572631359100342; avg train loss: 1.2077125524213903\n",
      "batch idx: 97 done; train loss: 2.3569693565368652; avg train loss: 1.2194396626674666\n",
      "batch idx: 98 done; train loss: 0.09426885098218918; avg train loss: 1.2080743009332717\n",
      "batch idx: 99 done; train loss: 0.2966676950454712; avg train loss: 1.1989602348743937\n",
      "batch idx: 100 done; train loss: 2.7325055599212646; avg train loss: 1.2141438519540657\n",
      "batch idx: 101 done; train loss: 2.4774162769317627; avg train loss: 1.226528875728357\n",
      "batch idx: 102 done; train loss: 1.2346447706222534; avg train loss: 1.2266076708244142\n",
      "batch idx: 103 done; train loss: 0.26234543323516846; avg train loss: 1.2173359185399022\n",
      "batch idx: 104 done; train loss: 0.4544066786766052; avg train loss: 1.2100699257792993\n",
      "batch idx: 105 done; train loss: 0.4543394446372986; avg train loss: 1.202940392938337\n",
      "batch idx: 106 done; train loss: 0.4061281681060791; avg train loss: 1.1954935497156056\n",
      "batch idx: 107 done; train loss: 2.013153314590454; avg train loss: 1.203064473464447\n",
      "batch idx: 108 done; train loss: 1.5911126136779785; avg train loss: 1.206624548145305\n",
      "batch idx: 109 done; train loss: 0.14605344831943512; avg train loss: 1.1969829926923425\n",
      "batch idx: 110 done; train loss: 0.11608713120222092; avg train loss: 1.1872451921383775\n",
      "batch idx: 111 done; train loss: 2.3191096782684326; avg train loss: 1.1973511250502529\n",
      "batch idx: 112 done; train loss: 0.07854630053043365; avg train loss: 1.1874501973996352\n",
      "batch idx: 113 done; train loss: 1.064672589302063; avg train loss: 1.1863732008373757\n",
      "batch idx: 114 done; train loss: 0.07129975408315659; avg train loss: 1.1766769099960346\n",
      "batch idx: 115 done; train loss: 0.141152486205101; avg train loss: 1.1677499753081817\n",
      "batch idx: 116 done; train loss: 3.2621188163757324; avg train loss: 1.1856505636933745\n",
      "batch idx: 117 done; train loss: 1.9654662609100342; avg train loss: 1.1922591712969055\n",
      "batch idx: 118 done; train loss: 0.04604465141892433; avg train loss: 1.1826271165080149\n",
      "batch idx: 119 done; train loss: 1.5768024921417236; avg train loss: 1.1859119113049625\n",
      "batch idx: 120 done; train loss: 0.7127075791358948; avg train loss: 1.1820011317002594\n",
      "batch idx: 121 done; train loss: 0.05541365593671799; avg train loss: 1.1727668081284273\n",
      "batch idx: 122 done; train loss: 2.3434338569641113; avg train loss: 1.182284426411644\n",
      "batch idx: 123 done; train loss: 0.3510530889034271; avg train loss: 1.175580947883352\n",
      "batch idx: 124 done; train loss: 2.9807612895965576; avg train loss: 1.1900223906170577\n",
      "batch idx: 125 done; train loss: 0.06068935617804527; avg train loss: 1.1810594300262718\n",
      "batch idx: 126 done; train loss: 0.014791934750974178; avg train loss: 1.1718762214020568\n",
      "batch idx: 127 done; train loss: 0.753412127494812; avg train loss: 1.1686069706684066\n",
      "batch idx: 128 done; train loss: 0.4959847331047058; avg train loss: 1.1633928447958197\n",
      "batch idx: 129 done; train loss: 0.2288987934589386; avg train loss: 1.1562044290163054\n",
      "batch idx: 130 done; train loss: 0.02664153464138508; avg train loss: 1.147581811501993\n",
      "batch idx: 131 done; train loss: 3.5132243633270264; avg train loss: 1.165503345985516\n",
      "batch idx: 132 done; train loss: 0.00985034555196762; avg train loss: 1.1568142256815042\n",
      "batch idx: 133 done; train loss: 0.026820622384548187; avg train loss: 1.1483814375971986\n",
      "batch idx: 134 done; train loss: 0.015071212314069271; avg train loss: 1.1399865470395458\n",
      "batch idx: 135 done; train loss: 2.3090810775756836; avg train loss: 1.1485828303523116\n",
      "batch idx: 136 done; train loss: 2.76233172416687; avg train loss: 1.1603620193582571\n",
      "batch idx: 137 done; train loss: 0.021898960694670677; avg train loss: 1.1521122870491007\n",
      "batch idx: 138 done; train loss: 0.01730743981897831; avg train loss: 1.1439482233999632\n",
      "batch idx: 139 done; train loss: 0.11197773367166519; avg train loss: 1.1365770056161897\n",
      "batch idx: 140 done; train loss: 0.2564184069633484; avg train loss: 1.1303347460512758\n",
      "batch idx: 141 done; train loss: 0.034835752099752426; avg train loss: 1.12261996440373\n",
      "batch idx: 142 done; train loss: 1.8358200788497925; avg train loss: 1.1276073777914646\n",
      "batch idx: 143 done; train loss: 1.4489847421646118; avg train loss: 1.129839165044056\n",
      "batch idx: 144 done; train loss: 3.7233779430389404; avg train loss: 1.1477256393750552\n",
      "batch idx: 145 done; train loss: 1.5326178073883057; avg train loss: 1.1503618871011734\n",
      "batch idx: 146 done; train loss: 1.406873345375061; avg train loss: 1.1521068630077984\n",
      "batch idx: 147 done; train loss: 1.0417752265930176; avg train loss: 1.1513613789779689\n",
      "batch idx: 148 done; train loss: 0.3586515188217163; avg train loss: 1.1460411785742355\n",
      "batch idx: 149 done; train loss: 0.024623503908514977; avg train loss: 1.1385650607431308\n",
      "batch idx: 150 done; train loss: 1.0815515518188477; avg train loss: 1.1381874878363474\n",
      "batch idx: 151 done; train loss: 0.10591326653957367; avg train loss: 1.1313962100646582\n",
      "batch idx: 152 done; train loss: 0.01295860018581152; avg train loss: 1.124086160326888\n",
      "batch idx: 153 done; train loss: 0.9583562016487122; avg train loss: 1.1230099917640426\n",
      "batch idx: 154 done; train loss: 0.030935561284422874; avg train loss: 1.1159643502770773\n",
      "batch idx: 155 done; train loss: 0.2328943908214569; avg train loss: 1.110303645408772\n",
      "batch idx: 156 done; train loss: 0.49911701679229736; avg train loss: 1.1064107369462468\n",
      "batch idx: 157 done; train loss: 0.04193713143467903; avg train loss: 1.099673562227819\n",
      "batch idx: 158 done; train loss: 1.1212658882141113; avg train loss: 1.0998093630201857\n",
      "batch idx: 159 done; train loss: 0.9732366800308228; avg train loss: 1.0990182837515021\n",
      "batch idx: 160 done; train loss: 1.2057791948318481; avg train loss: 1.0996813950004485\n",
      "batch idx: 161 done; train loss: 0.39159753918647766; avg train loss: 1.0953105070015967\n",
      "batch idx: 162 done; train loss: 0.260123074054718; avg train loss: 1.0901866577197141\n",
      "Current learning rate: 5e-05\n",
      "\n",
      "Epoch 4/10\n",
      "Train - Loss: 0.8939, Acc: 0.6012, F1: 0.6701, Recall: 0.8250, AUC: 0.6527\n",
      "Val   - Loss: 1.1795, Acc: 0.4727, F1: 0.5246, Recall: 0.9412, AUC: 0.7523\n",
      "batch idx: 0 done; train loss: 2.2977101802825928; avg train loss: 2.2977101802825928\n",
      "batch idx: 1 done; train loss: 1.768065333366394; avg train loss: 2.0328877568244934\n",
      "batch idx: 2 done; train loss: 3.5891106128692627; avg train loss: 2.5516287088394165\n",
      "batch idx: 3 done; train loss: 0.3376532196998596; avg train loss: 1.9981348365545273\n",
      "batch idx: 4 done; train loss: 2.0140914916992188; avg train loss: 2.0013261675834655\n",
      "batch idx: 5 done; train loss: 1.1224654912948608; avg train loss: 1.8548493882020314\n",
      "batch idx: 6 done; train loss: 0.12237965315580368; avg train loss: 1.6073537117668562\n",
      "batch idx: 7 done; train loss: 1.8492916822433472; avg train loss: 1.6375959580764174\n",
      "batch idx: 8 done; train loss: 0.03905193507671356; avg train loss: 1.4599799555208948\n",
      "batch idx: 9 done; train loss: 0.03423137590289116; avg train loss: 1.3174050975590945\n",
      "batch idx: 10 done; train loss: 0.35619989037513733; avg train loss: 1.2300228059969165\n",
      "batch idx: 11 done; train loss: 1.4937410354614258; avg train loss: 1.251999325118959\n",
      "batch idx: 12 done; train loss: 0.16657781600952148; avg train loss: 1.1685053628797715\n",
      "batch idx: 13 done; train loss: 1.0666062831878662; avg train loss: 1.1612268571874924\n",
      "batch idx: 14 done; train loss: 2.69108247756958; avg train loss: 1.2632172318796318\n",
      "batch idx: 15 done; train loss: 0.22168880701065063; avg train loss: 1.1981217053253204\n",
      "batch idx: 16 done; train loss: 0.6855709552764893; avg train loss: 1.1679716612048008\n",
      "batch idx: 17 done; train loss: 0.36117103695869446; avg train loss: 1.1231494043022394\n",
      "batch idx: 18 done; train loss: 0.01818905957043171; avg train loss: 1.064993596684776\n",
      "batch idx: 19 done; train loss: 0.15245118737220764; avg train loss: 1.0193664762191474\n",
      "batch idx: 20 done; train loss: 2.287327289581299; avg train loss: 1.079745562569726\n",
      "batch idx: 21 done; train loss: 0.644582211971283; avg train loss: 1.0599654102697968\n",
      "batch idx: 22 done; train loss: 0.3511737585067749; avg train loss: 1.0291483819322742\n",
      "batch idx: 23 done; train loss: 0.3821154534816742; avg train loss: 1.0021886765801657\n",
      "batch idx: 24 done; train loss: 0.34591981768608093; avg train loss: 0.9759379222244025\n",
      "batch idx: 25 done; train loss: 1.1494277715682983; avg train loss: 0.9826106087376292\n",
      "batch idx: 26 done; train loss: 0.017660275101661682; avg train loss: 0.9468717074918526\n",
      "batch idx: 27 done; train loss: 0.2745291590690613; avg train loss: 0.9228594736196101\n",
      "batch idx: 28 done; train loss: 0.0910959541797638; avg train loss: 0.8941779729492706\n",
      "batch idx: 29 done; train loss: 0.017508355900645256; avg train loss: 0.864955652380983\n",
      "batch idx: 30 done; train loss: 0.16788294911384583; avg train loss: 0.8424694361465592\n",
      "batch idx: 31 done; train loss: 0.1292392462491989; avg train loss: 0.8201809927122667\n",
      "batch idx: 32 done; train loss: 0.591407835483551; avg train loss: 0.813248472796245\n",
      "batch idx: 33 done; train loss: 2.6331570148468018; avg train loss: 0.8667751946212614\n",
      "batch idx: 34 done; train loss: 0.23051653802394867; avg train loss: 0.8485963758613382\n",
      "batch idx: 35 done; train loss: 0.3616696000099182; avg train loss: 0.8350706320876876\n",
      "batch idx: 36 done; train loss: 1.839958667755127; avg train loss: 0.8622297681868076\n",
      "batch idx: 37 done; train loss: 0.012551599182188511; avg train loss: 0.8398698163708966\n",
      "batch idx: 38 done; train loss: 0.23667144775390625; avg train loss: 0.8244031915345635\n",
      "batch idx: 39 done; train loss: 0.113089919090271; avg train loss: 0.8066203597234562\n",
      "batch idx: 40 done; train loss: 2.5653326511383057; avg train loss: 0.8495157814652818\n",
      "batch idx: 41 done; train loss: 0.05116274580359459; avg train loss: 0.8305073758542892\n",
      "batch idx: 42 done; train loss: 0.49857616424560547; avg train loss: 0.8227880453517618\n",
      "batch idx: 43 done; train loss: 1.2302361726760864; avg train loss: 0.8320482300636782\n",
      "batch idx: 44 done; train loss: 0.2592822313308716; avg train loss: 0.8193200967585047\n",
      "batch idx: 45 done; train loss: 0.658100962638855; avg train loss: 0.8158153329732949\n",
      "batch idx: 46 done; train loss: 0.13451354205608368; avg train loss: 0.8013195501878223\n",
      "batch idx: 47 done; train loss: 0.20600563287734985; avg train loss: 0.7889171769105209\n",
      "batch idx: 48 done; train loss: 0.13392741978168488; avg train loss: 0.7755500390099324\n",
      "batch idx: 49 done; train loss: 0.651978075504303; avg train loss: 0.7730785997398197\n",
      "batch idx: 50 done; train loss: 0.3380735218524933; avg train loss: 0.7645490884086957\n",
      "batch idx: 51 done; train loss: 0.25381338596343994; avg train loss: 0.7547272479770561\n",
      "batch idx: 52 done; train loss: 1.519638180732727; avg train loss: 0.7691595297271632\n",
      "batch idx: 53 done; train loss: 0.012465422973036766; avg train loss: 0.755146675898383\n",
      "batch idx: 54 done; train loss: 0.33892253041267395; avg train loss: 0.7475789641622793\n",
      "batch idx: 55 done; train loss: 2.588170289993286; avg train loss: 0.7804466664092615\n",
      "batch idx: 56 done; train loss: 0.6312918663024902; avg train loss: 0.7778299155301953\n",
      "batch idx: 57 done; train loss: 0.04437896981835365; avg train loss: 0.7651842095696464\n",
      "batch idx: 58 done; train loss: 2.66815447807312; avg train loss: 0.7974379429341121\n",
      "batch idx: 59 done; train loss: 2.6310362815856934; avg train loss: 0.8279979152449717\n",
      "batch idx: 60 done; train loss: 0.016432076692581177; avg train loss: 0.8146935572359161\n",
      "batch idx: 61 done; train loss: 0.016464442014694214; avg train loss: 0.8018188940871868\n",
      "batch idx: 62 done; train loss: 0.13742756843566895; avg train loss: 0.7912730000292262\n",
      "batch idx: 63 done; train loss: 0.04270582273602486; avg train loss: 0.7795766378840199\n",
      "batch idx: 64 done; train loss: 0.8557425737380981; avg train loss: 0.7807484215125442\n",
      "batch idx: 65 done; train loss: 0.014272631146013737; avg train loss: 0.7691351519615361\n",
      "batch idx: 66 done; train loss: 0.7213069200515747; avg train loss: 0.7684212977539248\n",
      "batch idx: 67 done; train loss: 0.20237676799297333; avg train loss: 0.7600971134927343\n",
      "batch idx: 68 done; train loss: 0.06589262187480927; avg train loss: 0.7500361788316049\n",
      "batch idx: 69 done; train loss: 2.8965353965759277; avg train loss: 0.7807004533708095\n",
      "batch idx: 70 done; train loss: 0.6026899814605713; avg train loss: 0.7781932636255949\n",
      "batch idx: 71 done; train loss: 0.26873335242271423; avg train loss: 0.7711174315255549\n",
      "batch idx: 72 done; train loss: 0.21669705212116241; avg train loss: 0.7635226318076865\n",
      "batch idx: 73 done; train loss: 0.013661693781614304; avg train loss: 0.7533893758884153\n",
      "batch idx: 74 done; train loss: 0.590844452381134; avg train loss: 0.7512221102416515\n",
      "batch idx: 75 done; train loss: 0.09086020290851593; avg train loss: 0.7425331377767419\n",
      "batch idx: 76 done; train loss: 0.5919585824012756; avg train loss: 0.7405776240705669\n",
      "batch idx: 77 done; train loss: 1.0740033388137817; avg train loss: 0.744852312721121\n",
      "batch idx: 78 done; train loss: 0.15267807245254517; avg train loss: 0.7373564362620251\n",
      "batch idx: 79 done; train loss: 0.013207568787038326; avg train loss: 0.7283045754185877\n",
      "batch idx: 80 done; train loss: 0.09807301312685013; avg train loss: 0.7205239388470849\n",
      "batch idx: 81 done; train loss: 1.1065388917922974; avg train loss: 0.725231438273246\n",
      "batch idx: 82 done; train loss: 0.15866424143314362; avg train loss: 0.7184053274679435\n",
      "batch idx: 83 done; train loss: 0.3254973292350769; avg train loss: 0.7137278512985047\n",
      "batch idx: 84 done; train loss: 2.7345428466796875; avg train loss: 0.7375021453618127\n",
      "batch idx: 85 done; train loss: 0.5367875099182129; avg train loss: 0.7351682542520034\n",
      "batch idx: 86 done; train loss: 0.045573681592941284; avg train loss: 0.7272418798536233\n",
      "batch idx: 87 done; train loss: 0.10882928967475891; avg train loss: 0.7202144640561362\n",
      "batch idx: 88 done; train loss: 0.07655248790979385; avg train loss: 0.7129823070207841\n",
      "batch idx: 89 done; train loss: 0.488031268119812; avg train loss: 0.7104828510329955\n",
      "batch idx: 90 done; train loss: 0.2950246334075928; avg train loss: 0.705917376114035\n",
      "batch idx: 91 done; train loss: 0.926278293132782; avg train loss: 0.7083126034729345\n",
      "batch idx: 92 done; train loss: 2.3872873783111572; avg train loss: 0.726366095675496\n",
      "batch idx: 93 done; train loss: 1.0023736953735352; avg train loss: 0.7293023467361135\n",
      "batch idx: 94 done; train loss: 1.868493676185608; avg train loss: 0.7412938344145291\n",
      "batch idx: 95 done; train loss: 0.2866629660129547; avg train loss: 0.7365580962020127\n",
      "batch idx: 96 done; train loss: 0.16695165634155273; avg train loss: 0.7306858648632452\n",
      "batch idx: 97 done; train loss: 0.0848260298371315; avg train loss: 0.7240954583833868\n",
      "batch idx: 98 done; train loss: 2.7331736087799072; avg train loss: 0.7443891770742608\n",
      "batch idx: 99 done; train loss: 3.3122410774230957; avg train loss: 0.7700676960777492\n",
      "batch idx: 100 done; train loss: 0.6355059742927551; avg train loss: 0.7687354018026502\n",
      "batch idx: 101 done; train loss: 0.29571959376335144; avg train loss: 0.764097991919912\n",
      "batch idx: 102 done; train loss: 0.33573514223098755; avg train loss: 0.7599391293015729\n",
      "batch idx: 103 done; train loss: 2.6245744228363037; avg train loss: 0.7778683148163299\n",
      "batch idx: 104 done; train loss: 3.5994791984558105; avg train loss: 0.8047407994224202\n",
      "batch idx: 105 done; train loss: 0.19442370533943176; avg train loss: 0.7989830909876751\n",
      "batch idx: 106 done; train loss: 0.03252292051911354; avg train loss: 0.7918199118244175\n",
      "batch idx: 107 done; train loss: 2.617142677307129; avg train loss: 0.80872104854185\n",
      "batch idx: 108 done; train loss: 0.057394083589315414; avg train loss: 0.8018281406065056\n",
      "batch idx: 109 done; train loss: 0.02834140509366989; avg train loss: 0.7947964430109343\n",
      "batch idx: 110 done; train loss: 1.5574859380722046; avg train loss: 0.8016675195430178\n",
      "batch idx: 111 done; train loss: 2.0670788288116455; avg train loss: 0.8129658348043449\n",
      "batch idx: 112 done; train loss: 3.119230270385742; avg train loss: 0.8333752545882511\n",
      "batch idx: 113 done; train loss: 1.4366189241409302; avg train loss: 0.8386668657246781\n",
      "batch idx: 114 done; train loss: 2.7599966526031494; avg train loss: 0.8553740812627517\n",
      "batch idx: 115 done; train loss: 1.5616015195846558; avg train loss: 0.8614622488344923\n",
      "batch idx: 116 done; train loss: 0.014688212424516678; avg train loss: 0.8542248639079113\n",
      "batch idx: 117 done; train loss: 1.5599147081375122; avg train loss: 0.8602052863166367\n",
      "batch idx: 118 done; train loss: 3.5249128341674805; avg train loss: 0.8825977867187447\n",
      "batch idx: 119 done; train loss: 1.279797077178955; avg train loss: 0.8859077808059131\n",
      "batch idx: 120 done; train loss: 2.5671157836914062; avg train loss: 0.8998020618214957\n",
      "batch idx: 121 done; train loss: 2.2617485523223877; avg train loss: 0.9109655576452735\n",
      "batch idx: 122 done; train loss: 0.05932846665382385; avg train loss: 0.9040416788567252\n",
      "batch idx: 123 done; train loss: 1.4895235300064087; avg train loss: 0.9087633066885774\n",
      "batch idx: 124 done; train loss: 0.013927297666668892; avg train loss: 0.9016046186164022\n",
      "batch idx: 125 done; train loss: 0.6720961928367615; avg train loss: 0.8997831231737066\n",
      "batch idx: 126 done; train loss: 0.2841840088367462; avg train loss: 0.8949358860529432\n",
      "batch idx: 127 done; train loss: 0.1837621033191681; avg train loss: 0.8893798408753355\n",
      "batch idx: 128 done; train loss: 0.6570195555686951; avg train loss: 0.8875785983535786\n",
      "batch idx: 129 done; train loss: 2.7474241256713867; avg train loss: 0.9018851024098694\n",
      "batch idx: 130 done; train loss: 3.788667917251587; avg train loss: 0.9239216124468291\n",
      "batch idx: 131 done; train loss: 0.04572458565235138; avg train loss: 0.917268604668083\n",
      "batch idx: 132 done; train loss: 0.25662076473236084; avg train loss: 0.9123013276760852\n",
      "batch idx: 133 done; train loss: 2.181563138961792; avg train loss: 0.9217734307453814\n",
      "batch idx: 134 done; train loss: 0.09728268533945084; avg train loss: 0.9156660918905227\n",
      "batch idx: 135 done; train loss: 0.20108512043952942; avg train loss: 0.9104118200416184\n",
      "batch idx: 136 done; train loss: 0.027959860861301422; avg train loss: 0.9039705648651197\n",
      "batch idx: 137 done; train loss: 2.932880401611328; avg train loss: 0.9186728100589328\n",
      "batch idx: 138 done; train loss: 0.19038085639476776; avg train loss: 0.9134332996009172\n",
      "batch idx: 139 done; train loss: 0.0433158315718174; avg train loss: 0.9072181748292808\n",
      "batch idx: 140 done; train loss: 0.026015345007181168; avg train loss: 0.9009685093695496\n",
      "batch idx: 141 done; train loss: 2.0365922451019287; avg train loss: 0.908965859621186\n",
      "batch idx: 142 done; train loss: 0.044237326830625534; avg train loss: 0.902918806944329\n",
      "batch idx: 143 done; train loss: 1.9026936292648315; avg train loss: 0.9098616876548881\n",
      "batch idx: 144 done; train loss: 0.1371997892856598; avg train loss: 0.9045329849075141\n",
      "batch idx: 145 done; train loss: 1.1402702331542969; avg train loss: 0.9061476235941358\n",
      "batch idx: 146 done; train loss: 0.8822804689407349; avg train loss: 0.9059852619978542\n",
      "batch idx: 147 done; train loss: 0.010526491329073906; avg train loss: 0.8999348648987409\n",
      "batch idx: 148 done; train loss: 0.02335655316710472; avg train loss: 0.8940517889810788\n",
      "batch idx: 149 done; train loss: 1.2632925510406494; avg train loss: 0.896513394061476\n",
      "batch idx: 150 done; train loss: 0.15689457952976227; avg train loss: 0.8916152562168951\n",
      "batch idx: 151 done; train loss: 0.8732097744941711; avg train loss: 0.8914941675213509\n",
      "batch idx: 152 done; train loss: 0.1943448930978775; avg train loss: 0.8869376363159687\n",
      "batch idx: 153 done; train loss: 0.26672226190567017; avg train loss: 0.8829102637548628\n",
      "batch idx: 154 done; train loss: 0.031129714101552963; avg train loss: 0.8774149053700028\n",
      "batch idx: 155 done; train loss: 4.449726104736328; avg train loss: 0.9003143361351715\n",
      "batch idx: 156 done; train loss: 0.020377803593873978; avg train loss: 0.8947096448450996\n",
      "batch idx: 157 done; train loss: 1.0207781791687012; avg train loss: 0.8955075469610717\n",
      "batch idx: 158 done; train loss: 0.18103499710559845; avg train loss: 0.8910140089116663\n",
      "batch idx: 159 done; train loss: 0.09054087847471237; avg train loss: 0.8860110518464352\n",
      "batch idx: 160 done; train loss: 0.030290428549051285; avg train loss: 0.8806960169191224\n",
      "batch idx: 161 done; train loss: 0.4232092797756195; avg train loss: 0.8778720247145329\n",
      "batch idx: 162 done; train loss: 0.571625292301178; avg train loss: 0.8759932104052485\n",
      "Current learning rate: 5e-05\n",
      "\n",
      "Epoch 5/10\n",
      "Train - Loss: 0.6549, Acc: 0.6933, F1: 0.7126, Recall: 0.8052, AUC: 0.7535\n",
      "Val   - Loss: 1.1115, Acc: 0.4909, F1: 0.5172, Recall: 0.8824, AUC: 0.7430\n",
      "batch idx: 0 done; train loss: 0.2452734112739563; avg train loss: 0.2452734112739563\n",
      "batch idx: 1 done; train loss: 0.8807101249694824; avg train loss: 0.5629917681217194\n",
      "batch idx: 2 done; train loss: 0.1654352843761444; avg train loss: 0.4304729402065277\n",
      "batch idx: 3 done; train loss: 0.24980688095092773; avg train loss: 0.3853064253926277\n",
      "batch idx: 4 done; train loss: 0.6078231334686279; avg train loss: 0.42980976700782775\n",
      "batch idx: 5 done; train loss: 0.19502271711826324; avg train loss: 0.3906785920262337\n",
      "batch idx: 6 done; train loss: 2.2890267372131348; avg train loss: 0.6618711841957909\n",
      "batch idx: 7 done; train loss: 1.9859883785247803; avg train loss: 0.8273858334869146\n",
      "batch idx: 8 done; train loss: 0.2741120755672455; avg train loss: 0.7659109714958403\n",
      "batch idx: 9 done; train loss: 1.7922292947769165; avg train loss: 0.8685428038239479\n",
      "batch idx: 10 done; train loss: 0.07131185382604599; avg train loss: 0.7960672629150477\n",
      "batch idx: 11 done; train loss: 0.44852185249328613; avg train loss: 0.7671051453799009\n",
      "batch idx: 12 done; train loss: 4.4429168701171875; avg train loss: 1.0498598934366152\n",
      "batch idx: 13 done; train loss: 0.6518149971961975; avg train loss: 1.0214281151337283\n",
      "batch idx: 14 done; train loss: 0.06423386931419373; avg train loss: 0.957615165412426\n",
      "batch idx: 15 done; train loss: 0.025781627744436264; avg train loss: 0.8993755693081766\n",
      "batch idx: 16 done; train loss: 0.2648547887802124; avg train loss: 0.8620508175124141\n",
      "batch idx: 17 done; train loss: 2.1673471927642822; avg train loss: 0.9345672828041844\n",
      "batch idx: 18 done; train loss: 1.6374948024749756; avg train loss: 0.9715634680500156\n",
      "batch idx: 19 done; train loss: 1.1140862703323364; avg train loss: 0.9786896081641316\n",
      "batch idx: 20 done; train loss: 0.1032976433634758; avg train loss: 0.9370042765069575\n",
      "batch idx: 21 done; train loss: 0.8164396286010742; avg train loss: 0.9315240652385083\n",
      "batch idx: 22 done; train loss: 1.7812496423721313; avg train loss: 0.9684686555486658\n",
      "batch idx: 23 done; train loss: 0.3564668297767639; avg train loss: 0.9429685794748366\n",
      "batch idx: 24 done; train loss: 0.2789764106273651; avg train loss: 0.9164088927209377\n",
      "batch idx: 25 done; train loss: 0.6800040602684021; avg train loss: 0.907316399165071\n",
      "batch idx: 26 done; train loss: 0.19729213416576385; avg train loss: 0.8810192041650966\n",
      "batch idx: 27 done; train loss: 0.03248991817235947; avg train loss: 0.8507145868082132\n",
      "batch idx: 28 done; train loss: 1.2851901054382324; avg train loss: 0.8656965012437311\n",
      "batch idx: 29 done; train loss: 0.4663742184638977; avg train loss: 0.8523857584844033\n",
      "batch idx: 30 done; train loss: 0.04364728555083275; avg train loss: 0.8262974206478365\n",
      "batch idx: 31 done; train loss: 1.8861645460128784; avg train loss: 0.8594182683154941\n",
      "batch idx: 32 done; train loss: 0.19326098263263702; avg train loss: 0.839231683900862\n",
      "batch idx: 33 done; train loss: 0.022462228313088417; avg train loss: 0.8152090528541628\n",
      "batch idx: 34 done; train loss: 0.09728333353996277; avg train loss: 0.7946968894451857\n",
      "batch idx: 35 done; train loss: 0.021300600841641426; avg train loss: 0.7732136592061983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PTSDTransformer().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=2\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = create_balanced_dataloader(\"train_normalized.h5\", batch_size=1, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(HDF5Dataset(\"dev_normalized.h5\"), batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "test_loader = DataLoader(HDF5Dataset(\"test_normalized.h5\"), batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    num_epochs=10,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19dda1d3-d8cb-4a89-90fa-3f738409edd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T18:47:14.978372Z",
     "iopub.status.busy": "2025-05-13T18:47:14.978108Z",
     "iopub.status.idle": "2025-05-13T18:47:15.078969Z",
     "shell.execute_reply": "2025-05-13T18:47:15.078652Z",
     "shell.execute_reply.started": "2025-05-13T18:47:14.978353Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b4a6f2-dc91-4a24-a744-026e0fd5118a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:28:42.890140Z",
     "iopub.status.busy": "2025-05-13T19:28:42.889897Z",
     "iopub.status.idle": "2025-05-13T19:28:42.892337Z",
     "shell.execute_reply": "2025-05-13T19:28:42.891974Z",
     "shell.execute_reply.started": "2025-05-13T19:28:42.890123Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
