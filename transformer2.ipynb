{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7294e37c-aae2-4365-80d0-358276483f89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T00:18:26.868850Z",
     "iopub.status.busy": "2025-05-15T00:18:26.868601Z",
     "iopub.status.idle": "2025-05-15T00:18:27.121852Z",
     "shell.execute_reply": "2025-05-15T00:18:27.121378Z",
     "shell.execute_reply.started": "2025-05-15T00:18:26.868825Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import glob\n",
    "import gc\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_dir = \"../storage/transform/edaicwoz_\"\n",
    "mods_name = ['audio_densenet', 'visual_resnet', 'audio_vgg16']\n",
    "mods_size = [1920, 2048, 4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f4649e-a005-4512-8b04-0e62eea4c115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T19:14:20.893979Z",
     "iopub.status.busy": "2025-05-14T19:14:20.893702Z",
     "iopub.status.idle": "2025-05-14T19:14:20.897343Z",
     "shell.execute_reply": "2025-05-14T19:14:20.896962Z",
     "shell.execute_reply.started": "2025-05-14T19:14:20.893963Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, h5_path):\n",
    "        h5_path = base_dir + h5_path\n",
    "        self.h5_path = h5_path\n",
    "        with h5py.File(h5_path, 'r', libver='latest', swmr=True) as f:\n",
    "            self.participants = list(f.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.participants)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_path, 'r', libver='latest', swmr=True) as f:\n",
    "            grp = f[self.participants[idx]]\n",
    "            features = {k: torch.from_numpy(grp[k][:]) for k in grp.keys() if k != 'info'}\n",
    "            label = torch.tensor(grp['info'][1][()])\n",
    "            return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb62910d-dae1-4efe-98ff-eb352bce99e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T19:14:20.898122Z",
     "iopub.status.busy": "2025-05-14T19:14:20.897793Z",
     "iopub.status.idle": "2025-05-14T19:14:20.901019Z",
     "shell.execute_reply": "2025-05-14T19:14:20.900492Z",
     "shell.execute_reply.started": "2025-05-14T19:14:20.898108Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "    \n",
    "    padded_batch = {}\n",
    "    for modality in features[0].keys():\n",
    "        sequences = [item[modality] for item in features]\n",
    "        padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "        padded_batch[modality] = padded\n",
    "        \n",
    "        masks = torch.ones_like(padded[:, :, 0])\n",
    "        for i, seq in enumerate(sequences):\n",
    "            masks[i, len(seq):] = 0\n",
    "        padded_batch[f'{modality}_mask'] = masks.bool()\n",
    "    \n",
    "    return padded_batch, torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36aeefa-289f-4e8f-b2f9-4ce42e964b5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T19:14:20.901724Z",
     "iopub.status.busy": "2025-05-14T19:14:20.901483Z",
     "iopub.status.idle": "2025-05-14T19:14:20.905838Z",
     "shell.execute_reply": "2025-05-14T19:14:20.905507Z",
     "shell.execute_reply.started": "2025-05-14T19:14:20.901711Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PTSDTransformer(nn.Module):\n",
    "    def __init__(self, d_model=128):\n",
    "        super().__init__()\n",
    "        self.audio_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead=8, dim_feedforward=64, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        self.audio_transformer2 = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead=8, dim_feedforward=64, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        self.video_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead=8, dim_feedforward=64, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        self.audio_proj = nn.Linear(mods_size[0], d_model)\n",
    "        self.video_proj = nn.Linear(mods_size[1], d_model)\n",
    "        \n",
    "        self.audio_proj2 = nn.Linear(mods_size[2], d_model)\n",
    "        \n",
    "        self.classifier = nn.Linear(len(mods_size)*d_model, 2)\n",
    "\n",
    "    def forward(self, audio, video, audio2, audio_mask=None, video_mask=None, audio_mask2=None):\n",
    "        # audio pathway\n",
    "        audio = self.audio_proj(audio)\n",
    "        audio = self.audio_transformer(audio, src_key_padding_mask=audio_mask)\n",
    "        audio_pooled = audio.mean(dim=1)\n",
    "        \n",
    "        # video pathway\n",
    "        video = self.video_proj(video)\n",
    "        video = self.video_transformer(video, src_key_padding_mask=video_mask)\n",
    "        video_pooled = video.mean(dim=1)\n",
    "                                     \n",
    "        audio2 = self.audio_proj2(audio2)\n",
    "        audio2 = self.audio_transformer2(audio2, src_key_padding_mask=audio_mask2)\n",
    "        audio_pooled2 = audio2.mean(dim=1)\n",
    "        \n",
    "        # fusion\n",
    "        fused = torch.cat([audio_pooled, video_pooled, audio_pooled2], dim=-1)\n",
    "        return self.classifier(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1501578e-fe4a-44a2-ac70-ae6c533a26e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T19:14:20.906472Z",
     "iopub.status.busy": "2025-05-14T19:14:20.906324Z",
     "iopub.status.idle": "2025-05-14T19:14:20.908482Z",
     "shell.execute_reply": "2025-05-14T19:14:20.908199Z",
     "shell.execute_reply.started": "2025-05-14T19:14:20.906458Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataloader(h5_path, batch_size=32, pin_memory=False, num_workers=0, shuffle=True):\n",
    "    dataset = HDF5Dataset(h5_path)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "802e504b-090c-4d1c-99b7-aaaa90df8b1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T19:14:20.909060Z",
     "iopub.status.busy": "2025-05-14T19:14:20.908892Z",
     "iopub.status.idle": "2025-05-14T19:14:20.911771Z",
     "shell.execute_reply": "2025-05-14T19:14:20.911483Z",
     "shell.execute_reply.started": "2025-05-14T19:14:20.909047Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_balanced_dataloader(h5_path, batch_size, collate_fn,  pin_memory=False, num_workers=0):\n",
    "    dataset = HDF5Dataset(h5_path)\n",
    "    \n",
    "    print(\"[Start] create balanced dataset\")\n",
    "    \n",
    "    all_labels = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        all_labels.append(label.item())\n",
    "    \n",
    "    class_counts = torch.bincount(torch.tensor(all_labels))\n",
    "    \n",
    "    weights_per_class = 1.0 / class_counts.float()\n",
    "    \n",
    "    sample_weights = [weights_per_class[label] for label in all_labels]\n",
    "    \n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    print(\"[End] create balanced dataset\")\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f36ead-3cc0-4d66-8023-fdebbaacb066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T19:14:20.912918Z",
     "iopub.status.busy": "2025-05-14T19:14:20.912695Z",
     "iopub.status.idle": "2025-05-14T19:14:20.916959Z",
     "shell.execute_reply": "2025-05-14T19:14:20.916621Z",
     "shell.execute_reply.started": "2025-05-14T19:14:20.912906Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    mods = mods_name\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in loader:\n",
    "            inputs = {\n",
    "                mod: features[mod].float().to(device, non_blocking=True)\n",
    "                for mod in mods\n",
    "            }\n",
    "            masks = {\n",
    "                mod: features[f'{mod}_mask'].to(device, non_blocking=True)\n",
    "                for mod in mods\n",
    "            }\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(\n",
    "                inputs[mods[0]],\n",
    "                inputs[mods[1]],\n",
    "                inputs[mods[2]],\n",
    "                audio_mask=~masks[mods[0]],\n",
    "                video_mask=~masks[mods[1]],\n",
    "                audio_mask2=~masks[mods[2]]\n",
    "            )\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': total_loss / len(loader),\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds),\n",
    "        'recall': recall_score(all_labels, all_preds),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        metrics['auc'] = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        metrics['auc'] = float('nan')\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20be981a-23eb-4f8f-99eb-02ffc9801817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T19:14:20.917814Z",
     "iopub.status.busy": "2025-05-14T19:14:20.917632Z",
     "iopub.status.idle": "2025-05-14T19:14:20.923676Z",
     "shell.execute_reply": "2025-05-14T19:14:20.923362Z",
     "shell.execute_reply.started": "2025-05-14T19:14:20.917796Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, test_loader, optimizer, scheduler, criterion, num_epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "    history = {\n",
    "        'train': {'loss': [], 'accuracy': [], 'f1': [], 'recall': [], 'auc': []},\n",
    "        'val': {'loss': [], 'accuracy': [], 'f1': [], 'recall': [], 'auc': []},\n",
    "        'test': {'loss': None, 'accuracy': None, 'f1': None, 'recall': None, 'auc': None}\n",
    "    }\n",
    "    \n",
    "    mods = mods_name\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            inputs = {\n",
    "                mod: features[mod].float().to(device, non_blocking=True)\n",
    "                for mod in mods\n",
    "            }\n",
    "            masks = {\n",
    "                mod: features[f'{mod}_mask'].to(device, non_blocking=True)\n",
    "                for mod in mods\n",
    "            }\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                inputs[mods[0]],\n",
    "                inputs[mods[1]],\n",
    "                inputs[mods[2]],\n",
    "                audio_mask=~masks[mods[0]],\n",
    "                video_mask=~masks[mods[1]],\n",
    "                audio_mask2=~masks[mods[2]]\n",
    "            )\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            print(f\"batch idx: {batch_idx} done; train loss: {loss.item()}; avg train loss: {train_loss/(batch_idx+1)}\")\n",
    "            \n",
    "\n",
    "        train_metrics = evaluate(model, train_loader, device)\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        \n",
    "        scheduler.step(val_metrics['loss'])\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current learning rate: {current_lr}\")\n",
    "\n",
    "        for metric in ['loss', 'accuracy', 'f1', 'recall', 'auc']:\n",
    "            history['train'][metric].append(train_metrics[metric])\n",
    "            history['val'][metric].append(val_metrics[metric])\n",
    "\n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, \" +\n",
    "              f\"F1: {train_metrics['f1']:.4f}, Recall: {train_metrics['recall']:.4f}, AUC: {train_metrics['auc']:.4f}\")\n",
    "        print(f\"Val   - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, \" +\n",
    "              f\"F1: {val_metrics['f1']:.4f}, Recall: {val_metrics['recall']:.4f}, AUC: {val_metrics['auc']:.4f}\")\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "    history['test'] = test_metrics\n",
    "\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_metrics['loss']:.4f}, Acc: {test_metrics['accuracy']:.4f}, \" +\n",
    "          f\"F1: {test_metrics['f1']:.4f}, Recall: {test_metrics['recall']:.4f}, AUC: {test_metrics['auc']:.4f}\")\n",
    "\n",
    "    plot_metrics(history['train'], history['val'], history['test'])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5978adc-2262-49c4-9b6b-ab763c7bda23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T19:14:20.924214Z",
     "iopub.status.busy": "2025-05-14T19:14:20.924096Z",
     "iopub.status.idle": "2025-05-14T19:14:20.927127Z",
     "shell.execute_reply": "2025-05-14T19:14:20.926801Z",
     "shell.execute_reply.started": "2025-05-14T19:14:20.924202Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics(train_metrics, val_metrics, test_metrics):\n",
    "    epochs = range(1, len(train_metrics['loss']) + 1)\n",
    "    metrics = ['loss', 'accuracy', 'f1', 'recall', 'auc']\n",
    "\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        plt.subplot(2, 3, i)\n",
    "        plt.plot(epochs, train_metrics[metric], label='Train')\n",
    "        plt.plot(epochs, val_metrics[metric], label='Validation')\n",
    "        \n",
    "        if test_metrics[metric] is not None:\n",
    "            plt.axhline(y=test_metrics[metric], color='r', linestyle='--', label='Test')\n",
    "        \n",
    "        plt.title(metric.capitalize())\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855ff108-7600-41a7-be93-ac6cf234d338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T19:14:20.927691Z",
     "iopub.status.busy": "2025-05-14T19:14:20.927524Z",
     "iopub.status.idle": "2025-05-14T22:03:09.501329Z",
     "shell.execute_reply": "2025-05-14T22:03:09.499382Z",
     "shell.execute_reply.started": "2025-05-14T19:14:20.927679Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Start] create balanced dataset\n",
      "[End] create balanced dataset\n",
      "batch idx: 0 done; train loss: 0.9249085783958435; avg train loss: 0.9249085783958435\n",
      "batch idx: 1 done; train loss: 0.5163732171058655; avg train loss: 0.7206408977508545\n",
      "batch idx: 2 done; train loss: 0.0026033578906208277; avg train loss: 0.4812950511307766\n",
      "batch idx: 3 done; train loss: 0.5895087718963623; avg train loss: 0.508348481322173\n",
      "batch idx: 4 done; train loss: 0.3536399304866791; avg train loss: 0.47740677115507424\n",
      "batch idx: 5 done; train loss: 0.005417665466666222; avg train loss: 0.3987419202070062\n",
      "batch idx: 6 done; train loss: 0.002528686309233308; avg train loss: 0.34214002965018153\n",
      "batch idx: 7 done; train loss: 0.14559443295001984; avg train loss: 0.3175718300626613\n",
      "batch idx: 8 done; train loss: 0.0038631348870694637; avg train loss: 0.28271530837648445\n",
      "batch idx: 9 done; train loss: 3.758331060409546; avg train loss: 0.6302768835797906\n",
      "batch idx: 10 done; train loss: 0.0020306934602558613; avg train loss: 0.5731635935689238\n",
      "batch idx: 11 done; train loss: 0.04552686586976051; avg train loss: 0.5291938662606602\n",
      "batch idx: 12 done; train loss: 0.5249477624893188; avg train loss: 0.528867242893634\n",
      "batch idx: 13 done; train loss: 1.639031171798706; avg train loss: 0.6081646663868534\n",
      "batch idx: 14 done; train loss: 0.28778114914894104; avg train loss: 0.5868057652376593\n",
      "batch idx: 15 done; train loss: 2.6606407165527344; avg train loss: 0.7164204496948514\n",
      "batch idx: 16 done; train loss: 0.01914837956428528; avg train loss: 0.675404445569524\n",
      "batch idx: 17 done; train loss: 2.6189286708831787; avg train loss: 0.7833780136425048\n",
      "batch idx: 18 done; train loss: 0.2473120242357254; avg train loss: 0.7551640142000428\n",
      "batch idx: 19 done; train loss: 3.8029870986938477; avg train loss: 0.9075551684247329\n",
      "batch idx: 20 done; train loss: 0.08683928102254868; avg train loss: 0.8684734595008194\n",
      "batch idx: 21 done; train loss: 0.3898431658744812; avg train loss: 0.8467175370632586\n",
      "batch idx: 22 done; train loss: 3.2806897163391113; avg train loss: 0.9525424144230783\n",
      "batch idx: 23 done; train loss: 1.152611255645752; avg train loss: 0.9608786161406897\n",
      "batch idx: 24 done; train loss: 0.011145477183163166; avg train loss: 0.9228892905823887\n",
      "batch idx: 25 done; train loss: 4.725162982940674; avg train loss: 1.0691305864423226\n",
      "batch idx: 26 done; train loss: 0.022788895294070244; avg train loss: 1.030377190473869\n",
      "batch idx: 27 done; train loss: 0.006679943297058344; avg train loss: 0.9938165745032685\n",
      "batch idx: 28 done; train loss: 0.04991232603788376; avg train loss: 0.9612681521423931\n",
      "batch idx: 29 done; train loss: 0.015390326268970966; avg train loss: 0.9297388912799458\n",
      "batch idx: 30 done; train loss: 0.00951316487044096; avg train loss: 0.9000541904280263\n",
      "batch idx: 31 done; train loss: 0.005608181469142437; avg train loss: 0.8721027526480611\n",
      "batch idx: 32 done; train loss: 0.272748738527298; avg train loss: 0.8539405097959168\n",
      "batch idx: 33 done; train loss: 0.08617805689573288; avg train loss: 0.8313592611812055\n",
      "batch idx: 34 done; train loss: 0.04608062654733658; avg train loss: 0.808922728763095\n",
      "batch idx: 35 done; train loss: 7.052343368530273; avg train loss: 0.9823510798677388\n",
      "batch idx: 36 done; train loss: 0.013048617169260979; avg train loss: 0.9561537160110232\n",
      "batch idx: 37 done; train loss: 1.434779167175293; avg train loss: 0.9687491226206092\n",
      "batch idx: 38 done; train loss: 6.1504902839660645; avg train loss: 1.101614280603826\n",
      "batch idx: 39 done; train loss: 0.0006551980040967464; avg train loss: 1.0740903035388327\n",
      "batch idx: 40 done; train loss: 0.22900232672691345; avg train loss: 1.0534784016653713\n",
      "batch idx: 41 done; train loss: 1.4887676239013672; avg train loss: 1.0638424307662284\n",
      "batch idx: 42 done; train loss: 2.5961854457855225; avg train loss: 1.0994783148364446\n",
      "batch idx: 43 done; train loss: 0.0006481691962108016; avg train loss: 1.0745049024355302\n",
      "batch idx: 44 done; train loss: 1.4653170108795166; avg train loss: 1.0831896159565075\n",
      "batch idx: 45 done; train loss: 0.0014840076910331845; avg train loss: 1.0596742766463887\n",
      "batch idx: 46 done; train loss: 1.77176034450531; avg train loss: 1.0748250440476423\n",
      "batch idx: 47 done; train loss: 2.474266767501831; avg train loss: 1.1039800799529378\n",
      "batch idx: 48 done; train loss: 0.0009048658539541066; avg train loss: 1.0814683408896932\n",
      "batch idx: 49 done; train loss: 0.13131964206695557; avg train loss: 1.0624653669132385\n",
      "batch idx: 50 done; train loss: 2.1384541988372803; avg train loss: 1.0835631871470432\n",
      "batch idx: 51 done; train loss: 0.5304433703422546; avg train loss: 1.072926267593105\n",
      "batch idx: 52 done; train loss: 0.05894935131072998; avg train loss: 1.0537946276632488\n",
      "batch idx: 53 done; train loss: 0.26514700055122375; avg train loss: 1.0391900419759892\n",
      "batch idx: 54 done; train loss: 0.05671398714184761; avg train loss: 1.0213268409790048\n",
      "batch idx: 55 done; train loss: 0.010696694254875183; avg train loss: 1.0032798740732167\n",
      "batch idx: 56 done; train loss: 0.1441727727651596; avg train loss: 0.9882078196643035\n",
      "batch idx: 57 done; train loss: 1.6305086612701416; avg train loss: 0.9992819721057834\n",
      "batch idx: 58 done; train loss: 5.33695650100708; avg train loss: 1.072801879375297\n",
      "batch idx: 59 done; train loss: 0.27894672751426697; avg train loss: 1.059570960177613\n",
      "batch idx: 60 done; train loss: 0.017714032903313637; avg train loss: 1.042491338419018\n",
      "batch idx: 61 done; train loss: 3.142469882965088; avg train loss: 1.076361960105245\n",
      "batch idx: 62 done; train loss: 0.026112787425518036; avg train loss: 1.059691338316678\n",
      "batch idx: 63 done; train loss: 1.6225597858428955; avg train loss: 1.068486157809275\n",
      "batch idx: 64 done; train loss: 6.896574020385742; avg train loss: 1.1581490480027592\n",
      "batch idx: 65 done; train loss: 0.0016963391099125147; avg train loss: 1.1406270372619585\n",
      "batch idx: 66 done; train loss: 0.02294921875; avg train loss: 1.1239452787767052\n",
      "batch idx: 67 done; train loss: 7.817113876342773; avg train loss: 1.2223742287409123\n",
      "batch idx: 68 done; train loss: 0.0745316743850708; avg train loss: 1.2057388294024218\n",
      "batch idx: 69 done; train loss: 7.058672904968262; avg train loss: 1.2893521733390767\n",
      "batch idx: 70 done; train loss: 5.191594123840332; avg train loss: 1.3443133275714887\n",
      "batch idx: 71 done; train loss: 2.173584461212158; avg train loss: 1.355830982205387\n",
      "batch idx: 72 done; train loss: 0.13694711029529572; avg train loss: 1.3391339428641527\n",
      "batch idx: 73 done; train loss: 0.0010844547068700194; avg train loss: 1.3210521930241894\n",
      "batch idx: 74 done; train loss: 5.190910816192627; avg train loss: 1.3726503079997685\n",
      "batch idx: 75 done; train loss: 7.3463568687438965; avg train loss: 1.451251710114823\n",
      "batch idx: 76 done; train loss: 0.001420323271304369; avg train loss: 1.4324227310649071\n",
      "batch idx: 77 done; train loss: 5.9408650398254395; avg train loss: 1.490223273484914\n",
      "batch idx: 78 done; train loss: 0.0012029323261231184; avg train loss: 1.4713749147360684\n",
      "batch idx: 79 done; train loss: 5.051985263824463; avg train loss: 1.5161325440996734\n",
      "batch idx: 80 done; train loss: 4.690729141235352; avg train loss: 1.5553250946815953\n",
      "batch idx: 81 done; train loss: 3.7650678157806396; avg train loss: 1.582273176646218\n",
      "batch idx: 82 done; train loss: 0.09219692647457123; avg train loss: 1.5643204507405353\n",
      "batch idx: 83 done; train loss: 5.020233631134033; avg train loss: 1.6054622743166485\n",
      "batch idx: 84 done; train loss: 0.061084188520908356; avg train loss: 1.5872931203661103\n",
      "batch idx: 85 done; train loss: 0.09936472773551941; avg train loss: 1.5699916274285453\n",
      "batch idx: 86 done; train loss: 2.2457633018493652; avg train loss: 1.5777591179391295\n",
      "batch idx: 87 done; train loss: 2.461879253387451; avg train loss: 1.587805937660133\n",
      "batch idx: 88 done; train loss: 0.03395221382379532; avg train loss: 1.5703469070552305\n",
      "batch idx: 89 done; train loss: 1.071458101272583; avg train loss: 1.5648036981020899\n",
      "batch idx: 90 done; train loss: 2.19878888130188; avg train loss: 1.5717705682471426\n",
      "batch idx: 91 done; train loss: 0.22228601574897766; avg train loss: 1.5571022578939016\n",
      "batch idx: 92 done; train loss: 0.2631855309009552; avg train loss: 1.543189174807956\n",
      "batch idx: 93 done; train loss: 1.5250327587127686; avg train loss: 1.5429960214452412\n",
      "batch idx: 94 done; train loss: 2.4308085441589355; avg train loss: 1.5523414164211748\n",
      "batch idx: 95 done; train loss: 0.25800958275794983; avg train loss: 1.5388587931538495\n",
      "batch idx: 96 done; train loss: 0.10906939208507538; avg train loss: 1.5241186962356148\n",
      "batch idx: 97 done; train loss: 2.5874946117401123; avg train loss: 1.53496947088362\n",
      "batch idx: 98 done; train loss: 0.23167240619659424; avg train loss: 1.5218048540685993\n",
      "batch idx: 99 done; train loss: 0.4166905879974365; avg train loss: 1.5107537114078877\n",
      "batch idx: 100 done; train loss: 0.09585310518741608; avg train loss: 1.4967447945146157\n",
      "batch idx: 101 done; train loss: 1.1898711919784546; avg train loss: 1.493736229783869\n",
      "batch idx: 102 done; train loss: 2.1088616847991943; avg train loss: 1.4997083215801343\n",
      "batch idx: 103 done; train loss: 2.340388298034668; avg train loss: 1.5077917828921972\n",
      "batch idx: 104 done; train loss: 0.04604192078113556; avg train loss: 1.4938703556339965\n",
      "batch idx: 105 done; train loss: 0.10879625380039215; avg train loss: 1.4808036188242457\n",
      "batch idx: 106 done; train loss: 1.7658747434616089; avg train loss: 1.4834678349423518\n",
      "batch idx: 107 done; train loss: 0.07716195285320282; avg train loss: 1.470446484182267\n",
      "batch idx: 108 done; train loss: 0.3572991192340851; avg train loss: 1.4602341230359535\n",
      "batch idx: 109 done; train loss: 4.030934810638428; avg train loss: 1.4836041292868851\n",
      "batch idx: 110 done; train loss: 0.7740511894226074; avg train loss: 1.477211760459279\n",
      "batch idx: 111 done; train loss: 0.01202641986310482; avg train loss: 1.4641297484896703\n",
      "batch idx: 112 done; train loss: 1.1039011478424072; avg train loss: 1.460941884767128\n",
      "batch idx: 113 done; train loss: 0.011382993310689926; avg train loss: 1.4482264558947033\n",
      "batch idx: 114 done; train loss: 1.3512694835662842; avg train loss: 1.4473833517874997\n",
      "batch idx: 115 done; train loss: 3.793578863143921; avg train loss: 1.4676091751612619\n",
      "batch idx: 116 done; train loss: 0.6054387092590332; avg train loss: 1.4602401968202172\n",
      "batch idx: 117 done; train loss: 3.0237746238708496; avg train loss: 1.4734904885748836\n",
      "batch idx: 118 done; train loss: 0.1519201397895813; avg train loss: 1.4623848553918137\n",
      "batch idx: 119 done; train loss: 1.6091886758804321; avg train loss: 1.4636082205625522\n",
      "batch idx: 120 done; train loss: 0.5700128078460693; avg train loss: 1.4562231345070442\n",
      "batch idx: 121 done; train loss: 0.17771288752555847; avg train loss: 1.4457435423186713\n",
      "batch idx: 122 done; train loss: 1.1122134923934937; avg train loss: 1.4430319158965155\n",
      "batch idx: 123 done; train loss: 0.22138744592666626; avg train loss: 1.4331799443645006\n",
      "batch idx: 124 done; train loss: 1.278218150138855; avg train loss: 1.4319402500106952\n",
      "batch idx: 125 done; train loss: 2.249516725540161; avg train loss: 1.4384289521974372\n",
      "batch idx: 126 done; train loss: 0.07305609434843063; avg train loss: 1.4276779848127992\n",
      "batch idx: 127 done; train loss: 0.17531763017177582; avg train loss: 1.4178939195421663\n",
      "batch idx: 128 done; train loss: 1.0907707214355469; avg train loss: 1.4153580807971538\n",
      "batch idx: 129 done; train loss: 0.4994482100009918; avg train loss: 1.4083126202525678\n",
      "batch idx: 130 done; train loss: 0.033505022525787354; avg train loss: 1.3978179057661038\n",
      "batch idx: 131 done; train loss: 0.19540458917617798; avg train loss: 1.388708713973756\n",
      "batch idx: 132 done; train loss: 0.016292989253997803; avg train loss: 1.378389798750299\n",
      "batch idx: 133 done; train loss: 0.4624970257282257; avg train loss: 1.3715547780561046\n",
      "batch idx: 134 done; train loss: 0.09236355870962143; avg train loss: 1.3620792875424268\n",
      "batch idx: 135 done; train loss: 0.2173699587583542; avg train loss: 1.3536623071837204\n",
      "batch idx: 136 done; train loss: 3.7317898273468018; avg train loss: 1.3710209022214073\n",
      "batch idx: 137 done; train loss: 3.243077039718628; avg train loss: 1.3845865264061696\n",
      "batch idx: 138 done; train loss: 1.3511630296707153; avg train loss: 1.3843460695951233\n",
      "batch idx: 139 done; train loss: 0.663387656211853; avg train loss: 1.3791963666423857\n",
      "batch idx: 140 done; train loss: 0.02260582149028778; avg train loss: 1.3695751571022998\n",
      "batch idx: 141 done; train loss: 1.3688980340957642; avg train loss: 1.3695703886304227\n",
      "batch idx: 142 done; train loss: 0.1709478795528412; avg train loss: 1.3611884130424676\n",
      "batch idx: 143 done; train loss: 0.8901890516281128; avg train loss: 1.357917584143757\n",
      "batch idx: 144 done; train loss: 0.49896925687789917; avg train loss: 1.3519938025764062\n",
      "batch idx: 145 done; train loss: 0.000812915270216763; avg train loss: 1.3427391389647199\n",
      "batch idx: 146 done; train loss: 0.0756077989935875; avg train loss: 1.3341191978764808\n",
      "batch idx: 147 done; train loss: 3.8392279148101807; avg train loss: 1.351045608126033\n",
      "batch idx: 148 done; train loss: 0.004670306574553251; avg train loss: 1.342009532276694\n",
      "batch idx: 149 done; train loss: 0.2605270743370056; avg train loss: 1.3347996492237628\n",
      "batch idx: 150 done; train loss: 0.0009683449170552194; avg train loss: 1.325966329327692\n",
      "batch idx: 151 done; train loss: 3.5844199657440186; avg train loss: 1.3408245769356941\n",
      "batch idx: 152 done; train loss: 0.3012537360191345; avg train loss: 1.3340299962761089\n",
      "batch idx: 153 done; train loss: 0.05013196915388107; avg train loss: 1.3256929960999904\n",
      "batch idx: 154 done; train loss: 2.1519126892089844; avg train loss: 1.3310234457329517\n",
      "batch idx: 155 done; train loss: 0.3688991963863373; avg train loss: 1.3248559825961144\n",
      "batch idx: 156 done; train loss: 0.03590437397360802; avg train loss: 1.3166460997386462\n",
      "batch idx: 157 done; train loss: 0.010591249912977219; avg train loss: 1.3083799298030407\n",
      "batch idx: 158 done; train loss: 0.0026049036532640457; avg train loss: 1.3001675082549289\n",
      "batch idx: 159 done; train loss: 0.0029187481850385666; avg train loss: 1.292059703504492\n",
      "batch idx: 160 done; train loss: 0.04888162016868591; avg train loss: 1.2843381005024064\n",
      "batch idx: 161 done; train loss: 2.305117607116699; avg train loss: 1.2906392085679266\n",
      "batch idx: 162 done; train loss: 0.0008759237825870514; avg train loss: 1.2827265503790595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate: 0.0005\n",
      "\n",
      "Epoch 1/10\n",
      "Train - Loss: 2.0504, Acc: 0.5521, F1: 0.1978, Recall: 0.1098, AUC: 0.6951\n",
      "Val   - Loss: 1.4972, Acc: 0.6909, F1: 0.0000, Recall: 0.0000, AUC: 0.5062\n",
      "batch idx: 0 done; train loss: 8.555012702941895; avg train loss: 8.555012702941895\n",
      "batch idx: 1 done; train loss: 0.0068819401785731316; avg train loss: 4.280947321560234\n",
      "batch idx: 2 done; train loss: 4.128085136413574; avg train loss: 4.22999325984468\n",
      "batch idx: 3 done; train loss: 2.3777360916137695; avg train loss: 3.766928967786953\n",
      "batch idx: 4 done; train loss: 0.003950174432247877; avg train loss: 3.0143332091160118\n",
      "batch idx: 5 done; train loss: 1.5470702648162842; avg train loss: 2.769789385066057\n",
      "batch idx: 6 done; train loss: 0.1305990070104599; avg train loss: 2.392762188200972\n",
      "batch idx: 7 done; train loss: 0.015805209055542946; avg train loss: 2.0956425658077933\n",
      "batch idx: 8 done; train loss: 0.030279556289315224; avg train loss: 1.8661577869724069\n",
      "batch idx: 9 done; train loss: 1.0186952352523804; avg train loss: 1.7814115318004042\n",
      "batch idx: 10 done; train loss: 0.14726534485816956; avg train loss: 1.6328527875329284\n",
      "batch idx: 11 done; train loss: 3.667069673538208; avg train loss: 1.8023708613667015\n",
      "batch idx: 12 done; train loss: 0.0005227153305895627; avg train loss: 1.6637671578254623\n",
      "batch idx: 13 done; train loss: 0.011143000796437263; avg train loss: 1.545722575180532\n",
      "batch idx: 14 done; train loss: 0.00048196621355600655; avg train loss: 1.4427065345827335\n",
      "batch idx: 15 done; train loss: 3.257642984390259; avg train loss: 1.5561400626957038\n",
      "batch idx: 16 done; train loss: 2.6917107105255127; avg train loss: 1.6229383360974572\n",
      "batch idx: 17 done; train loss: 0.09121185541152954; avg train loss: 1.5378424205037946\n",
      "batch idx: 18 done; train loss: 2.720782518386841; avg train loss: 1.6001024256555338\n",
      "batch idx: 19 done; train loss: 0.04453917592763901; avg train loss: 1.5223242631691392\n",
      "batch idx: 20 done; train loss: 0.029511436820030212; avg train loss: 1.4512379381048959\n",
      "batch idx: 21 done; train loss: 0.22084417939186096; avg train loss: 1.395310949072485\n",
      "batch idx: 22 done; train loss: 0.1412755697965622; avg train loss: 1.3407876717126626\n",
      "batch idx: 23 done; train loss: 0.46783822774887085; avg train loss: 1.304414778214171\n",
      "batch idx: 24 done; train loss: 1.0796704292297363; avg train loss: 1.2954250042547937\n",
      "batch idx: 25 done; train loss: 5.670506954193115; avg train loss: 1.4636973869447292\n",
      "batch idx: 26 done; train loss: 5.517245292663574; avg train loss: 1.613828790860242\n",
      "batch idx: 27 done; train loss: 0.8400147557258606; avg train loss: 1.5861925753197283\n",
      "batch idx: 28 done; train loss: 5.151629447937012; avg train loss: 1.7091386743754968\n",
      "batch idx: 29 done; train loss: 1.0709409713745117; avg train loss: 1.6878654176087973\n",
      "batch idx: 30 done; train loss: 0.03027227148413658; avg train loss: 1.6343946709596147\n",
      "batch idx: 31 done; train loss: 5.265118598937988; avg train loss: 1.7478547937089388\n",
      "batch idx: 32 done; train loss: 0.012158564291894436; avg train loss: 1.6952579382720587\n",
      "batch idx: 33 done; train loss: 0.010823472402989864; avg train loss: 1.645715748099439\n",
      "batch idx: 34 done; train loss: 0.5473461747169495; avg train loss: 1.6143337602885108\n",
      "batch idx: 35 done; train loss: 0.022115513682365417; avg train loss: 1.5701054756605624\n",
      "batch idx: 36 done; train loss: 1.0013110637664795; avg train loss: 1.554732653717479\n",
      "batch idx: 37 done; train loss: 0.144612118601799; avg train loss: 1.5176242185828557\n",
      "batch idx: 38 done; train loss: 0.2254602015018463; avg train loss: 1.484491807888471\n",
      "batch idx: 39 done; train loss: 0.005221539177000523; avg train loss: 1.4475100511706842\n",
      "batch idx: 40 done; train loss: 0.005658797919750214; avg train loss: 1.4123429474328566\n",
      "batch idx: 41 done; train loss: 0.01609816402196884; avg train loss: 1.3790990240183116\n",
      "batch idx: 42 done; train loss: 0.02003714255988598; avg train loss: 1.3474929337518367\n",
      "batch idx: 43 done; train loss: 0.23455311357975006; avg train loss: 1.3221988469297437\n",
      "batch idx: 44 done; train loss: 0.05362105369567871; avg train loss: 1.29400822930232\n",
      "batch idx: 45 done; train loss: 1.7283399105072021; avg train loss: 1.3034502223719913\n",
      "batch idx: 46 done; train loss: 2.1684036254882812; avg train loss: 1.3218534862680826\n",
      "batch idx: 47 done; train loss: 2.416757822036743; avg train loss: 1.3446639932632631\n",
      "batch idx: 48 done; train loss: 0.0019584777764976025; avg train loss: 1.317261839885982\n",
      "batch idx: 49 done; train loss: 4.649409294128418; avg train loss: 1.383904788970831\n",
      "batch idx: 50 done; train loss: 0.6974985599517822; avg train loss: 1.3704458433037907\n",
      "batch idx: 51 done; train loss: 1.0798518657684326; avg train loss: 1.364857497581957\n",
      "batch idx: 52 done; train loss: 0.20852497220039368; avg train loss: 1.3430399027634368\n",
      "batch idx: 53 done; train loss: 1.5121396780014038; avg train loss: 1.3461713800826585\n",
      "batch idx: 54 done; train loss: 0.82932049036026; avg train loss: 1.3367740911786148\n",
      "batch idx: 55 done; train loss: 1.1775295734405518; avg train loss: 1.3339304390761495\n",
      "batch idx: 56 done; train loss: 0.1398109495639801; avg train loss: 1.3129809743478658\n",
      "batch idx: 57 done; train loss: 0.442997545003891; avg train loss: 1.2979812600488316\n",
      "batch idx: 58 done; train loss: 0.7337067127227783; avg train loss: 1.288417284670424\n",
      "batch idx: 59 done; train loss: 0.0922524705529213; avg train loss: 1.2684812044351323\n",
      "batch idx: 60 done; train loss: 0.00964670442044735; avg train loss: 1.2478445732873507\n",
      "batch idx: 61 done; train loss: 0.05504681542515755; avg train loss: 1.2286058997734441\n",
      "batch idx: 62 done; train loss: 0.040115147829055786; avg train loss: 1.2097409672028985\n",
      "batch idx: 63 done; train loss: 0.32553279399871826; avg train loss: 1.195925214496583\n",
      "batch idx: 64 done; train loss: 0.005972870625555515; avg train loss: 1.1776182553601058\n",
      "batch idx: 65 done; train loss: 3.6075632572174072; avg train loss: 1.2144356038730952\n",
      "batch idx: 66 done; train loss: 5.000454902648926; avg train loss: 1.2709433546010926\n",
      "batch idx: 67 done; train loss: 1.598433256149292; avg train loss: 1.2757593825650366\n",
      "batch idx: 68 done; train loss: 0.0008469808381050825; avg train loss: 1.257282391235661\n",
      "batch idx: 69 done; train loss: 0.07428091019392014; avg train loss: 1.2403823700779217\n",
      "batch idx: 70 done; train loss: 1.0288054943084717; avg train loss: 1.237402414081169\n",
      "batch idx: 71 done; train loss: 0.6231380105018616; avg train loss: 1.2288709640314563\n",
      "batch idx: 72 done; train loss: 0.09082318842411041; avg train loss: 1.2132812684751912\n",
      "batch idx: 73 done; train loss: 0.02181370183825493; avg train loss: 1.19718035541253\n",
      "batch idx: 74 done; train loss: 0.004409590270370245; avg train loss: 1.1812767452106345\n",
      "batch idx: 75 done; train loss: 0.02252761647105217; avg train loss: 1.1660300461482715\n",
      "batch idx: 76 done; train loss: 1.1698611974716187; avg train loss: 1.1660798013602631\n",
      "batch idx: 77 done; train loss: 0.07068725675344467; avg train loss: 1.1520363071986373\n",
      "batch idx: 78 done; train loss: 0.07952465116977692; avg train loss: 1.1384602102868795\n",
      "batch idx: 79 done; train loss: 1.4508602619171143; avg train loss: 1.1423652109322575\n",
      "batch idx: 80 done; train loss: 4.802626132965088; avg train loss: 1.1875536173771073\n",
      "batch idx: 81 done; train loss: 0.001465795561671257; avg train loss: 1.1730891317452117\n",
      "batch idx: 82 done; train loss: 3.4337575435638428; avg train loss: 1.2003261005623036\n",
      "batch idx: 83 done; train loss: 0.00043644916149787605; avg train loss: 1.1860416999503893\n",
      "batch idx: 84 done; train loss: 3.5767087936401367; avg train loss: 1.2141671951702686\n",
      "batch idx: 85 done; train loss: 2.0077879428863525; avg train loss: 1.2233953433995255\n",
      "batch idx: 86 done; train loss: 0.009530994109809399; avg train loss: 1.2094428796145862\n",
      "batch idx: 87 done; train loss: 1.5762794017791748; avg train loss: 1.2136114764573656\n",
      "batch idx: 88 done; train loss: 3.0889196395874023; avg train loss: 1.2346823546947818\n",
      "batch idx: 89 done; train loss: 0.6962074637413025; avg train loss: 1.2286993003508542\n",
      "batch idx: 90 done; train loss: 0.24275484681129456; avg train loss: 1.2178647459163534\n",
      "batch idx: 91 done; train loss: 0.0457729808986187; avg train loss: 1.205124618035726\n",
      "batch idx: 92 done; train loss: 0.7936534285545349; avg train loss: 1.2007001966434552\n",
      "batch idx: 93 done; train loss: 0.00991798099130392; avg train loss: 1.188032300732262\n",
      "batch idx: 94 done; train loss: 0.00866522267460823; avg train loss: 1.1756179104369182\n",
      "batch idx: 95 done; train loss: 0.0159563310444355; avg train loss: 1.1635381023182465\n",
      "batch idx: 96 done; train loss: 0.006945159286260605; avg train loss: 1.1516144637302879\n",
      "batch idx: 97 done; train loss: 2.6739022731781006; avg train loss: 1.167148012806286\n",
      "batch idx: 98 done; train loss: 0.0015354283386841416; avg train loss: 1.1553741483167144\n",
      "batch idx: 99 done; train loss: 0.11037378758192062; avg train loss: 1.1449241447093663\n",
      "batch idx: 100 done; train loss: 0.3793395161628723; avg train loss: 1.1373440988821735\n",
      "batch idx: 101 done; train loss: 0.07405898720026016; avg train loss: 1.1269197350421547\n",
      "batch idx: 102 done; train loss: 0.007165447808802128; avg train loss: 1.1160483341952288\n",
      "batch idx: 103 done; train loss: 0.9619994163513184; avg train loss: 1.114567094600576\n",
      "batch idx: 104 done; train loss: 0.002997193718329072; avg train loss: 1.1039807145921736\n",
      "batch idx: 105 done; train loss: 0.006787814199924469; avg train loss: 1.0936298381733787\n",
      "batch idx: 106 done; train loss: 2.5586910247802734; avg train loss: 1.1073219987958731\n",
      "batch idx: 107 done; train loss: 2.256481170654297; avg train loss: 1.1179623614982659\n",
      "batch idx: 108 done; train loss: 0.056371740996837616; avg train loss: 1.1082229980074272\n",
      "batch idx: 109 done; train loss: 0.4385547935962677; avg train loss: 1.102135105240053\n",
      "batch idx: 110 done; train loss: 0.0023960948456078768; avg train loss: 1.0922275465878506\n",
      "batch idx: 111 done; train loss: 0.0022122215013951063; avg train loss: 1.0824952668995789\n",
      "batch idx: 112 done; train loss: 0.004567191004753113; avg train loss: 1.0729560803872351\n",
      "batch idx: 113 done; train loss: 4.58793830871582; avg train loss: 1.103789257828714\n",
      "batch idx: 114 done; train loss: 0.005196279380470514; avg train loss: 1.0942362754074249\n",
      "batch idx: 115 done; train loss: 0.003226434113457799; avg train loss: 1.0848310181548908\n",
      "batch idx: 116 done; train loss: 0.008184465579688549; avg train loss: 1.0756289108679231\n",
      "batch idx: 117 done; train loss: 2.6641554832458496; avg train loss: 1.0890910004643464\n",
      "batch idx: 118 done; train loss: 0.003629645798355341; avg train loss: 1.0799694764755565\n",
      "batch idx: 119 done; train loss: 0.14897486567497253; avg train loss: 1.0722111880522183\n",
      "batch idx: 120 done; train loss: 1.358780026435852; avg train loss: 1.074579525559521\n",
      "batch idx: 121 done; train loss: 0.00048720886115916073; avg train loss: 1.0657754901767476\n",
      "batch idx: 122 done; train loss: 2.6663050651550293; avg train loss: 1.0787879257456767\n",
      "batch idx: 123 done; train loss: 3.1568803787231445; avg train loss: 1.0955467358503337\n",
      "batch idx: 124 done; train loss: 0.04873130843043327; avg train loss: 1.0871722124309744\n",
      "batch idx: 125 done; train loss: 0.05038239434361458; avg train loss: 1.0789437218112334\n",
      "batch idx: 126 done; train loss: 0.4536703824996948; avg train loss: 1.0740203096906702\n",
      "batch idx: 127 done; train loss: 0.03990429267287254; avg train loss: 1.0659412783077187\n",
      "batch idx: 128 done; train loss: 0.6738113164901733; avg train loss: 1.0629015111618463\n",
      "batch idx: 129 done; train loss: 6.446012020111084; avg train loss: 1.1043100535383787\n",
      "batch idx: 130 done; train loss: 4.309845924377441; avg train loss: 1.1287797930104329\n",
      "batch idx: 131 done; train loss: 2.842785120010376; avg train loss: 1.1417646818513414\n",
      "batch idx: 132 done; train loss: 2.6080024242401123; avg train loss: 1.1527890257790765\n",
      "batch idx: 133 done; train loss: 0.028864458203315735; avg train loss: 1.1444015290061231\n",
      "batch idx: 134 done; train loss: 0.0827607586979866; avg train loss: 1.136537523300137\n",
      "batch idx: 135 done; train loss: 0.028524938970804214; avg train loss: 1.128390371944774\n",
      "batch idx: 136 done; train loss: 2.1745476722717285; avg train loss: 1.1360265566186936\n",
      "batch idx: 137 done; train loss: 0.0027524465695023537; avg train loss: 1.127814425386453\n",
      "batch idx: 138 done; train loss: 0.24699397385120392; avg train loss: 1.1214775876056238\n",
      "batch idx: 139 done; train loss: 0.009512456133961678; avg train loss: 1.1135349795236835\n",
      "batch idx: 140 done; train loss: 0.04670894518494606; avg train loss: 1.1059688374361747\n",
      "batch idx: 141 done; train loss: 0.021009400486946106; avg train loss: 1.0983282780210393\n",
      "batch idx: 142 done; train loss: 1.2236768007278442; avg train loss: 1.099204841116891\n",
      "batch idx: 143 done; train loss: 2.2224581241607666; avg train loss: 1.107005211138029\n",
      "batch idx: 144 done; train loss: 0.0016504012746736407; avg train loss: 1.0993820745182818\n",
      "batch idx: 145 done; train loss: 0.010221279226243496; avg train loss: 1.091922069071076\n",
      "batch idx: 146 done; train loss: 2.4885001182556152; avg train loss: 1.1014226000179097\n",
      "batch idx: 147 done; train loss: 0.037234097719192505; avg train loss: 1.0942321371645398\n",
      "batch idx: 148 done; train loss: 0.12267874926328659; avg train loss: 1.08771164462829\n",
      "batch idx: 149 done; train loss: 0.08871115744113922; avg train loss: 1.0810516413803755\n",
      "batch idx: 150 done; train loss: 0.05655684694647789; avg train loss: 1.0742669076424027\n",
      "batch idx: 151 done; train loss: 0.3525156080722809; avg train loss: 1.0695185438294414\n",
      "batch idx: 152 done; train loss: 0.04533674195408821; avg train loss: 1.0628245451243736\n",
      "batch idx: 153 done; train loss: 0.02509305253624916; avg train loss: 1.0560860289387366\n",
      "batch idx: 154 done; train loss: 0.45355117321014404; avg train loss: 1.0521987072888748\n",
      "batch idx: 155 done; train loss: 0.1489391177892685; avg train loss: 1.0464085817151592\n",
      "batch idx: 156 done; train loss: 5.890949726104736; avg train loss: 1.0772655316794242\n",
      "batch idx: 157 done; train loss: 0.010997516103088856; avg train loss: 1.07051699993527\n",
      "batch idx: 158 done; train loss: 0.018602315336465836; avg train loss: 1.0639011843088624\n",
      "batch idx: 159 done; train loss: 0.001517812255769968; avg train loss: 1.0572612882335306\n",
      "batch idx: 160 done; train loss: 0.0073456913232803345; avg train loss: 1.0507400733458894\n",
      "batch idx: 161 done; train loss: 0.19274646043777466; avg train loss: 1.0454438164760862\n",
      "batch idx: 162 done; train loss: 4.927074909210205; avg train loss: 1.0692575041615715\n",
      "Current learning rate: 0.0005\n",
      "\n",
      "Epoch 2/10\n",
      "Train - Loss: 0.9134, Acc: 0.7239, F1: 0.5872, Recall: 0.4211, AUC: 0.9279\n",
      "Val   - Loss: 1.5872, Acc: 0.6727, F1: 0.1000, Recall: 0.0588, AUC: 0.5464\n",
      "batch idx: 0 done; train loss: 0.001189835020340979; avg train loss: 0.001189835020340979\n",
      "batch idx: 1 done; train loss: 0.00013791563105769455; avg train loss: 0.0006638753256993368\n",
      "batch idx: 2 done; train loss: 0.008195343427360058; avg train loss: 0.003174364692919577\n",
      "batch idx: 3 done; train loss: 5.574617385864258; avg train loss: 1.3960351199857541\n",
      "batch idx: 4 done; train loss: 5.48330545425415; avg train loss: 2.2134891868394333\n",
      "batch idx: 5 done; train loss: 0.011858915910124779; avg train loss: 1.8465508083512152\n",
      "batch idx: 6 done; train loss: 0.003228097688406706; avg train loss: 1.5832189925422426\n",
      "batch idx: 7 done; train loss: 0.0020060669630765915; avg train loss: 1.3855673768448469\n",
      "batch idx: 8 done; train loss: 0.02424146980047226; avg train loss: 1.2343089427288052\n",
      "batch idx: 9 done; train loss: 3.9169440269470215; avg train loss: 1.5025724511506269\n",
      "batch idx: 10 done; train loss: 0.002877264516428113; avg train loss: 1.3662365250929724\n",
      "batch idx: 11 done; train loss: 6.240384578704834; avg train loss: 1.7724155295606276\n",
      "batch idx: 12 done; train loss: 0.00025781645672395825; avg train loss: 1.636095705475712\n",
      "batch idx: 13 done; train loss: 0.009816703386604786; avg train loss: 1.5199329196122042\n",
      "batch idx: 14 done; train loss: 4.228116512298584; avg train loss: 1.700478492457963\n",
      "batch idx: 15 done; train loss: 0.01007530651986599; avg train loss: 1.5948282933368318\n",
      "batch idx: 16 done; train loss: 0.002889388706535101; avg train loss: 1.5011848283585791\n",
      "batch idx: 17 done; train loss: 0.004331729840487242; avg train loss: 1.4180263228853518\n",
      "batch idx: 18 done; train loss: 0.004394636023789644; avg train loss: 1.3436246551557958\n",
      "batch idx: 19 done; train loss: 5.760864734649658; avg train loss: 1.564486659130489\n",
      "batch idx: 20 done; train loss: 6.188764572143555; avg train loss: 1.7846903692739684\n",
      "batch idx: 21 done; train loss: 0.12558820843696594; avg train loss: 1.7092766346904682\n",
      "batch idx: 22 done; train loss: 0.793879508972168; avg train loss: 1.6694767596592377\n",
      "batch idx: 23 done; train loss: 0.003663140581920743; avg train loss: 1.6000678588643495\n",
      "batch idx: 24 done; train loss: 1.3481637239456177; avg train loss: 1.5899916934676002\n",
      "batch idx: 25 done; train loss: 0.00390374637208879; avg train loss: 1.5289883108870037\n",
      "batch idx: 26 done; train loss: 0.0033864788711071014; avg train loss: 1.4724845393308594\n",
      "batch idx: 27 done; train loss: 0.042522016912698746; avg train loss: 1.4214144492444964\n",
      "batch idx: 28 done; train loss: 0.061977922916412354; avg train loss: 1.3745373276469763\n",
      "batch idx: 29 done; train loss: 0.08287126570940018; avg train loss: 1.3314817922490572\n",
      "batch idx: 30 done; train loss: 0.24309396743774414; avg train loss: 1.2963725075777244\n",
      "batch idx: 31 done; train loss: 0.4505003094673157; avg train loss: 1.2699390013867742\n",
      "batch idx: 32 done; train loss: 0.027157124131917953; avg train loss: 1.2322789445002633\n",
      "batch idx: 33 done; train loss: 4.646214962005615; avg train loss: 1.332688827368068\n",
      "batch idx: 34 done; train loss: 0.028875228017568588; avg train loss: 1.2954370102437678\n",
      "batch idx: 35 done; train loss: 0.020853202790021896; avg train loss: 1.2600319044811639\n",
      "batch idx: 36 done; train loss: 0.004698664415627718; avg train loss: 1.2261039790739872\n",
      "batch idx: 37 done; train loss: 0.5271032452583313; avg train loss: 1.2077092229209436\n",
      "batch idx: 38 done; train loss: 0.0019904817454516888; avg train loss: 1.1767933577625977\n",
      "batch idx: 39 done; train loss: 4.640283584594727; avg train loss: 1.263380613433401\n",
      "batch idx: 40 done; train loss: 0.33302992582321167; avg train loss: 1.2406891332477865\n",
      "batch idx: 41 done; train loss: 0.00010883215873036534; avg train loss: 1.2111515070313803\n",
      "batch idx: 42 done; train loss: 3.0162177085876465; avg train loss: 1.2531297907885028\n",
      "batch idx: 43 done; train loss: 1.2339907884597778; avg train loss: 1.25269481346285\n",
      "batch idx: 44 done; train loss: 0.0026959760580211878; avg train loss: 1.2249170615205205\n",
      "batch idx: 45 done; train loss: 0.19469180703163147; avg train loss: 1.2025208603359794\n",
      "batch idx: 46 done; train loss: 0.0014785320963710546; avg train loss: 1.176966768245775\n",
      "batch idx: 47 done; train loss: 0.012174345552921295; avg train loss: 1.1527002594396738\n",
      "batch idx: 48 done; train loss: 0.0011069605825468898; avg train loss: 1.1291983553813651\n",
      "batch idx: 49 done; train loss: 0.0014194899704307318; avg train loss: 1.1066427780731465\n",
      "batch idx: 50 done; train loss: 0.010280747897922993; avg train loss: 1.0851454833638283\n",
      "batch idx: 51 done; train loss: 3.116917371749878; avg train loss: 1.1242180196789446\n",
      "batch idx: 52 done; train loss: 0.0002026352594839409; avg train loss: 1.1030101822370681\n",
      "batch idx: 53 done; train loss: 0.05136433616280556; avg train loss: 1.0835352591616187\n",
      "batch idx: 54 done; train loss: 7.235741941258311e-05; avg train loss: 1.0638359336753969\n",
      "batch idx: 55 done; train loss: 0.03894989565014839; avg train loss: 1.0455343972820887\n",
      "batch idx: 56 done; train loss: 0.23613980412483215; avg train loss: 1.031334492138979\n",
      "batch idx: 57 done; train loss: 0.00029583368450403214; avg train loss: 1.0135579635449363\n",
      "batch idx: 58 done; train loss: 0.00039760314393788576; avg train loss: 0.9963857540466144\n",
      "batch idx: 59 done; train loss: 0.000671042304020375; avg train loss: 0.9797905088509045\n",
      "batch idx: 60 done; train loss: 0.7058359384536743; avg train loss: 0.9752994503198024\n",
      "batch idx: 61 done; train loss: 0.160013347864151; avg train loss: 0.9621496744737434\n",
      "batch idx: 62 done; train loss: 0.00018308870494365692; avg train loss: 0.9468803635885245\n",
      "batch idx: 63 done; train loss: 0.09901906549930573; avg train loss: 0.9336325308058804\n",
      "batch idx: 64 done; train loss: 1.6914833784103394; avg train loss: 0.9452917746151798\n",
      "batch idx: 65 done; train loss: 0.03672080859541893; avg train loss: 0.9315255478573046\n",
      "batch idx: 66 done; train loss: 0.0018147443188354373; avg train loss: 0.9176492672074766\n",
      "batch idx: 67 done; train loss: 0.7022541761398315; avg train loss: 0.9144816923388348\n",
      "batch idx: 68 done; train loss: 7.163280010223389; avg train loss: 1.0050439868009298\n",
      "batch idx: 69 done; train loss: 0.2067510187625885; avg train loss: 0.9936398015432393\n",
      "batch idx: 70 done; train loss: 0.002020938089117408; avg train loss: 0.9796733386776882\n",
      "batch idx: 71 done; train loss: 0.00011300401820335537; avg train loss: 0.9660683340296399\n",
      "batch idx: 72 done; train loss: 0.003214908065274358; avg train loss: 0.9528785610712238\n",
      "batch idx: 73 done; train loss: 2.0076682567596436; avg train loss: 0.9671324758778241\n",
      "batch idx: 74 done; train loss: 0.003374479478225112; avg train loss: 0.9542823692591628\n",
      "batch idx: 75 done; train loss: 0.02424379624426365; avg train loss: 0.9420450196142299\n",
      "batch idx: 76 done; train loss: 0.0005103239673189819; avg train loss: 0.9298172962941401\n",
      "batch idx: 77 done; train loss: 0.0011676882859319448; avg train loss: 0.9179115320889067\n",
      "batch idx: 78 done; train loss: 0.006330082193017006; avg train loss: 0.9063725263940221\n",
      "batch idx: 79 done; train loss: 0.0034583541564643383; avg train loss: 0.8950860992410525\n",
      "batch idx: 80 done; train loss: 1.0251946150674485e-05; avg train loss: 0.8840357801386464\n",
      "batch idx: 81 done; train loss: 0.11625546216964722; avg train loss: 0.8746726055292684\n",
      "batch idx: 82 done; train loss: 5.5432586669921875; avg train loss: 0.930920630366171\n",
      "batch idx: 83 done; train loss: 0.001689436612650752; avg train loss: 0.9198583542500577\n",
      "batch idx: 84 done; train loss: 5.650132179260254; avg train loss: 0.9755086345442953\n",
      "batch idx: 85 done; train loss: 0.0003392120997887105; avg train loss: 0.9641694552135451\n",
      "batch idx: 86 done; train loss: 0.17601355910301208; avg train loss: 0.9551101920398609\n",
      "batch idx: 87 done; train loss: 0.16001425683498383; avg train loss: 0.9460750109579873\n",
      "batch idx: 88 done; train loss: 0.0003721021639648825; avg train loss: 0.9354491355782791\n",
      "batch idx: 89 done; train loss: 0.00043156370520591736; avg train loss: 0.9250600514463562\n",
      "batch idx: 90 done; train loss: 1.1522819995880127; avg train loss: 0.9275569959314293\n",
      "batch idx: 91 done; train loss: 0.0003296785580459982; avg train loss: 0.9174784381338925\n",
      "batch idx: 92 done; train loss: 0.0024628089740872383; avg train loss: 0.9076395604009914\n",
      "batch idx: 93 done; train loss: 0.0009557208395563066; avg train loss: 0.8979939876396995\n",
      "batch idx: 94 done; train loss: 3.465292453765869; avg train loss: 0.925018182019975\n",
      "batch idx: 95 done; train loss: 0.01083479356020689; avg train loss: 0.9154954383901858\n",
      "batch idx: 96 done; train loss: 0.0023514972999691963; avg train loss: 0.9060815833274\n",
      "batch idx: 97 done; train loss: 0.007177993655204773; avg train loss: 0.8969090977185\n",
      "batch idx: 98 done; train loss: 0.002944659674540162; avg train loss: 0.8878791538998742\n",
      "batch idx: 99 done; train loss: 7.271502545336261e-05; avg train loss: 0.87900108951113\n",
      "batch idx: 100 done; train loss: 0.009136062115430832; avg train loss: 0.8703885644874102\n",
      "batch idx: 101 done; train loss: 0.005595853086560965; avg train loss: 0.8619102045717156\n",
      "batch idx: 102 done; train loss: 0.0036317838821560144; avg train loss: 0.8535774043708461\n",
      "batch idx: 103 done; train loss: 0.00019905969384126365; avg train loss: 0.8453718433643365\n",
      "batch idx: 104 done; train loss: 0.21695934236049652; avg train loss: 0.8393869624023951\n",
      "batch idx: 105 done; train loss: 0.001303895260207355; avg train loss: 0.8314805183727518\n",
      "batch idx: 106 done; train loss: 0.0003337303060106933; avg train loss: 0.8237127913814738\n",
      "batch idx: 107 done; train loss: 0.002667204709723592; avg train loss: 0.8161105174308095\n",
      "batch idx: 108 done; train loss: 0.48163890838623047; avg train loss: 0.8130419705588409\n",
      "batch idx: 109 done; train loss: 0.014431975781917572; avg train loss: 0.8057818796972325\n",
      "batch idx: 110 done; train loss: 0.002894618781283498; avg train loss: 0.7985486611304221\n",
      "batch idx: 111 done; train loss: 0.0006820021662861109; avg train loss: 0.7914248516753852\n",
      "batch idx: 112 done; train loss: 0.00025376438861712813; avg train loss: 0.7844233376285996\n",
      "batch idx: 113 done; train loss: 0.008120733313262463; avg train loss: 0.7776136656609213\n",
      "batch idx: 114 done; train loss: 1.3829882144927979; avg train loss: 0.7828777921725028\n",
      "batch idx: 115 done; train loss: 0.03329576551914215; avg train loss: 0.776415878149629\n",
      "batch idx: 116 done; train loss: 0.000732868502382189; avg train loss: 0.7697861088364046\n",
      "batch idx: 117 done; train loss: 0.001505433232523501; avg train loss: 0.7632752556533209\n",
      "batch idx: 118 done; train loss: 0.005697202868759632; avg train loss: 0.7569090535290809\n",
      "batch idx: 119 done; train loss: 4.712147235870361; avg train loss: 0.7898693717152583\n",
      "batch idx: 120 done; train loss: 3.5359933376312256; avg train loss: 0.8125646111029935\n",
      "batch idx: 121 done; train loss: 0.00015937011630740017; avg train loss: 0.8059055517506436\n",
      "batch idx: 122 done; train loss: 0.0032900036312639713; avg train loss: 0.7993802220911365\n",
      "batch idx: 123 done; train loss: 0.00036161558819003403; avg train loss: 0.7929365236515966\n",
      "batch idx: 124 done; train loss: 0.00025769727653823793; avg train loss: 0.7865950930405962\n",
      "batch idx: 125 done; train loss: 4.235988616943359; avg train loss: 0.8139712321191894\n",
      "batch idx: 126 done; train loss: 0.00020632999076042324; avg train loss: 0.8075636344646349\n",
      "batch idx: 127 done; train loss: 0.46896523237228394; avg train loss: 0.8049183344482884\n",
      "batch idx: 128 done; train loss: 0.0017880895175039768; avg train loss: 0.7986925185961118\n",
      "batch idx: 129 done; train loss: 0.000388665939681232; avg train loss: 0.7925517197295239\n",
      "batch idx: 130 done; train loss: 0.0009614374139346182; avg train loss: 0.7865090458187178\n",
      "batch idx: 131 done; train loss: 9.929640509653836e-05; avg train loss: 0.780551396201948\n",
      "batch idx: 132 done; train loss: 0.00045468006283044815; avg train loss: 0.7746860073587968\n",
      "batch idx: 133 done; train loss: 0.0013833011034876108; avg train loss: 0.7689150916404736\n",
      "batch idx: 134 done; train loss: 2.571810245513916; avg train loss: 0.7822698705580545\n",
      "batch idx: 135 done; train loss: 0.0009308296139352024; avg train loss: 0.7765247305511125\n",
      "batch idx: 136 done; train loss: 3.6930062770843506; avg train loss: 0.7978129170221581\n",
      "batch idx: 137 done; train loss: 0.0004724340105894953; avg train loss: 0.7920350874351177\n",
      "batch idx: 138 done; train loss: 6.223893165588379; avg train loss: 0.831113203105285\n",
      "batch idx: 139 done; train loss: 6.890059739816934e-05; avg train loss: 0.8251771723730859\n",
      "batch idx: 140 done; train loss: 0.002973779570311308; avg train loss: 0.819345942636896\n",
      "batch idx: 141 done; train loss: 0.0018195039592683315; avg train loss: 0.8135887141955043\n",
      "batch idx: 142 done; train loss: 0.11752010136842728; avg train loss: 0.8087211015183918\n",
      "batch idx: 143 done; train loss: 0.001524715917184949; avg train loss: 0.8031155710628278\n",
      "batch idx: 144 done; train loss: 0.00026639728457666934; avg train loss: 0.7975786802091848\n",
      "batch idx: 145 done; train loss: 0.1501666158437729; avg train loss: 0.7931443510012025\n",
      "batch idx: 146 done; train loss: 1.997895359992981; avg train loss: 0.8013399360963847\n",
      "batch idx: 147 done; train loss: 0.0003053675754927099; avg train loss: 0.7959275403631354\n",
      "batch idx: 148 done; train loss: 5.526353359222412; avg train loss: 0.8276753646507815\n",
      "batch idx: 149 done; train loss: 0.5075355172157288; avg train loss: 0.8255410990012145\n",
      "batch idx: 150 done; train loss: 0.26947492361068726; avg train loss: 0.8218585415482971\n",
      "batch idx: 151 done; train loss: 0.659870445728302; avg train loss: 0.8207928303915866\n",
      "batch idx: 152 done; train loss: 0.000545472139492631; avg train loss: 0.8154317365468017\n",
      "batch idx: 153 done; train loss: 1.442852258682251; avg train loss: 0.8195058957814475\n",
      "batch idx: 154 done; train loss: 6.1025309562683105; avg train loss: 0.8535899284297498\n",
      "batch idx: 155 done; train loss: 5.72221565246582; avg train loss: 0.8847990676863913\n",
      "batch idx: 156 done; train loss: 0.00044312194222584367; avg train loss: 0.8791662272676386\n",
      "batch idx: 157 done; train loss: 5.036037921905518; avg train loss: 0.9054755417906631\n",
      "batch idx: 158 done; train loss: 0.00970514491200447; avg train loss: 0.8998417657096653\n",
      "batch idx: 159 done; train loss: 0.0010925520909950137; avg train loss: 0.8942245831245487\n",
      "batch idx: 160 done; train loss: 0.07321066409349442; avg train loss: 0.889125117788952\n",
      "batch idx: 161 done; train loss: 0.0002431573811918497; avg train loss: 0.8836381921074227\n",
      "batch idx: 162 done; train loss: 0.029520811513066292; avg train loss: 0.8783982081773959\n",
      "Current learning rate: 0.0005\n",
      "\n",
      "Epoch 3/10\n",
      "Train - Loss: 0.4950, Acc: 0.8160, F1: 0.8315, Recall: 0.8916, AUC: 0.9221\n",
      "Val   - Loss: 1.9461, Acc: 0.5091, F1: 0.4000, Recall: 0.5294, AUC: 0.4675\n",
      "batch idx: 0 done; train loss: 0.009985493496060371; avg train loss: 0.009985493496060371\n",
      "batch idx: 1 done; train loss: 0.0001389883691444993; avg train loss: 0.005062240932602435\n",
      "batch idx: 2 done; train loss: 0.05953167378902435; avg train loss: 0.02321871855140974\n",
      "batch idx: 3 done; train loss: 0.00021050144277978688; avg train loss: 0.017466664274252253\n",
      "batch idx: 4 done; train loss: 0.009422711096704006; avg train loss: 0.015857873638742602\n",
      "batch idx: 5 done; train loss: 1.0326058864593506; avg train loss: 0.1853158757755106\n",
      "batch idx: 6 done; train loss: 0.0183109138160944; avg train loss: 0.16145802406702256\n",
      "batch idx: 7 done; train loss: 2.0503786799963564e-05; avg train loss: 0.14127833403199475\n",
      "batch idx: 8 done; train loss: 0.3199327290058136; avg train loss: 0.16112882236241907\n",
      "batch idx: 9 done; train loss: 0.0017625049222260714; avg train loss: 0.14519219061839977\n",
      "batch idx: 10 done; train loss: 4.362591743469238; avg train loss: 0.528592149968476\n",
      "batch idx: 11 done; train loss: 0.09944233298301697; avg train loss: 0.49282966521968774\n",
      "batch idx: 12 done; train loss: 0.0008142255246639252; avg train loss: 0.4549823237046859\n",
      "batch idx: 13 done; train loss: 0.0010119322687387466; avg train loss: 0.4225558671735468\n",
      "batch idx: 14 done; train loss: 0.0038140900433063507; avg train loss: 0.39463974869819746\n",
      "batch idx: 15 done; train loss: 0.0004349001101218164; avg train loss: 0.37000194566144273\n",
      "batch idx: 16 done; train loss: 0.04998547211289406; avg train loss: 0.35117744721741045\n",
      "batch idx: 17 done; train loss: 4.245423793792725; avg train loss: 0.5675244664715946\n",
      "batch idx: 18 done; train loss: 0.027312573045492172; avg train loss: 0.5390922615544312\n",
      "batch idx: 19 done; train loss: 4.306842803955078; avg train loss: 0.7274797886744636\n",
      "batch idx: 20 done; train loss: 0.041759587824344635; avg train loss: 0.6948264457768389\n",
      "batch idx: 21 done; train loss: 0.009463215246796608; avg train loss: 0.663673571661837\n",
      "batch idx: 22 done; train loss: 0.0002668739762157202; avg train loss: 0.6348298021972447\n",
      "batch idx: 23 done; train loss: 2.4467573165893555; avg train loss: 0.7103267819635827\n",
      "batch idx: 24 done; train loss: 0.00027509720530360937; avg train loss: 0.6819247145732515\n",
      "batch idx: 25 done; train loss: 0.001122916815802455; avg train loss: 0.6557400300441188\n",
      "batch idx: 26 done; train loss: 0.002686821622774005; avg train loss: 0.6315528741766617\n",
      "batch idx: 27 done; train loss: 4.115179538726807; avg train loss: 0.7559681121963097\n",
      "batch idx: 28 done; train loss: 0.7013550996780396; avg train loss: 0.7540849048680935\n",
      "batch idx: 29 done; train loss: 0.004866066388785839; avg train loss: 0.7291109435854499\n",
      "batch idx: 30 done; train loss: 2.2862234115600586; avg train loss: 0.7793403780362438\n",
      "batch idx: 31 done; train loss: 0.0073085324838757515; avg train loss: 0.7552143828627322\n",
      "batch idx: 32 done; train loss: 0.0028363734018057585; avg train loss: 0.7324150492427042\n",
      "batch idx: 33 done; train loss: 0.005598934832960367; avg train loss: 0.7110381047012411\n",
      "batch idx: 34 done; train loss: 2.0458412170410156; avg train loss: 0.7491753364823776\n",
      "batch idx: 35 done; train loss: 0.013497992418706417; avg train loss: 0.7287398547028311\n",
      "batch idx: 36 done; train loss: 2.319037437438965; avg train loss: 0.7717208704524563\n",
      "batch idx: 37 done; train loss: 0.006987658329308033; avg train loss: 0.7515963122386893\n",
      "batch idx: 38 done; train loss: 0.02968098595738411; avg train loss: 0.7330856628468609\n",
      "batch idx: 39 done; train loss: 0.014509875327348709; avg train loss: 0.7151212681588731\n",
      "batch idx: 40 done; train loss: 0.0065695736557245255; avg train loss: 0.6978395195124549\n",
      "batch idx: 41 done; train loss: 0.7195425033569336; avg train loss: 0.6983562572230377\n",
      "batch idx: 42 done; train loss: 0.021038932725787163; avg train loss: 0.6826046915370552\n",
      "batch idx: 43 done; train loss: 0.008518076501786709; avg train loss: 0.6672845411953445\n",
      "batch idx: 44 done; train loss: 0.005357550922781229; avg train loss: 0.6525750525226208\n",
      "batch idx: 45 done; train loss: 0.0369795523583889; avg train loss: 0.6391925416494854\n",
      "batch idx: 46 done; train loss: 0.017939221113920212; avg train loss: 0.6259743858934095\n",
      "batch idx: 47 done; train loss: 0.008282361552119255; avg train loss: 0.6131058020529659\n",
      "batch idx: 48 done; train loss: 0.14473775029182434; avg train loss: 0.6035472703843713\n",
      "batch idx: 49 done; train loss: 0.00012170527770649642; avg train loss: 0.5914787590822379\n",
      "batch idx: 50 done; train loss: 0.0004029654082842171; avg train loss: 0.5798890376376507\n",
      "batch idx: 51 done; train loss: 0.09442625194787979; avg train loss: 0.5705532148359242\n",
      "batch idx: 52 done; train loss: 0.07270808517932892; avg train loss: 0.5611599105027809\n",
      "batch idx: 53 done; train loss: 0.08839234709739685; avg train loss: 0.5524049556249034\n",
      "batch idx: 54 done; train loss: 0.07615233957767487; avg train loss: 0.5437458171513175\n",
      "batch idx: 55 done; train loss: 0.00040260792593471706; avg train loss: 0.5340432598437214\n",
      "batch idx: 56 done; train loss: 0.009161104448139668; avg train loss: 0.5248348009771322\n",
      "batch idx: 57 done; train loss: 0.260109007358551; avg train loss: 0.5202705631561222\n",
      "batch idx: 58 done; train loss: 6.8662193370983e-05; avg train loss: 0.5114535817838722\n",
      "batch idx: 59 done; train loss: 0.0002317160106031224; avg train loss: 0.5029332173543177\n",
      "batch idx: 60 done; train loss: 0.00293717160820961; avg train loss: 0.4947365608666766\n",
      "batch idx: 61 done; train loss: 0.22267594933509827; avg train loss: 0.49034848648713497\n",
      "batch idx: 62 done; train loss: 9.417090768693015e-05; avg train loss: 0.4825666719541279\n",
      "batch idx: 63 done; train loss: 0.0026844439562410116; avg train loss: 0.4750685121416609\n",
      "batch idx: 64 done; train loss: 0.0007397775771096349; avg train loss: 0.46777114699451394\n",
      "batch idx: 65 done; train loss: 0.0033684202935546637; avg train loss: 0.46073474204449943\n",
      "batch idx: 66 done; train loss: 0.0014996008248999715; avg train loss: 0.45388048620540095\n",
      "batch idx: 67 done; train loss: 0.000739539333153516; avg train loss: 0.44721664875139727\n",
      "batch idx: 68 done; train loss: 0.0019482456846162677; avg train loss: 0.4407634834895599\n",
      "batch idx: 69 done; train loss: 0.004439260810613632; avg train loss: 0.4345302803084321\n",
      "batch idx: 70 done; train loss: 0.0001323135511483997; avg train loss: 0.4284119990864985\n",
      "batch idx: 71 done; train loss: 0.019343547523021698; avg train loss: 0.42273049281478353\n",
      "batch idx: 72 done; train loss: 0.8527446389198303; avg train loss: 0.42862109755594857\n",
      "batch idx: 73 done; train loss: 0.7371159791946411; avg train loss: 0.4327899473078228\n",
      "batch idx: 74 done; train loss: 0.0014675810234621167; avg train loss: 0.4270389824240313\n",
      "batch idx: 75 done; train loss: 0.0012206730898469687; avg train loss: 0.42143610993279207\n",
      "batch idx: 76 done; train loss: 0.00813858862966299; avg train loss: 0.416068609656128\n",
      "batch idx: 77 done; train loss: 0.0002643712505232543; avg train loss: 0.41073778608682543\n",
      "batch idx: 78 done; train loss: 1.5131839513778687; avg train loss: 0.42469280083734495\n",
      "batch idx: 79 done; train loss: 5.8212103843688965; avg train loss: 0.49214927063148933\n",
      "batch idx: 80 done; train loss: 0.0004935238393954933; avg train loss: 0.48607944659701907\n",
      "batch idx: 81 done; train loss: 0.0016362386522814631; avg train loss: 0.480171602597693\n",
      "batch idx: 82 done; train loss: 0.0008062449633143842; avg train loss: 0.4743961163611342\n",
      "batch idx: 83 done; train loss: 2.33798885345459; avg train loss: 0.49658174418367534\n",
      "batch idx: 84 done; train loss: 0.1688285768032074; avg train loss: 0.4927258245674345\n",
      "batch idx: 85 done; train loss: 0.005738214123994112; avg train loss: 0.48706317793437126\n",
      "batch idx: 86 done; train loss: 0.005655952729284763; avg train loss: 0.4815297615527036\n",
      "batch idx: 87 done; train loss: 0.3482826054096222; avg train loss: 0.48001558932380495\n",
      "batch idx: 88 done; train loss: 0.00010597144137136638; avg train loss: 0.47462334642624954\n",
      "batch idx: 89 done; train loss: 0.0004619484825525433; avg train loss: 0.46935488644909734\n",
      "batch idx: 90 done; train loss: 5.130864143371582; avg train loss: 0.520580262898795\n",
      "batch idx: 91 done; train loss: 0.0004967409186065197; avg train loss: 0.5149271811381407\n",
      "batch idx: 92 done; train loss: 0.008787532337009907; avg train loss: 0.5094848193230749\n",
      "batch idx: 93 done; train loss: 0.002422019839286804; avg train loss: 0.5040905342221834\n",
      "batch idx: 94 done; train loss: 0.026408543810248375; avg train loss: 0.4990623027441631\n",
      "batch idx: 95 done; train loss: 0.00033420699764974415; avg train loss: 0.49386721841347025\n",
      "batch idx: 96 done; train loss: 0.00040642108069732785; avg train loss: 0.4887799936986994\n",
      "batch idx: 97 done; train loss: 2.380481243133545; avg train loss: 0.5080830676725243\n",
      "batch idx: 98 done; train loss: 0.8770797252655029; avg train loss: 0.51181030663811\n",
      "batch idx: 99 done; train loss: 0.0408465601503849; avg train loss: 0.5071006691732327\n",
      "batch idx: 100 done; train loss: 0.000364713923772797; avg train loss: 0.5020834814974955\n",
      "batch idx: 101 done; train loss: 0.00026353701832704246; avg train loss: 0.49716367812024875\n",
      "batch idx: 102 done; train loss: 0.368341326713562; avg train loss: 0.4959129756794072\n",
      "batch idx: 103 done; train loss: 0.5362881422042847; avg train loss: 0.496301198434454\n",
      "batch idx: 104 done; train loss: 7.9435200691223145; avg train loss: 0.5672270924410051\n",
      "batch idx: 105 done; train loss: 0.00018809456378221512; avg train loss: 0.5618776679327294\n",
      "batch idx: 106 done; train loss: 0.0007587176514789462; avg train loss: 0.5566335655936523\n",
      "batch idx: 107 done; train loss: 0.9132441282272339; avg train loss: 0.5599355152476669\n",
      "batch idx: 108 done; train loss: 7.35250997543335; avg train loss: 0.6222527121301044\n",
      "batch idx: 109 done; train loss: 0.0011198208667337894; avg train loss: 0.6166060494822556\n",
      "batch idx: 110 done; train loss: 0.0014379409840330482; avg train loss: 0.6110639944507401\n",
      "batch idx: 111 done; train loss: 0.00021288513380568475; avg train loss: 0.6056099666889817\n",
      "batch idx: 112 done; train loss: 5.9126061387360096e-05; avg train loss: 0.6002511096922774\n",
      "batch idx: 113 done; train loss: 0.10208871215581894; avg train loss: 0.5958812640998523\n",
      "batch idx: 114 done; train loss: 6.806059837341309; avg train loss: 0.6498828169106475\n",
      "batch idx: 115 done; train loss: 0.0004465774691198021; avg train loss: 0.6442842286395999\n",
      "batch idx: 116 done; train loss: 0.006303310859948397; avg train loss: 0.6388314002825088\n",
      "batch idx: 117 done; train loss: 0.00028451209072954953; avg train loss: 0.6334199859757988\n",
      "batch idx: 118 done; train loss: 0.0016881275223568082; avg train loss: 0.6281113148963582\n",
      "batch idx: 119 done; train loss: 0.01008875947445631; avg train loss: 0.622961126934509\n",
      "batch idx: 120 done; train loss: 0.000844479538500309; avg train loss: 0.6178196670386742\n",
      "batch idx: 121 done; train loss: 0.002176774898543954; avg train loss: 0.6127734138244109\n",
      "batch idx: 122 done; train loss: 2.063730239868164; avg train loss: 0.6245698107841161\n",
      "batch idx: 123 done; train loss: 0.0156882144510746; avg train loss: 0.6196594753298175\n",
      "batch idx: 124 done; train loss: 0.00039319414645433426; avg train loss: 0.6147053450803506\n",
      "batch idx: 125 done; train loss: 0.002542241709306836; avg train loss: 0.6098469077520089\n",
      "batch idx: 126 done; train loss: 0.002685514045879245; avg train loss: 0.6050661093763701\n",
      "batch idx: 127 done; train loss: 0.000271879427600652; avg train loss: 0.6003411544548953\n",
      "batch idx: 128 done; train loss: 0.0005952732171863317; avg train loss: 0.5956919615770836\n",
      "batch idx: 129 done; train loss: 0.00029380773776210845; avg train loss: 0.5911119757783196\n",
      "batch idx: 130 done; train loss: 0.00025662468397058547; avg train loss: 0.5866016295867597\n",
      "batch idx: 131 done; train loss: 0.0009365463047288358; avg train loss: 0.5821647728952292\n",
      "batch idx: 132 done; train loss: 0.001302228425629437; avg train loss: 0.5777973853428262\n",
      "batch idx: 133 done; train loss: 0.0004002247005701065; avg train loss: 0.5734884513081825\n",
      "batch idx: 134 done; train loss: 0.004834866151213646; avg train loss: 0.5692762025292419\n",
      "batch idx: 135 done; train loss: 0.00042441420373506844; avg train loss: 0.5650934687915544\n",
      "batch idx: 136 done; train loss: 0.28853923082351685; avg train loss: 0.563074824718795\n",
      "batch idx: 137 done; train loss: 0.0001995364436879754; avg train loss: 0.5589960182820188\n",
      "batch idx: 138 done; train loss: 0.00014518637908622622; avg train loss: 0.5549755086999834\n",
      "batch idx: 139 done; train loss: 0.0003296785580459982; avg train loss: 0.5510137527703981\n",
      "batch idx: 140 done; train loss: 0.00669900793582201; avg train loss: 0.54715336450916\n",
      "batch idx: 141 done; train loss: 0.0002632986579556018; avg train loss: 0.5433020260172501\n",
      "batch idx: 142 done; train loss: 0.00037174468161538243; avg train loss: 0.5395053107631548\n",
      "batch idx: 143 done; train loss: 0.32363787293434143; avg train loss: 0.538006231333788\n",
      "batch idx: 144 done; train loss: 0.0026336766313761473; avg train loss: 0.534314006818599\n",
      "batch idx: 145 done; train loss: 8.964136941358447e-05; avg train loss: 0.5306549358223717\n",
      "batch idx: 146 done; train loss: 0.00013326710904948413; avg train loss: 0.5270459448787436\n",
      "batch idx: 147 done; train loss: 0.0002101439022226259; avg train loss: 0.5234862435207941\n",
      "batch idx: 148 done; train loss: 0.006066125351935625; avg train loss: 0.5200136252780501\n",
      "batch idx: 149 done; train loss: 0.0005398723296821117; avg train loss: 0.516550466925061\n",
      "batch idx: 150 done; train loss: 0.014796985313296318; avg train loss: 0.5132275961859102\n",
      "batch idx: 151 done; train loss: 0.2873329818248749; avg train loss: 0.5117414474072192\n",
      "batch idx: 152 done; train loss: 0.3905097246170044; avg train loss: 0.510949083205976\n",
      "batch idx: 153 done; train loss: 1.2149457931518555; avg train loss: 0.5155204904134167\n",
      "batch idx: 154 done; train loss: 0.00014161060971673578; avg train loss: 0.5121954653824251\n",
      "batch idx: 155 done; train loss: 0.0009658439084887505; avg train loss: 0.5089183524242589\n",
      "batch idx: 156 done; train loss: 0.004629963543266058; avg train loss: 0.5057063244696028\n",
      "batch idx: 157 done; train loss: 0.0014077048981562257; avg train loss: 0.5025145610545937\n",
      "batch idx: 158 done; train loss: 0.0011999557027593255; avg train loss: 0.4993616390083558\n",
      "batch idx: 159 done; train loss: 0.001259725191630423; avg train loss: 0.4962485020470012\n",
      "batch idx: 160 done; train loss: 0.00014041867689229548; avg train loss: 0.4931670853801062\n",
      "batch idx: 161 done; train loss: 0.018046118319034576; avg train loss: 0.4902342399044205\n",
      "batch idx: 162 done; train loss: 0.0007150000892579556; avg train loss: 0.4872310543840821\n",
      "Current learning rate: 5e-05\n",
      "\n",
      "Epoch 4/10\n",
      "Train - Loss: 0.4222, Acc: 0.9202, F1: 0.9065, Recall: 0.8514, AUC: 0.9543\n",
      "Val   - Loss: 1.9478, Acc: 0.6000, F1: 0.1538, Recall: 0.1176, AUC: 0.5294\n",
      "batch idx: 0 done; train loss: 0.1212286502122879; avg train loss: 0.1212286502122879\n",
      "batch idx: 1 done; train loss: 0.17382562160491943; avg train loss: 0.14752713590860367\n",
      "batch idx: 2 done; train loss: 0.00041106835124082863; avg train loss: 0.09848844672281605\n",
      "batch idx: 3 done; train loss: 0.00033098942367359996; avg train loss: 0.07394908239803044\n",
      "batch idx: 4 done; train loss: 0.0009556017466820776; avg train loss: 0.05935038626776077\n",
      "batch idx: 5 done; train loss: 0.0298805870115757; avg train loss: 0.05443875305839659\n",
      "batch idx: 6 done; train loss: 0.004763801582157612; avg train loss: 0.04734233141893388\n",
      "batch idx: 7 done; train loss: 0.002627375302836299; avg train loss: 0.04175296190442168\n",
      "batch idx: 8 done; train loss: 0.0024307011626660824; avg train loss: 0.03738382182200439\n",
      "batch idx: 9 done; train loss: 0.0007177399238571525; avg train loss: 0.03371721363218967\n",
      "batch idx: 10 done; train loss: 0.0008791395812295377; avg train loss: 0.030731934173011476\n",
      "batch idx: 11 done; train loss: 7.986990567587782e-06; avg train loss: 0.02817160524114115\n",
      "batch idx: 12 done; train loss: 0.0012497241841629148; avg train loss: 0.026100691313681288\n",
      "batch idx: 13 done; train loss: 0.0021798675879836082; avg train loss: 0.024392061047560025\n",
      "batch idx: 14 done; train loss: 6.031808152329177e-05; avg train loss: 0.022769944849824243\n",
      "batch idx: 15 done; train loss: 0.00039867559098638594; avg train loss: 0.021371740521146876\n",
      "batch idx: 16 done; train loss: 2.4254183769226074; avg train loss: 0.1627862485447622\n",
      "batch idx: 17 done; train loss: 0.00017998983094003052; avg train loss: 0.1537525675051054\n",
      "batch idx: 18 done; train loss: 0.00817358773201704; avg train loss: 0.14609051593810077\n",
      "batch idx: 19 done; train loss: 0.00388985313475132; avg train loss: 0.1389804827979333\n",
      "batch idx: 20 done; train loss: 3.7431014789035544e-05; avg train loss: 0.13236414699873594\n",
      "batch idx: 21 done; train loss: 0.053325213491916656; avg train loss: 0.12877146820297145\n",
      "batch idx: 22 done; train loss: 0.0022684813011437654; avg train loss: 0.12327133833767458\n",
      "batch idx: 23 done; train loss: 0.00018976318824570626; avg train loss: 0.11814293937311504\n",
      "batch idx: 24 done; train loss: 0.010489214211702347; avg train loss: 0.11383679036665853\n",
      "batch idx: 25 done; train loss: 0.00649484246969223; avg train loss: 0.10970825390908291\n",
      "batch idx: 26 done; train loss: 0.00019298121333122253; avg train loss: 0.10565213269812913\n",
      "batch idx: 27 done; train loss: 0.0002791491860989481; avg train loss: 0.10188881185841378\n",
      "batch idx: 28 done; train loss: 0.0014747231034561992; avg train loss: 0.09842625707376007\n",
      "batch idx: 29 done; train loss: 0.0001787979417713359; avg train loss: 0.09515134176936044\n",
      "batch idx: 30 done; train loss: 5.445734024047852; avg train loss: 0.26775078313318273\n",
      "batch idx: 31 done; train loss: 0.004849695134907961; avg train loss: 0.25953512413323665\n",
      "batch idx: 32 done; train loss: 1.5258672647178173e-05; avg train loss: 0.25167088578594604\n",
      "batch idx: 33 done; train loss: 1.5616295058862306e-05; avg train loss: 0.24426926021268466\n",
      "batch idx: 34 done; train loss: 0.009669018909335136; avg train loss: 0.23756639617544612\n",
      "batch idx: 35 done; train loss: 7.760223525110632e-05; avg train loss: 0.23096948523266292\n",
      "batch idx: 36 done; train loss: 0.00048065552255138755; avg train loss: 0.2247400574026599\n",
      "batch idx: 37 done; train loss: 0.06062215194106102; avg train loss: 0.22042116515367047\n",
      "batch idx: 38 done; train loss: 0.11564834415912628; avg train loss: 0.21773468256406675\n",
      "batch idx: 39 done; train loss: 0.00030858523678034544; avg train loss: 0.2122990301308846\n",
      "batch idx: 40 done; train loss: 0.000316927267704159; avg train loss: 0.20712873493909972\n",
      "batch idx: 41 done; train loss: 0.00040165462996810675; avg train loss: 0.2022066615984061\n",
      "batch idx: 42 done; train loss: 2.610700845718384; avg train loss: 0.25821815425235906\n",
      "batch idx: 43 done; train loss: 0.0012215065071359277; avg train loss: 0.25237732134905855\n",
      "batch idx: 44 done; train loss: 2.9168806076049805; avg train loss: 0.31158850548807904\n",
      "batch idx: 45 done; train loss: 0.0005999195855110884; avg train loss: 0.3048278840554145\n",
      "batch idx: 46 done; train loss: 0.0002588890492916107; avg train loss: 0.29834769267230554\n",
      "batch idx: 47 done; train loss: 0.0005988473421894014; avg train loss: 0.2921445917279281\n",
      "batch idx: 48 done; train loss: 0.0007443041540682316; avg train loss: 0.2861976470835636\n",
      "batch idx: 49 done; train loss: 0.0001401803019689396; avg train loss: 0.2804764977479317\n",
      "batch idx: 50 done; train loss: 0.0020706658251583576; avg train loss: 0.27501755986709303\n",
      "batch idx: 51 done; train loss: 7.208343982696533; avg train loss: 0.4083507603061207\n",
      "batch idx: 52 done; train loss: 2.9070019721984863; avg train loss: 0.4554951227946559\n",
      "batch idx: 53 done; train loss: 0.0009978797752410173; avg train loss: 0.44707850718318526\n",
      "batch idx: 54 done; train loss: 0.0005864569102413952; avg train loss: 0.43896046990549537\n",
      "batch idx: 55 done; train loss: 1.8386608362197876; avg train loss: 0.4639551193039649\n",
      "batch idx: 56 done; train loss: 0.00043501926120370626; avg train loss: 0.45582318772426733\n",
      "batch idx: 57 done; train loss: 0.0006055190460756421; avg train loss: 0.44797460722981575\n",
      "batch idx: 58 done; train loss: 0.12381797283887863; avg train loss: 0.44248042698590156\n",
      "batch idx: 59 done; train loss: 0.0012388896429911256; avg train loss: 0.4351264013635197\n",
      "batch idx: 60 done; train loss: 5.294520854949951; avg train loss: 0.5147886055206743\n",
      "batch idx: 61 done; train loss: 1.597391747054644e-05; avg train loss: 0.5064858211399775\n",
      "batch idx: 62 done; train loss: 0.00038961926475167274; avg train loss: 0.498452548094339\n",
      "batch idx: 63 done; train loss: 0.0013777059502899647; avg train loss: 0.4906857536858382\n",
      "batch idx: 64 done; train loss: 0.0013002045452594757; avg train loss: 0.4831567452375216\n",
      "batch idx: 65 done; train loss: 0.0002649671514518559; avg train loss: 0.47584020314530845\n",
      "batch idx: 66 done; train loss: 0.06830400228500366; avg train loss: 0.4697575732817218\n",
      "batch idx: 67 done; train loss: 0.0003773453936446458; avg train loss: 0.462854922871603\n",
      "batch idx: 68 done; train loss: 0.10564634203910828; avg train loss: 0.4576779869175089\n",
      "batch idx: 69 done; train loss: 2.898771047592163; avg train loss: 0.49255074492714684\n",
      "batch idx: 70 done; train loss: 0.000753476400859654; avg train loss: 0.48562402283522726\n",
      "batch idx: 71 done; train loss: 0.0010515881003811955; avg train loss: 0.4788938501305766\n",
      "batch idx: 72 done; train loss: 0.0009303532424382865; avg train loss: 0.47234640496772545\n",
      "batch idx: 73 done; train loss: 0.002389316214248538; avg train loss: 0.46599563349808387\n",
      "batch idx: 74 done; train loss: 0.0042140972800552845; avg train loss: 0.4598385463485101\n",
      "batch idx: 75 done; train loss: 0.005615531001240015; avg train loss: 0.45386192772551975\n",
      "batch idx: 76 done; train loss: 0.0005305789527483284; avg train loss: 0.44797450761158764\n",
      "batch idx: 77 done; train loss: 0.00015925093612167984; avg train loss: 0.4422332863721586\n",
      "batch idx: 78 done; train loss: 0.0001463782973587513; avg train loss: 0.4366372495610852\n",
      "batch idx: 79 done; train loss: 6.74203634262085; avg train loss: 0.5154547382243322\n",
      "batch idx: 80 done; train loss: 0.0005314130103215575; avg train loss: 0.5090976601352704\n",
      "batch idx: 81 done; train loss: 0.00015937011630740017; avg train loss: 0.502891095622844\n",
      "batch idx: 82 done; train loss: 0.011122134514153004; avg train loss: 0.4969661683805706\n",
      "batch idx: 83 done; train loss: 0.07802015542984009; avg train loss: 0.49197871584544284\n",
      "batch idx: 84 done; train loss: 0.0008761619683355093; avg train loss: 0.4862010387410063\n",
      "batch idx: 85 done; train loss: 0.0017258524894714355; avg train loss: 0.48056760634273266\n",
      "batch idx: 86 done; train loss: 7.152129173278809; avg train loss: 0.5572522220546415\n",
      "batch idx: 87 done; train loss: 0.00020287363440729678; avg train loss: 0.5509221158225934\n",
      "batch idx: 88 done; train loss: 1.8355430364608765; avg train loss: 0.5653560587511135\n",
      "batch idx: 89 done; train loss: 1.811685562133789; avg train loss: 0.5792041643442544\n",
      "batch idx: 90 done; train loss: 0.003148600459098816; avg train loss: 0.5728738834224394\n",
      "batch idx: 91 done; train loss: 0.0013583013787865639; avg train loss: 0.5666617575306606\n",
      "batch idx: 92 done; train loss: 0.001090885023586452; avg train loss: 0.5605803502994017\n",
      "batch idx: 93 done; train loss: 0.024736667051911354; avg train loss: 0.5548798855840029\n",
      "batch idx: 94 done; train loss: 0.005242529325187206; avg train loss: 0.5490942292023312\n",
      "batch idx: 95 done; train loss: 0.08427370339632034; avg train loss: 0.5442523487251852\n",
      "batch idx: 96 done; train loss: 0.4585583209991455; avg train loss: 0.5433689051403807\n",
      "batch idx: 97 done; train loss: 0.00013147920253686607; avg train loss: 0.5378256661001986\n",
      "batch idx: 98 done; train loss: 0.36998501420021057; avg train loss: 0.5361303059799967\n",
      "batch idx: 99 done; train loss: 2.0214314460754395; avg train loss: 0.5509833173809511\n",
      "batch idx: 100 done; train loss: 0.00014900050882715732; avg train loss: 0.5455295122634053\n",
      "batch idx: 101 done; train loss: 0.0004553949984256178; avg train loss: 0.5401856483686507\n",
      "batch idx: 102 done; train loss: 0.00019226610311307013; avg train loss: 0.5349429941718978\n",
      "batch idx: 103 done; train loss: 0.00013731967192143202; avg train loss: 0.5298006319170904\n",
      "batch idx: 104 done; train loss: 0.0023925271816551685; avg train loss: 0.5247776975862767\n",
      "batch idx: 105 done; train loss: 9.655486064730212e-05; avg train loss: 0.5198278754850916\n",
      "batch idx: 106 done; train loss: 0.0012642494402825832; avg train loss: 0.5149814864566353\n",
      "batch idx: 107 done; train loss: 0.0009896624833345413; avg train loss: 0.510222302901327\n",
      "batch idx: 108 done; train loss: 0.0002517383254598826; avg train loss: 0.505543673868521\n",
      "batch idx: 109 done; train loss: 0.0010737375123426318; avg train loss: 0.5009575835380102\n",
      "batch idx: 110 done; train loss: 0.022238753736019135; avg train loss: 0.4966448012875418\n",
      "batch idx: 111 done; train loss: 0.0032587540335953236; avg train loss: 0.4922395687227744\n",
      "batch idx: 112 done; train loss: 0.013975496403872967; avg train loss: 0.48800714330402306\n",
      "batch idx: 113 done; train loss: 0.0020231986418366432; avg train loss: 0.48374412624558283\n",
      "batch idx: 114 done; train loss: 7.629103492945433e-05; avg train loss: 0.4795383189828815\n",
      "batch idx: 115 done; train loss: 0.19566184282302856; avg train loss: 0.4770911079815035\n",
      "batch idx: 116 done; train loss: 0.00027259447961114347; avg train loss: 0.4730157360712309\n",
      "batch idx: 117 done; train loss: 0.0004577780782710761; avg train loss: 0.46901100761366343\n",
      "batch idx: 118 done; train loss: 0.0005922947311773896; avg train loss: 0.4650747159087686\n",
      "batch idx: 119 done; train loss: 0.0001928620331455022; avg train loss: 0.46120070045980505\n",
      "batch idx: 120 done; train loss: 0.00011622230522334576; avg train loss: 0.45739008493786637\n",
      "batch idx: 121 done; train loss: 9.703165414975956e-05; avg train loss: 0.4536417812224261\n",
      "batch idx: 122 done; train loss: 0.0009417866240255535; avg train loss: 0.44996129346146346\n",
      "batch idx: 123 done; train loss: 0.07482561469078064; avg train loss: 0.44693600572944187\n",
      "batch idx: 124 done; train loss: 0.0010642108973115683; avg train loss: 0.4433690313707848\n",
      "batch idx: 125 done; train loss: 0.16378416121006012; avg train loss: 0.44115010382982667\n",
      "batch idx: 126 done; train loss: 0.00011634149996098131; avg train loss: 0.4376773970398277\n",
      "batch idx: 127 done; train loss: 0.07934550940990448; avg train loss: 0.43487792916771895\n",
      "batch idx: 128 done; train loss: 0.0021967582870274782; avg train loss: 0.43152381156399267\n",
      "batch idx: 129 done; train loss: 0.00020430385484360158; avg train loss: 0.4282059691969992\n",
      "batch idx: 130 done; train loss: 2.5629668016335927e-05; avg train loss: 0.4249374169868543\n",
      "batch idx: 131 done; train loss: 0.0008225633064284921; avg train loss: 0.4217244256710935\n",
      "batch idx: 132 done; train loss: 0.00017855956684798002; avg train loss: 0.418554907880836\n",
      "batch idx: 133 done; train loss: 0.00010895135346800089; avg train loss: 0.41543217686197503\n",
      "batch idx: 134 done; train loss: 0.00028224775451235473; avg train loss: 0.4123569922019198\n",
      "batch idx: 135 done; train loss: 5.711141109466553; avg train loss: 0.45131864012298323\n",
      "batch idx: 136 done; train loss: 0.2832111716270447; avg train loss: 0.45009157830914426\n",
      "batch idx: 137 done; train loss: 0.0007378716254606843; avg train loss: 0.4468353920288277\n",
      "batch idx: 138 done; train loss: 1.0967194612021558e-05; avg train loss: 0.44362082782138734\n",
      "batch idx: 139 done; train loss: 0.26775917410850525; avg train loss: 0.4423646731520096\n",
      "batch idx: 140 done; train loss: 0.0004189328756183386; avg train loss: 0.4392303061996948\n",
      "batch idx: 141 done; train loss: 0.00026425207033753395; avg train loss: 0.4361389959593472\n",
      "batch idx: 142 done; train loss: 0.0005765683017671108; avg train loss: 0.4330931048568466\n",
      "batch idx: 143 done; train loss: 0.26157182455062866; avg train loss: 0.4319019848547201\n",
      "batch idx: 144 done; train loss: 0.00651154201477766; avg train loss: 0.4289682576627205\n",
      "batch idx: 145 done; train loss: 1.07287787614041e-05; avg train loss: 0.426030192396392\n",
      "batch idx: 146 done; train loss: 0.26840609312057495; avg train loss: 0.4249579196122028\n",
      "batch idx: 147 done; train loss: 0.00018916724366135895; avg train loss: 0.4220878604745775\n",
      "batch idx: 148 done; train loss: 0.000985136954113841; avg train loss: 0.41926166769927237\n",
      "batch idx: 149 done; train loss: 0.002508709440007806; avg train loss: 0.4164833146442106\n",
      "batch idx: 150 done; train loss: 0.032098524272441864; avg train loss: 0.413937720005987\n",
      "batch idx: 151 done; train loss: 0.00018142008048016578; avg train loss: 0.41121563908542447\n",
      "batch idx: 152 done; train loss: 0.14236900210380554; avg train loss: 0.4094584715234531\n",
      "batch idx: 153 done; train loss: 0.00040904260822571814; avg train loss: 0.4068023064006269\n",
      "batch idx: 154 done; train loss: 0.2965097725391388; avg train loss: 0.4060907416660367\n",
      "batch idx: 155 done; train loss: 0.0010870745172724128; avg train loss: 0.40349456431251896\n",
      "batch idx: 156 done; train loss: 0.0002557904226705432; avg train loss: 0.4009261644788257\n",
      "batch idx: 157 done; train loss: 0.00022015532886143774; avg train loss: 0.3983900504968639\n",
      "batch idx: 158 done; train loss: 0.11210647225379944; avg train loss: 0.3965895248475364\n",
      "batch idx: 159 done; train loss: 0.00013553177996072918; avg train loss: 0.39411168739086405\n",
      "batch idx: 160 done; train loss: 0.0507647730410099; avg train loss: 0.39197909786074075\n",
      "batch idx: 161 done; train loss: 0.0006768796010874212; avg train loss: 0.3895636520690145\n",
      "batch idx: 162 done; train loss: 9.512448741588742e-05; avg train loss: 0.38717427459918874\n",
      "Current learning rate: 5e-05\n",
      "\n",
      "Epoch 5/10\n",
      "Train - Loss: 0.4336, Acc: 0.9264, F1: 0.9286, Recall: 0.8667, AUC: 0.9723\n",
      "Val   - Loss: 1.9645, Acc: 0.6182, F1: 0.0870, Recall: 0.0588, AUC: 0.5557\n",
      "batch idx: 0 done; train loss: 0.02253204584121704; avg train loss: 0.02253204584121704\n",
      "batch idx: 1 done; train loss: 9.035655966727063e-05; avg train loss: 0.011311201200442156\n",
      "batch idx: 2 done; train loss: 0.00031251792097464204; avg train loss: 0.007644973440619651\n",
      "batch idx: 3 done; train loss: 0.0024183334317058325; avg train loss: 0.0063383134383911965\n",
      "batch idx: 4 done; train loss: 0.08569322526454926; avg train loss: 0.022209295803622808\n",
      "batch idx: 5 done; train loss: 8.25507640838623; avg train loss: 1.3943538145673908\n",
      "batch idx: 6 done; train loss: 4.792098479811102e-05; avg train loss: 1.195167258341306\n",
      "batch idx: 7 done; train loss: 3.6238969187252223e-05; avg train loss: 1.0457758809197912\n",
      "batch idx: 8 done; train loss: 7.393854141235352; avg train loss: 1.7511179098437424\n",
      "batch idx: 9 done; train loss: 6.949660019017756e-05; avg train loss: 1.5760130685193872\n",
      "batch idx: 10 done; train loss: 0.00042381841922178864; avg train loss: 1.432777682146645\n",
      "batch idx: 11 done; train loss: 0.00014029949670657516; avg train loss: 1.3133912335924833\n",
      "batch idx: 12 done; train loss: 0.008143909275531769; avg train loss: 1.2129875932604102\n",
      "batch idx: 13 done; train loss: 3.635817120084539e-05; avg train loss: 1.1263482193254666\n",
      "batch idx: 14 done; train loss: 0.000460876093711704; avg train loss: 1.0512890631100162\n",
      "batch idx: 15 done; train loss: 9.179073458653875e-06; avg train loss: 0.9855840703577314\n",
      "batch idx: 16 done; train loss: 0.00022587609419133514; avg train loss: 0.9276218236363467\n",
      "batch idx: 17 done; train loss: 0.0007290565990842879; avg train loss: 0.8761277810231655\n",
      "batch idx: 18 done; train loss: 0.0007197650265879929; avg train loss: 0.8300536749180825\n",
      "batch idx: 19 done; train loss: 0.0054444605484604836; avg train loss: 0.7888232141996013\n",
      "batch idx: 20 done; train loss: 0.004354756325483322; avg train loss: 0.7514675733484528\n",
      "batch idx: 21 done; train loss: 4.729684352874756; avg train loss: 0.9322956087814667\n",
      "batch idx: 22 done; train loss: 0.0015905360924080014; avg train loss: 0.891830170838464\n",
      "batch idx: 23 done; train loss: 0.0016085079405456781; avg train loss: 0.8547376015510508\n",
      "batch idx: 24 done; train loss: 0.0014229421503841877; avg train loss: 0.8206050151750242\n",
      "batch idx: 25 done; train loss: 3.7372660636901855; avg train loss: 0.9327842862717611\n",
      "batch idx: 26 done; train loss: 0.0018816161900758743; avg train loss: 0.8983064096020691\n",
      "batch idx: 27 done; train loss: 0.0005566716426983476; avg train loss: 0.866243918960663\n",
      "batch idx: 28 done; train loss: 0.33462217450141907; avg train loss: 0.847912134668965\n",
      "batch idx: 29 done; train loss: 0.002256349427625537; avg train loss: 0.8197236084942536\n",
      "batch idx: 30 done; train loss: 0.006523503921926022; avg train loss: 0.7934913470564366\n",
      "batch idx: 31 done; train loss: 0.07262727618217468; avg train loss: 0.7709643448416159\n",
      "batch idx: 32 done; train loss: 0.0003023882454726845; avg train loss: 0.7476109522174904\n",
      "batch idx: 33 done; train loss: 0.005720316432416439; avg train loss: 0.7257906394002823\n",
      "batch idx: 34 done; train loss: 0.004782191012054682; avg train loss: 0.7051903980177615\n",
      "batch idx: 35 done; train loss: 3.313963316031732e-05; avg train loss: 0.685602696395967\n",
      "batch idx: 36 done; train loss: 0.0003409996279515326; avg train loss: 0.6670821099968315\n",
      "batch idx: 37 done; train loss: 8.216337203979492; avg train loss: 0.8657467177332173\n",
      "batch idx: 38 done; train loss: 3.9782164096832275; avg train loss: 0.9455536329114227\n",
      "batch idx: 39 done; train loss: 0.00027652730932459235; avg train loss: 0.9219217052713702\n",
      "batch idx: 40 done; train loss: 2.766284704208374; avg train loss: 0.9669061686600776\n",
      "batch idx: 41 done; train loss: 2.985823154449463; avg train loss: 1.014975620702682\n",
      "batch idx: 42 done; train loss: 0.0006624649395234883; avg train loss: 0.9913869426616784\n",
      "batch idx: 43 done; train loss: 0.27801308035850525; avg train loss: 0.9751739003366062\n",
      "batch idx: 44 done; train loss: 0.00014911970356479287; avg train loss: 0.9535066829892054\n",
      "batch idx: 45 done; train loss: 0.0004020121123176068; avg train loss: 0.9327870162310121\n",
      "batch idx: 46 done; train loss: 0.00014399446081370115; avg train loss: 0.91294354768271\n",
      "batch idx: 47 done; train loss: 0.12113498151302338; avg train loss: 0.8964475358875083\n",
      "batch idx: 48 done; train loss: 0.0001658063702052459; avg train loss: 0.8781560720198082\n",
      "batch idx: 49 done; train loss: 0.0004619484825525433; avg train loss: 0.8606021895490631\n",
      "batch idx: 50 done; train loss: 6.153040409088135; avg train loss: 0.9643754879713978\n",
      "batch idx: 51 done; train loss: 0.00023278864682652056; avg train loss: 0.945834282215156\n",
      "batch idx: 52 done; train loss: 0.00046754872892051935; avg train loss: 0.9279971740361704\n",
      "batch idx: 53 done; train loss: 0.0008740180637687445; avg train loss: 0.9108282267033482\n",
      "batch idx: 54 done; train loss: 0.002257063053548336; avg train loss: 0.8943087510006246\n",
      "batch idx: 55 done; train loss: 0.00016139635408762842; avg train loss: 0.878341833953365\n",
      "batch idx: 56 done; train loss: 0.00012516192509792745; avg train loss: 0.8629345239177814\n",
      "batch idx: 57 done; train loss: 8.189342770492658e-05; avg train loss: 0.8480577544265732\n",
      "batch idx: 58 done; train loss: 0.00011526874004630372; avg train loss: 0.8336858478895134\n",
      "batch idx: 59 done; train loss: 0.00011359999916749075; avg train loss: 0.8197929770913409\n",
      "batch idx: 60 done; train loss: 0.00023338454775512218; avg train loss: 0.8063575739348887\n",
      "batch idx: 61 done; train loss: 0.0005423743859864771; avg train loss: 0.7933605545873258\n",
      "batch idx: 62 done; train loss: 0.000169382052263245; avg train loss: 0.7807702185153407\n",
      "batch idx: 63 done; train loss: 0.0007483542431145906; avg train loss: 0.7685823768860871\n",
      "batch idx: 64 done; train loss: 8.201262971851975e-05; avg train loss: 0.7567592943590661\n",
      "batch idx: 65 done; train loss: 2.6086058616638184; avg train loss: 0.7848175756818653\n",
      "batch idx: 66 done; train loss: 0.0031432530377060175; avg train loss: 0.7731507947468779\n",
      "batch idx: 67 done; train loss: 0.0008380476501770318; avg train loss: 0.761793254348397\n",
      "batch idx: 68 done; train loss: 0.0009553635609336197; avg train loss: 0.750766618250028\n",
      "batch idx: 69 done; train loss: 0.0005675135762430727; avg train loss: 0.7400494881832596\n",
      "batch idx: 70 done; train loss: 0.003277052426710725; avg train loss: 0.7296724116233082\n",
      "batch idx: 71 done; train loss: 7.35589599609375; avg train loss: 0.8217032947409533\n",
      "batch idx: 72 done; train loss: 7.70062324590981e-05; avg train loss: 0.8104481401038506\n",
      "batch idx: 73 done; train loss: 0.00021443451987579465; avg train loss: 0.7994990359743374\n",
      "batch idx: 74 done; train loss: 0.00043561504571698606; avg train loss: 0.7888448570286225\n",
      "batch idx: 75 done; train loss: 0.0005857420619577169; avg train loss: 0.7784730265685348\n",
      "batch idx: 76 done; train loss: 3.71926071238704e-05; avg train loss: 0.7683634702833216\n",
      "batch idx: 77 done; train loss: 2.4252448081970215; avg train loss: 0.7896055387181127\n",
      "batch idx: 78 done; train loss: 0.0005379660287871957; avg train loss: 0.7796173415954629\n",
      "batch idx: 79 done; train loss: 2.3609073162078857; avg train loss: 0.7993834662781183\n",
      "batch idx: 80 done; train loss: 0.00017927470616996288; avg train loss: 0.7895167478636498\n",
      "batch idx: 81 done; train loss: 0.0004886387032456696; avg train loss: 0.7798944538494985\n",
      "batch idx: 82 done; train loss: 0.0003779412363655865; avg train loss: 0.7705026886372921\n",
      "batch idx: 83 done; train loss: 0.015489984303712845; avg train loss: 0.7615144421571304\n",
      "batch idx: 84 done; train loss: 0.0004196478403173387; avg train loss: 0.7525603857534032\n",
      "batch idx: 85 done; train loss: 0.00021646064124070108; avg train loss: 0.7438122005776804\n",
      "batch idx: 86 done; train loss: 0.009757679887115955; avg train loss: 0.7353747922938808\n",
      "batch idx: 87 done; train loss: 0.0010688550537452102; avg train loss: 0.7270304066434247\n",
      "batch idx: 88 done; train loss: 0.00020311199477873743; avg train loss: 0.7188638078271478\n",
      "batch idx: 89 done; train loss: 0.014273923821747303; avg train loss: 0.7110350313381989\n",
      "batch idx: 90 done; train loss: 2.1457441107486375e-05; avg train loss: 0.7032216953613077\n",
      "batch idx: 91 done; train loss: 0.00013064485392533243; avg train loss: 0.6955794013340536\n",
      "batch idx: 92 done; train loss: 0.008020220324397087; avg train loss: 0.6881862918608315\n",
      "batch idx: 93 done; train loss: 2.6940935640595853e-05; avg train loss: 0.6808654477020528\n",
      "batch idx: 94 done; train loss: 0.0004592079494614154; avg train loss: 0.6737032767572887\n",
      "batch idx: 95 done; train loss: 0.0001722425949992612; avg train loss: 0.6666873284847649\n",
      "batch idx: 96 done; train loss: 0.0003573255962692201; avg train loss: 0.6598179470116876\n",
      "batch idx: 97 done; train loss: 1.1901227235794067; avg train loss: 0.6652292202419705\n",
      "batch idx: 98 done; train loss: 0.0011157722910866141; avg train loss: 0.658521003596002\n",
      "batch idx: 99 done; train loss: 0.00010477947944309562; avg train loss: 0.6519368413548363\n",
      "batch idx: 100 done; train loss: 0.0001560327800689265; avg train loss: 0.645483566022413\n",
      "batch idx: 101 done; train loss: 0.002908882452175021; avg train loss: 0.6391838142227048\n",
      "batch idx: 102 done; train loss: 0.0003846143954433501; avg train loss: 0.6329818802437993\n",
      "batch idx: 103 done; train loss: 0.002067691646516323; avg train loss: 0.626915397661133\n",
      "batch idx: 104 done; train loss: 0.00048673225683160126; avg train loss: 0.6209494103715683\n",
      "batch idx: 105 done; train loss: 6.195170879364014; avg train loss: 0.673536405362063\n",
      "batch idx: 106 done; train loss: 0.00015531764074694365; avg train loss: 0.6672431241684059\n",
      "batch idx: 107 done; train loss: 0.00017975145601667464; avg train loss: 0.661066611458106\n",
      "batch idx: 108 done; train loss: 0.0010381315369158983; avg train loss: 0.6550113043028657\n",
      "batch idx: 109 done; train loss: 0.17268182337284088; avg train loss: 0.6506264908398655\n",
      "batch idx: 110 done; train loss: 8.250935554504395; avg train loss: 0.719097743665672\n",
      "batch idx: 111 done; train loss: 3.755022044060752e-05; avg train loss: 0.7126775633670539\n",
      "batch idx: 112 done; train loss: 0.00012408917245920748; avg train loss: 0.7063717804095796\n",
      "batch idx: 113 done; train loss: 0.0009832315845414996; avg train loss: 0.7001841615602372\n",
      "batch idx: 114 done; train loss: 2.471665382385254; avg train loss: 0.7155883460891503\n",
      "batch idx: 115 done; train loss: 0.00032634177478030324; avg train loss: 0.7094222943278197\n",
      "batch idx: 116 done; train loss: 0.012120053172111511; avg train loss: 0.7034624461128136\n",
      "batch idx: 117 done; train loss: 1.2040065485052764e-05; avg train loss: 0.6975010019937684\n",
      "batch idx: 118 done; train loss: 0.00016127715934999287; avg train loss: 0.6916410043060842\n",
      "batch idx: 119 done; train loss: 4.362964682513848e-05; avg train loss: 0.6858776928505904\n",
      "batch idx: 120 done; train loss: 0.017529206350445747; avg train loss: 0.6803541516398454\n",
      "batch idx: 121 done; train loss: 2.8013790142722428e-05; avg train loss: 0.674777707886979\n",
      "batch idx: 122 done; train loss: 0.0007839705212973058; avg train loss: 0.6692980840059571\n",
      "batch idx: 123 done; train loss: 0.0023478104267269373; avg train loss: 0.663919452767415\n",
      "batch idx: 124 done; train loss: 0.000982159748673439; avg train loss: 0.658615954423265\n",
      "batch idx: 125 done; train loss: 0.0017597679980099201; avg train loss: 0.6534028100865567\n",
      "batch idx: 126 done; train loss: 0.0004468158003874123; avg train loss: 0.6482614243047758\n",
      "batch idx: 127 done; train loss: 0.0002919009421020746; avg train loss: 0.6431991624035049\n",
      "batch idx: 128 done; train loss: 0.00040642108069732785; avg train loss: 0.6382162729358862\n",
      "batch idx: 129 done; train loss: 0.00018559163436293602; avg train loss: 0.6333083446181823\n",
      "batch idx: 130 done; train loss: 0.0009411911014467478; avg train loss: 0.6284811144386652\n",
      "batch idx: 131 done; train loss: 2.5748875486897305e-05; avg train loss: 0.6237200889419744\n",
      "batch idx: 132 done; train loss: 0.001788327470421791; avg train loss: 0.6190439102842936\n",
      "batch idx: 133 done; train loss: 0.0001752223033690825; avg train loss: 0.6144254872396598\n",
      "batch idx: 134 done; train loss: 6.285736083984375; avg train loss: 0.6564351953636948\n",
      "batch idx: 135 done; train loss: 0.0004462200158741325; avg train loss: 0.651611747015549\n",
      "batch idx: 136 done; train loss: 0.00018952481332235038; avg train loss: 0.6468568402841459\n",
      "batch idx: 137 done; train loss: 1.1911789178848267; avg train loss: 0.6508012031653102\n",
      "batch idx: 138 done; train loss: 6.250523567199707; avg train loss: 0.6910869755684355\n",
      "batch idx: 139 done; train loss: 0.00029762129997834563; avg train loss: 0.6861527658950893\n",
      "batch idx: 140 done; train loss: 0.0007346553611569107; avg train loss: 0.6812916445437848\n",
      "batch idx: 141 done; train loss: 0.0005965837044641376; avg train loss: 0.6764980173547755\n",
      "batch idx: 142 done; train loss: 3.253883123397827; avg train loss: 0.6945216894250066\n",
      "batch idx: 143 done; train loss: 0.019962593913078308; avg train loss: 0.6898372512617293\n",
      "batch idx: 144 done; train loss: 0.0003357561945449561; avg train loss: 0.685082068537128\n",
      "batch idx: 145 done; train loss: 0.0003623305819928646; avg train loss: 0.6803922073182573\n",
      "batch idx: 146 done; train loss: 0.0011622109450399876; avg train loss: 0.6757715950980313\n",
      "batch idx: 147 done; train loss: 0.0015468548517674208; avg train loss: 0.6712160225287999\n",
      "batch idx: 148 done; train loss: 0.0003358753747306764; avg train loss: 0.6667134712056182\n",
      "batch idx: 149 done; train loss: 0.00043871314846910536; avg train loss: 0.6622716394852372\n",
      "batch idx: 150 done; train loss: 0.08036576956510544; avg train loss: 0.6584179582274879\n",
      "batch idx: 151 done; train loss: 0.001524953986518085; avg train loss: 0.6540962937259026\n",
      "batch idx: 152 done; train loss: 4.2199197196168825e-05; avg train loss: 0.6498214303629699\n",
      "batch idx: 153 done; train loss: 0.11934880167245865; avg train loss: 0.6463768029039406\n",
      "batch idx: 154 done; train loss: 7.617183291586116e-05; avg train loss: 0.6422071214131598\n",
      "batch idx: 155 done; train loss: 0.0001070442158379592; avg train loss: 0.6380910952772795\n",
      "batch idx: 156 done; train loss: 0.0037437842693179846; avg train loss: 0.6340506665447447\n",
      "batch idx: 157 done; train loss: 0.0011829291470348835; avg train loss: 0.6300451745358985\n",
      "batch idx: 158 done; train loss: 0.016497978940606117; avg train loss: 0.6261863871422174\n",
      "batch idx: 159 done; train loss: 8.463501580990851e-05; avg train loss: 0.6222732511914273\n",
      "batch idx: 160 done; train loss: 0.0006824786541983485; avg train loss: 0.6184124389396433\n",
      "batch idx: 161 done; train loss: 4.410734163684538e-06; avg train loss: 0.6145951054322021\n",
      "batch idx: 162 done; train loss: 0.00044645831803791225; avg train loss: 0.6108273223210722\n",
      "Current learning rate: 5e-05\n",
      "\n",
      "Epoch 6/10\n",
      "Train - Loss: 0.4385, Acc: 0.8957, F1: 0.8618, Recall: 0.7910, AUC: 0.9709\n",
      "Val   - Loss: 1.9459, Acc: 0.6364, F1: 0.1667, Recall: 0.1176, AUC: 0.5418\n",
      "batch idx: 0 done; train loss: 0.0008867622236721218; avg train loss: 0.0008867622236721218\n",
      "batch idx: 1 done; train loss: 0.04681064933538437; avg train loss: 0.023848705779528245\n",
      "batch idx: 2 done; train loss: 0.00012337400403339416; avg train loss: 0.015940261854363296\n",
      "batch idx: 3 done; train loss: 0.01542002521455288; avg train loss: 0.01581020269441069\n",
      "batch idx: 4 done; train loss: 0.005517255049198866; avg train loss: 0.013751613165368326\n",
      "batch idx: 5 done; train loss: 7.748573807475623e-06; avg train loss: 0.011460969066774851\n",
      "batch idx: 6 done; train loss: 2.13382354559144e-05; avg train loss: 0.009826736090872146\n",
      "batch idx: 7 done; train loss: 0.00045718232286162674; avg train loss: 0.00865554186987083\n",
      "batch idx: 8 done; train loss: 0.0016046992968767881; avg train loss: 0.007872114917315938\n",
      "batch idx: 9 done; train loss: 0.0014466306893154979; avg train loss: 0.007229566494515893\n",
      "batch idx: 10 done; train loss: 0.00016342257731594145; avg train loss: 0.006587189774770443\n",
      "batch idx: 11 done; train loss: 2.288792165927589e-05; avg train loss: 0.0060401646203445125\n",
      "batch idx: 12 done; train loss: 0.0002932118659373373; avg train loss: 0.00559809133154396\n",
      "batch idx: 13 done; train loss: 0.0003911683743353933; avg train loss: 0.00522616826317192\n",
      "batch idx: 14 done; train loss: 0.0011692361440509558; avg train loss: 0.004955706121897189\n",
      "batch idx: 15 done; train loss: 6.294129848480225; avg train loss: 0.39802909001929265\n",
      "batch idx: 16 done; train loss: 0.00020001317898277193; avg train loss: 0.3746273796169215\n",
      "batch idx: 17 done; train loss: 0.1176300123333931; avg train loss: 0.36034974810116993\n",
      "batch idx: 18 done; train loss: 8.523101132595912e-05; avg train loss: 0.34138845772802023\n",
      "batch idx: 19 done; train loss: 0.000125281119835563; avg train loss: 0.32432529889761097\n",
      "batch idx: 20 done; train loss: 0.00015925093612167984; avg train loss: 0.3088888204232544\n",
      "batch idx: 21 done; train loss: 0.0019677577074617147; avg train loss: 0.29493786302708197\n",
      "batch idx: 22 done; train loss: 0.00041214076918549836; avg train loss: 0.282132396841956\n",
      "batch idx: 23 done; train loss: 0.0015261442167684436; avg train loss: 0.2704404696492399\n",
      "batch idx: 24 done; train loss: 7.86750388215296e-05; avg train loss: 0.25962599786482315\n",
      "batch idx: 25 done; train loss: 2.741307497024536; avg train loss: 0.3550752862940429\n",
      "batch idx: 26 done; train loss: 0.0003073934931308031; avg train loss: 0.34193573470882394\n",
      "batch idx: 27 done; train loss: 0.0014246086357161403; avg train loss: 0.32977462306335575\n",
      "batch idx: 28 done; train loss: 0.004886945243924856; avg train loss: 0.31857159969027193\n",
      "batch idx: 29 done; train loss: 0.004789665341377258; avg train loss: 0.30811220187864213\n",
      "batch idx: 30 done; train loss: 1.5513246059417725; avg train loss: 0.34821582781616245\n",
      "batch idx: 31 done; train loss: 1.8715683836489916e-05; avg train loss: 0.3373346680620273\n",
      "batch idx: 32 done; train loss: 7.393707752227783; avg train loss: 0.5511641554609896\n",
      "batch idx: 33 done; train loss: 0.0001401803019689396; avg train loss: 0.5349575679563126\n",
      "batch idx: 34 done; train loss: 0.0011645924532786012; avg train loss: 0.5197063400847972\n",
      "batch idx: 35 done; train loss: 0.0004371640970930457; avg train loss: 0.505282196307361\n",
      "batch idx: 36 done; train loss: 2.21758770942688; avg train loss: 0.5515607236889697\n",
      "batch idx: 37 done; train loss: 0.000546425289940089; avg train loss: 0.537060347415311\n",
      "batch idx: 38 done; train loss: 0.04275425523519516; avg train loss: 0.5243858322312054\n",
      "batch idx: 39 done; train loss: 0.0006518622976727784; avg train loss: 0.5112924829828671\n",
      "batch idx: 40 done; train loss: 6.318072337307967e-06; avg train loss: 0.49882208871675665\n",
      "batch idx: 41 done; train loss: 0.00043156370520591736; avg train loss: 0.48695564764505306\n",
      "batch idx: 42 done; train loss: 0.00024291902082040906; avg train loss: 0.4756367469793732\n",
      "batch idx: 43 done; train loss: 3.302042750874534e-05; avg train loss: 0.46482757137592173\n",
      "batch idx: 44 done; train loss: 8.308542601298541e-05; avg train loss: 0.45449991613259044\n",
      "batch idx: 45 done; train loss: 3.725168466567993; avg train loss: 0.5256014063594471\n",
      "batch idx: 46 done; train loss: 0.00041321321623399854; avg train loss: 0.5144271894840595\n",
      "batch idx: 47 done; train loss: 0.0005996812833473086; avg train loss: 0.503722449729878\n",
      "batch idx: 48 done; train loss: 0.00041571559268049896; avg train loss: 0.49345088372707807\n",
      "batch idx: 49 done; train loss: 0.00030012393835932016; avg train loss: 0.4835878685313037\n",
      "batch idx: 50 done; train loss: 0.0016608743462711573; avg train loss: 0.4741383196257148\n",
      "batch idx: 51 done; train loss: 0.017512807622551918; avg train loss: 0.46535705977950015\n",
      "batch idx: 52 done; train loss: 0.00018404220463708043; avg train loss: 0.45658021039129515\n",
      "batch idx: 53 done; train loss: 0.0008021951070986688; avg train loss: 0.4481398767749212\n",
      "batch idx: 54 done; train loss: 0.000285227142740041; avg train loss: 0.43999706496342694\n",
      "batch idx: 55 done; train loss: 0.0005040090763941407; avg train loss: 0.43214897467972996\n",
      "batch idx: 56 done; train loss: 0.042714960873126984; avg train loss: 0.42531679899891234\n",
      "batch idx: 57 done; train loss: 2.2172682292875834e-05; avg train loss: 0.41798413302793613\n",
      "batch idx: 58 done; train loss: 0.00010418349120300263; avg train loss: 0.410901422018839\n",
      "batch idx: 59 done; train loss: 0.001091004116460681; avg train loss: 0.40407124838713265\n",
      "batch idx: 60 done; train loss: 8.34430247778073e-05; avg train loss: 0.3974484974795531\n",
      "batch idx: 61 done; train loss: 0.006570165976881981; avg train loss: 0.3911440082617681\n",
      "batch idx: 62 done; train loss: 0.0034589481074362993; avg train loss: 0.38499027714820727\n",
      "batch idx: 63 done; train loss: 0.0018810213077813387; avg train loss: 0.3790041950257006\n",
      "batch idx: 64 done; train loss: 0.01645541377365589; avg train loss: 0.37342652146797684\n",
      "batch idx: 65 done; train loss: 0.0007541911327280104; avg train loss: 0.36777997100835186\n",
      "batch idx: 66 done; train loss: 0.0007960011716932058; avg train loss: 0.3623025983242226\n",
      "batch idx: 67 done; train loss: 0.0007937379996292293; avg train loss: 0.3569862915547433\n",
      "batch idx: 68 done; train loss: 0.0024441389832645655; avg train loss: 0.35184799948849\n",
      "batch idx: 69 done; train loss: 0.08311066031455994; avg train loss: 0.3480088946431481\n",
      "batch idx: 70 done; train loss: 0.03069189377129078; avg train loss: 0.34353964110974167\n",
      "batch idx: 71 done; train loss: 0.00970278400927782; avg train loss: 0.33890301809445744\n",
      "batch idx: 72 done; train loss: 0.0004085659747943282; avg train loss: 0.3342661077914484\n",
      "batch idx: 73 done; train loss: 0.0009161804337054491; avg train loss: 0.3297613790433708\n",
      "batch idx: 74 done; train loss: 0.0003036991402041167; avg train loss: 0.3253686099779952\n",
      "batch idx: 75 done; train loss: 0.0009266611887142062; avg train loss: 0.32109963696760996\n",
      "batch idx: 76 done; train loss: 0.0006104036583565176; avg train loss: 0.31693743913242484\n",
      "batch idx: 77 done; train loss: 0.0007101159426383674; avg train loss: 0.3128832426812737\n",
      "batch idx: 78 done; train loss: 0.0006347072194330394; avg train loss: 0.30893072957416184\n",
      "batch idx: 79 done; train loss: 0.00017081231635529548; avg train loss: 0.3050712306084392\n",
      "batch idx: 80 done; train loss: 0.0004365683125797659; avg train loss: 0.30131030885170024\n",
      "batch idx: 81 done; train loss: 0.00023731753753963858; avg train loss: 0.2976386870064056\n",
      "batch idx: 82 done; train loss: 0.008661204017698765; avg train loss: 0.29415703058485493\n",
      "batch idx: 83 done; train loss: 0.00011824862303910777; avg train loss: 0.29065656889483327\n",
      "batch idx: 84 done; train loss: 0.0006712805479764938; avg train loss: 0.2872449772672232\n",
      "batch idx: 85 done; train loss: 0.0040901582688093185; avg train loss: 0.28395247937189283\n",
      "batch idx: 86 done; train loss: 3.755022044060752e-05; avg train loss: 0.2806890893816462\n",
      "batch idx: 87 done; train loss: 0.00027164106722921133; avg train loss: 0.2775025274689824\n",
      "batch idx: 88 done; train loss: 0.00031263710116036236; avg train loss: 0.27438803431878217\n",
      "batch idx: 89 done; train loss: 2.2053474822314456e-05; avg train loss: 0.2713395234205159\n",
      "batch idx: 90 done; train loss: 0.003015021560713649; avg train loss: 0.26839090252095765\n",
      "batch idx: 91 done; train loss: 6.961580220377073e-05; avg train loss: 0.2654743667957538\n",
      "batch idx: 92 done; train loss: 0.007766526658087969; avg train loss: 0.2627033147512628\n",
      "batch idx: 93 done; train loss: 0.000500196241773665; avg train loss: 0.25991391987350226\n",
      "batch idx: 94 done; train loss: 1.490105023549404e-05; avg train loss: 0.2571781407279942\n",
      "batch idx: 95 done; train loss: 0.002885466208681464; avg train loss: 0.25452925870175136\n",
      "batch idx: 96 done; train loss: 5.960446742392378e-06; avg train loss: 0.2519053071733492\n",
      "batch idx: 97 done; train loss: 7.4619011878967285; avg train loss: 0.32547669371134286\n",
      "batch idx: 98 done; train loss: 0.0005265279905870557; avg train loss: 0.32219436880507263\n",
      "batch idx: 99 done; train loss: 3.39389705657959; avg train loss: 0.35291139568281776\n",
      "batch idx: 100 done; train loss: 0.00012766500003635883; avg train loss: 0.3494184874582358\n",
      "batch idx: 101 done; train loss: 0.0002008474839385599; avg train loss: 0.3459947851055466\n",
      "batch idx: 102 done; train loss: 0.00012575789878610522; avg train loss: 0.3426368333850926\n",
      "batch idx: 103 done; train loss: 0.0009856133256107569; avg train loss: 0.3393517254999053\n",
      "batch idx: 104 done; train loss: 7.519914150238037; avg train loss: 0.4077380343069351\n",
      "batch idx: 105 done; train loss: 2.9682672902708873e-05; avg train loss: 0.4038917291028405\n",
      "batch idx: 106 done; train loss: 0.0033080638386309147; avg train loss: 0.40014795653027774\n",
      "batch idx: 107 done; train loss: 0.00019643761334009469; avg train loss: 0.3964447017254913\n",
      "batch idx: 108 done; train loss: 0.00011705666838679463; avg train loss: 0.3928086682846004\n",
      "batch idx: 109 done; train loss: 0.0002653246629051864; avg train loss: 0.3892400924334941\n",
      "batch idx: 110 done; train loss: 0.005004498641937971; avg train loss: 0.38577851050744405\n",
      "batch idx: 111 done; train loss: 0.012065052054822445; avg train loss: 0.38244178319983135\n",
      "batch idx: 112 done; train loss: 2.7287793159484863; avg train loss: 0.40320583216220884\n",
      "batch idx: 113 done; train loss: 0.00045753977610729635; avg train loss: 0.399672952404436\n",
      "batch idx: 114 done; train loss: 0.00032300499151460826; avg train loss: 0.3962003441660628\n",
      "batch idx: 115 done; train loss: 0.0025113255251199007; avg train loss: 0.39280647331570984\n",
      "batch idx: 116 done; train loss: 0.0004839917819481343; avg train loss: 0.3894532897128572\n",
      "batch idx: 117 done; train loss: 0.0003756771038752049; avg train loss: 0.38615602180939124\n",
      "batch idx: 118 done; train loss: 7.068861305015162e-05; avg train loss: 0.38291160724471607\n",
      "batch idx: 119 done; train loss: 0.0003819928097072989; avg train loss: 0.3797238604577577\n",
      "batch idx: 120 done; train loss: 0.00010191874753218144; avg train loss: 0.37658648903866493\n",
      "batch idx: 121 done; train loss: 0.00345015712082386; avg train loss: 0.37352799451474816\n",
      "batch idx: 122 done; train loss: 0.0001454247540095821; avg train loss: 0.37049236386628687\n",
      "batch idx: 123 done; train loss: 2.0265373677830212e-05; avg train loss: 0.36750468565263683\n",
      "batch idx: 124 done; train loss: 2.1118130683898926; avg train loss: 0.38145915271453484\n",
      "batch idx: 125 done; train loss: 0.0005943200667388737; avg train loss: 0.3784364159474889\n",
      "batch idx: 126 done; train loss: 0.00022027450904715806; avg train loss: 0.3754583360936429\n",
      "batch idx: 127 done; train loss: 0.00048065552255138755; avg train loss: 0.3725288229641812\n",
      "batch idx: 128 done; train loss: 0.00032145579461939633; avg train loss: 0.3696434945365102\n",
      "batch idx: 129 done; train loss: 0.001053493469953537; avg train loss: 0.3668081868359982\n",
      "batch idx: 130 done; train loss: 1.6399632692337036; avg train loss: 0.37652692792300363\n",
      "batch idx: 131 done; train loss: 0.00010024998482549563; avg train loss: 0.3736752106658962\n",
      "batch idx: 132 done; train loss: 0.000327652640407905; avg train loss: 0.37086808616946393\n",
      "batch idx: 133 done; train loss: 0.00036542891757562757; avg train loss: 0.36810314096609165\n",
      "batch idx: 134 done; train loss: 0.0004745787591673434; avg train loss: 0.36537996643122556\n",
      "batch idx: 135 done; train loss: 0.0005834784242324531; avg train loss: 0.36269763931352705\n",
      "batch idx: 136 done; train loss: 1.680836794548668e-05; avg train loss: 0.36005033397815783\n",
      "batch idx: 137 done; train loss: 9.965400386136025e-05; avg train loss: 0.35744199571747454\n",
      "batch idx: 138 done; train loss: 0.0006890306831337512; avg train loss: 0.35487542762370233\n",
      "batch idx: 139 done; train loss: 0.0003040566807612777; avg train loss: 0.3523427749741099\n",
      "batch idx: 140 done; train loss: 0.00010334911348763853; avg train loss: 0.34984462301765157\n",
      "batch idx: 141 done; train loss: 0.00017808281700126827; avg train loss: 0.34738218259370335\n",
      "batch idx: 142 done; train loss: 0.0006692553870379925; avg train loss: 0.3449576166691812\n",
      "batch idx: 143 done; train loss: 2.8967437174287625e-05; avg train loss: 0.34256227882729223\n",
      "batch idx: 144 done; train loss: 0.0003046525234822184; avg train loss: 0.34020188140450736\n",
      "batch idx: 145 done; train loss: 0.00011729506513802335; avg train loss: 0.33787253492273084\n",
      "batch idx: 146 done; train loss: 0.020739715546369553; avg train loss: 0.3357151688045243\n",
      "batch idx: 147 done; train loss: 7.478621006011963; avg train loss: 0.38397804608295294\n",
      "batch idx: 148 done; train loss: 6.193580150604248; avg train loss: 0.42296866423410256\n",
      "batch idx: 149 done; train loss: 0.0001264730526600033; avg train loss: 0.42014971629289294\n",
      "batch idx: 150 done; train loss: 0.0002919009421020746; avg train loss: 0.417369200959444\n",
      "batch idx: 151 done; train loss: 0.000621959799900651; avg train loss: 0.4146274427939207\n",
      "batch idx: 152 done; train loss: 0.005973344668745995; avg train loss: 0.4119565009761091\n",
      "batch idx: 153 done; train loss: 8.689979586051777e-05; avg train loss: 0.40928202304636724\n",
      "batch idx: 154 done; train loss: 1.5139465176616795e-05; avg train loss: 0.40664159153939183\n",
      "batch idx: 155 done; train loss: 0.0002493547508493066; avg train loss: 0.40403651309843963\n",
      "batch idx: 156 done; train loss: 0.0002474478678777814; avg train loss: 0.4014646082243596\n",
      "batch idx: 157 done; train loss: 0.00030787018476985395; avg train loss: 0.3989256415279065\n",
      "batch idx: 158 done; train loss: 9.881961887003854e-05; avg train loss: 0.39641729673602577\n",
      "batch idx: 159 done; train loss: 0.00034874555421993136; avg train loss: 0.3939418682911395\n",
      "batch idx: 160 done; train loss: 0.010907904244959354; avg train loss: 0.39156277534675327\n",
      "batch idx: 161 done; train loss: 0.001279250718653202; avg train loss: 0.38915361778732055\n",
      "batch idx: 162 done; train loss: 0.0033050933852791786; avg train loss: 0.3867864489259583\n",
      "Current learning rate: 5e-06\n",
      "\n",
      "Epoch 7/10\n",
      "Train - Loss: 0.4743, Acc: 0.8712, F1: 0.8627, Recall: 0.7952, AUC: 0.9721\n",
      "Val   - Loss: 1.9365, Acc: 0.6182, F1: 0.1600, Recall: 0.1176, AUC: 0.5526\n",
      "batch idx: 0 done; train loss: 0.0018198610050603747; avg train loss: 0.0018198610050603747\n",
      "batch idx: 1 done; train loss: 6.16293036728166e-05; avg train loss: 0.0009407451543665957\n",
      "batch idx: 2 done; train loss: 0.0010297955013811588; avg train loss: 0.0009704286033714501\n",
      "batch idx: 3 done; train loss: 0.015932632610201836; avg train loss: 0.0047109796050790465\n",
      "batch idx: 4 done; train loss: 0.0025693520437926054; avg train loss: 0.004282654092821758\n",
      "batch idx: 5 done; train loss: 7.795983401592821e-05; avg train loss: 0.00358187171635412\n",
      "batch idx: 6 done; train loss: 0.0009860896971076727; avg train loss: 0.0032110457136046273\n",
      "batch idx: 7 done; train loss: 0.00022587609419133514; avg train loss: 0.002837899511177966\n",
      "batch idx: 8 done; train loss: 0.00011622230522334576; avg train loss: 0.0025354909327385635\n",
      "batch idx: 9 done; train loss: 0.00012230125139467418; avg train loss: 0.0022941719646041745\n",
      "batch idx: 10 done; train loss: 0.001824858714826405; avg train loss: 0.0022515071237152865\n",
      "batch idx: 11 done; train loss: 0.08672025799751282; avg train loss: 0.009290569696531747\n",
      "batch idx: 12 done; train loss: 0.0006000387365929782; avg train loss: 0.008622067314997995\n",
      "batch idx: 13 done; train loss: 0.0005899119423702359; avg train loss: 0.00804834193123887\n",
      "batch idx: 14 done; train loss: 8.380061626667157e-05; avg train loss: 0.007517372510240723\n",
      "batch idx: 15 done; train loss: 0.0014835315523669124; avg train loss: 0.0071402574503736105\n",
      "batch idx: 16 done; train loss: 0.00040975757292471826; avg train loss: 0.006744345692876617\n",
      "batch idx: 17 done; train loss: 0.00034314466756768525; avg train loss: 0.006388723413692787\n",
      "batch idx: 18 done; train loss: 0.00026306029758416116; avg train loss: 0.006066320091792333\n",
      "batch idx: 19 done; train loss: 0.00011646069469861686; avg train loss: 0.005768827121937647\n",
      "batch idx: 20 done; train loss: 0.00010346830822527409; avg train loss: 0.005499048130808487\n",
      "batch idx: 21 done; train loss: 0.002610967494547367; avg train loss: 0.005367771738251163\n",
      "batch idx: 22 done; train loss: 0.00011467275908216834; avg train loss: 0.005139376130461207\n",
      "batch idx: 23 done; train loss: 0.0005523824947886169; avg train loss: 0.004948251395641516\n",
      "batch idx: 24 done; train loss: 8.15163803100586; avg train loss: 0.33081584258005026\n",
      "batch idx: 25 done; train loss: 0.0020825620740652084; avg train loss: 0.3181722548682816\n",
      "batch idx: 26 done; train loss: 0.0006948678637854755; avg train loss: 0.3064138331273743\n",
      "batch idx: 27 done; train loss: 1.3240834474563599; avg train loss: 0.34275917649626664\n",
      "batch idx: 28 done; train loss: 0.0004781533498317003; avg train loss: 0.3309563825946654\n",
      "batch idx: 29 done; train loss: 7.429389953613281; avg train loss: 0.5675708349619526\n",
      "batch idx: 30 done; train loss: 8.193604469299316; avg train loss: 0.8135719199405773\n",
      "batch idx: 31 done; train loss: 0.0005109196645207703; avg train loss: 0.7881637636819505\n",
      "batch idx: 32 done; train loss: 0.0029948167502880096; avg train loss: 0.764370765290082\n",
      "batch idx: 33 done; train loss: 0.00042882305569946766; avg train loss: 0.7419018846361295\n",
      "batch idx: 34 done; train loss: 0.0005308172549121082; avg train loss: 0.7207198541395233\n",
      "batch idx: 35 done; train loss: 0.002054724609479308; avg train loss: 0.7007569338747999\n",
      "batch idx: 36 done; train loss: 1.4212291240692139; avg train loss: 0.7202291552314056\n",
      "batch idx: 37 done; train loss: 5.722029527532868e-06; avg train loss: 0.701275906989251\n",
      "batch idx: 38 done; train loss: 0.00280344532802701; avg train loss: 0.6833663566902453\n",
      "batch idx: 39 done; train loss: 0.0001699779968475923; avg train loss: 0.6662864472229103\n",
      "batch idx: 40 done; train loss: 2.3834121227264404; avg train loss: 0.7081675612595818\n",
      "batch idx: 41 done; train loss: 0.035507988184690475; avg train loss: 0.692151857138751\n",
      "batch idx: 42 done; train loss: 0.009952209889888763; avg train loss: 0.6762867490631961\n",
      "batch idx: 43 done; train loss: 0.005739992018789053; avg train loss: 0.6610470500394595\n",
      "batch idx: 44 done; train loss: 7.43351411819458; avg train loss: 0.8115463182206845\n",
      "batch idx: 45 done; train loss: 7.46520471572876; avg train loss: 0.9561910659925992\n",
      "batch idx: 46 done; train loss: 0.00010477947944309562; avg train loss: 0.9358488045774256\n",
      "batch idx: 47 done; train loss: 0.0030928654596209526; avg train loss: 0.916416389179138\n",
      "batch idx: 48 done; train loss: 0.00029202012228779495; avg train loss: 0.8977199734841003\n",
      "batch idx: 49 done; train loss: 0.0002747396647464484; avg train loss: 0.8797710688077132\n",
      "batch idx: 50 done; train loss: 0.06944401562213898; avg train loss: 0.8638823030589764\n",
      "batch idx: 51 done; train loss: 0.0004808938247151673; avg train loss: 0.8472784298044714\n",
      "batch idx: 52 done; train loss: 0.0014935302315279841; avg train loss: 0.8313202241521517\n",
      "batch idx: 53 done; train loss: 0.00046397410915233195; avg train loss: 0.8159339972995036\n",
      "batch idx: 54 done; train loss: 0.000542493537068367; avg train loss: 0.8011086972310957\n",
      "batch idx: 55 done; train loss: 4.1483970562694594e-05; avg train loss: 0.786803925565729\n",
      "batch idx: 56 done; train loss: 0.0002779574424494058; avg train loss: 0.7730052243705837\n",
      "batch idx: 57 done; train loss: 0.0015704215038567781; avg train loss: 0.7597046243211574\n",
      "batch idx: 58 done; train loss: 0.008931802585721016; avg train loss: 0.7469796612408958\n",
      "batch idx: 59 done; train loss: 0.00037043384509161115; avg train loss: 0.7345361741176324\n",
      "batch idx: 60 done; train loss: 0.0003846143954433501; avg train loss: 0.7225009026467768\n",
      "batch idx: 61 done; train loss: 0.0001854724541772157; avg train loss: 0.7108506537727026\n",
      "batch idx: 62 done; train loss: 0.01134327333420515; avg train loss: 0.6997473620197107\n",
      "batch idx: 63 done; train loss: 5.769562994828448e-05; avg train loss: 0.6888147109823706\n",
      "batch idx: 64 done; train loss: 0.0001714082609396428; avg train loss: 0.6782201986328101\n",
      "batch idx: 65 done; train loss: 0.009146574884653091; avg train loss: 0.6680827194851108\n",
      "batch idx: 66 done; train loss: 0.0005146132898516953; avg train loss: 0.6581190164075695\n",
      "batch idx: 67 done; train loss: 0.00027616979787126184; avg train loss: 0.6484448568986034\n",
      "batch idx: 68 done; train loss: 0.0024216631427407265; avg train loss: 0.6390822019166343\n",
      "batch idx: 69 done; train loss: 0.0748744010925293; avg train loss: 0.63102209047629\n",
      "batch idx: 70 done; train loss: 0.00012194366718176752; avg train loss: 0.6221361729155984\n",
      "batch idx: 71 done; train loss: 0.0009826361201703548; avg train loss: 0.6135090404601063\n",
      "batch idx: 72 done; train loss: 0.00035172473872080445; avg train loss: 0.6051096251762517\n",
      "batch idx: 73 done; train loss: 0.00016735584358684719; avg train loss: 0.5969347296447293\n",
      "batch idx: 74 done; train loss: 0.000498289882671088; avg train loss: 0.5889822437812351\n",
      "batch idx: 75 done; train loss: 0.00021824838768225163; avg train loss: 0.5812353491050042\n",
      "batch idx: 76 done; train loss: 2.3459432125091553; avg train loss: 0.6041536330453178\n",
      "batch idx: 77 done; train loss: 0.00016449528629891574; avg train loss: 0.5964101825612278\n",
      "batch idx: 78 done; train loss: 0.003388260956853628; avg train loss: 0.5889035759586408\n",
      "batch idx: 79 done; train loss: 0.00301276333630085; avg train loss: 0.5815799408008615\n",
      "batch idx: 80 done; train loss: 0.0002786724944598973; avg train loss: 0.5744033819328813\n",
      "batch idx: 81 done; train loss: 0.0026386703830212355; avg train loss: 0.567430641548127\n",
      "batch idx: 82 done; train loss: 7.4377665519714355; avg train loss: 0.6502057729990102\n",
      "batch idx: 83 done; train loss: 0.055450648069381714; avg train loss: 0.6431253548450859\n",
      "batch idx: 84 done; train loss: 0.0001662831346038729; avg train loss: 0.6355611304720215\n",
      "batch idx: 85 done; train loss: 0.00014840454969089478; avg train loss: 0.6281726104031572\n",
      "batch idx: 86 done; train loss: 6.174491882324219; avg train loss: 0.6919234066321349\n",
      "batch idx: 87 done; train loss: 0.00013314791431184858; avg train loss: 0.6840621536921596\n",
      "batch idx: 88 done; train loss: 0.00021002470748499036; avg train loss: 0.676378421905815\n",
      "batch idx: 89 done; train loss: 1.5181974172592163; avg train loss: 0.6857319662986305\n",
      "batch idx: 90 done; train loss: 0.0025137036573141813; avg train loss: 0.6782240733025722\n",
      "batch idx: 91 done; train loss: 1.621364951133728; avg train loss: 0.6884756045833456\n",
      "batch idx: 92 done; train loss: 0.00014351768186315894; avg train loss: 0.6810741842940823\n",
      "batch idx: 93 done; train loss: 0.057678818702697754; avg train loss: 0.6744423187026846\n",
      "batch idx: 94 done; train loss: 0.0005586970364674926; avg train loss: 0.6673488068956718\n",
      "batch idx: 95 done; train loss: 0.00014840454969089478; avg train loss: 0.6603988027045679\n",
      "batch idx: 96 done; train loss: 1.3953453302383423; avg train loss: 0.6679755710296583\n",
      "batch idx: 97 done; train loss: 7.382648944854736; avg train loss: 0.7364926462727713\n",
      "batch idx: 98 done; train loss: 0.0029933906625956297; avg train loss: 0.7290835628827695\n",
      "batch idx: 99 done; train loss: 0.0003545847721397877; avg train loss: 0.7217962731016633\n",
      "batch idx: 100 done; train loss: 0.0002795067266561091; avg train loss: 0.7146525427415147\n",
      "batch idx: 101 done; train loss: 2.59657883644104; avg train loss: 0.7331028005228826\n",
      "batch idx: 102 done; train loss: 2.5126750469207764; avg train loss: 0.7503802009733476\n",
      "batch idx: 103 done; train loss: 0.000558220490347594; avg train loss: 0.7431703742379341\n",
      "batch idx: 104 done; train loss: 1.323162317276001; avg train loss: 0.7486941070287728\n",
      "batch idx: 105 done; train loss: 0.05035348981618881; avg train loss: 0.7421059879984654\n",
      "batch idx: 106 done; train loss: 0.00016437610611319542; avg train loss: 0.7351719542424622\n",
      "batch idx: 107 done; train loss: 0.00028379703871905804; avg train loss: 0.7283674342683534\n",
      "batch idx: 108 done; train loss: 0.000286657246761024; avg train loss: 0.721687794112192\n",
      "batch idx: 109 done; train loss: 0.00011753345461329445; avg train loss: 0.7151280644698503\n",
      "batch idx: 110 done; train loss: 2.622600959512056e-06; avg train loss: 0.708685492921482\n",
      "batch idx: 111 done; train loss: 0.000800408364739269; avg train loss: 0.7023650903807968\n",
      "batch idx: 112 done; train loss: 0.00010895135346800089; avg train loss: 0.6961504342832098\n",
      "batch idx: 113 done; train loss: 0.00038628268521279097; avg train loss: 0.6900472399709466\n",
      "batch idx: 114 done; train loss: 0.000559292733669281; avg train loss: 0.684051692603666\n",
      "batch idx: 115 done; train loss: 2.622600959512056e-06; avg train loss: 0.6781547178622633\n",
      "batch idx: 116 done; train loss: 9.989239333663136e-05; avg train loss: 0.67235937747364\n",
      "batch idx: 117 done; train loss: 0.001891610911116004; avg train loss: 0.666677447248534\n",
      "batch idx: 118 done; train loss: 0.02394763194024563; avg train loss: 0.6612763563635903\n",
      "batch idx: 119 done; train loss: 0.00027926836628466845; avg train loss: 0.6557680472969462\n",
      "batch idx: 120 done; train loss: 0.0001677133986959234; avg train loss: 0.6503498627192746\n",
      "batch idx: 121 done; train loss: 0.0008355463505722582; avg train loss: 0.6450259748801869\n",
      "batch idx: 122 done; train loss: 0.00024971229140646756; avg train loss: 0.6397838914445058\n",
      "batch idx: 123 done; train loss: 0.00034540885826572776; avg train loss: 0.6346271294881651\n",
      "batch idx: 124 done; train loss: 0.06140397489070892; avg train loss: 0.6300413442513855\n",
      "batch idx: 125 done; train loss: 0.000358159770257771; avg train loss: 0.6250438586602654\n",
      "batch idx: 126 done; train loss: 0.000263894529780373; avg train loss: 0.6201243313836474\n",
      "batch idx: 127 done; train loss: 0.04611329734325409; avg train loss: 0.6156398701802068\n",
      "batch idx: 128 done; train loss: 0.0004930472350679338; avg train loss: 0.6108712901573763\n",
      "batch idx: 129 done; train loss: 0.00011669908417388797; avg train loss: 0.6061731779183517\n",
      "batch idx: 130 done; train loss: 9.536288416711614e-05; avg train loss: 0.6015466297119838\n",
      "batch idx: 131 done; train loss: 0.0004976941272616386; avg train loss: 0.5969932286848268\n",
      "batch idx: 132 done; train loss: 0.00036554806865751743; avg train loss: 0.5925073062741789\n",
      "batch idx: 133 done; train loss: 0.0006032554083503783; avg train loss: 0.5880901118647325\n",
      "batch idx: 134 done; train loss: 0.00018225439998786896; avg train loss: 0.5837352388464752\n",
      "batch idx: 135 done; train loss: 0.0014641289599239826; avg train loss: 0.5794538336267211\n",
      "batch idx: 136 done; train loss: 7.903263758635148e-05; avg train loss: 0.5752248204808149\n",
      "batch idx: 137 done; train loss: 8.999896090244874e-05; avg train loss: 0.5710571768466127\n",
      "batch idx: 138 done; train loss: 1.9550132492440753e-05; avg train loss: 0.5669489924817629\n",
      "batch idx: 139 done; train loss: 0.0271789338439703; avg train loss: 0.5630934920629216\n",
      "batch idx: 140 done; train loss: 6.179455280303955; avg train loss: 0.602925845171014\n",
      "batch idx: 141 done; train loss: 5.483612312673358e-06; avg train loss: 0.5986799271318682\n",
      "batch idx: 142 done; train loss: 0.0001433984871255234; avg train loss: 0.5944943570014853\n",
      "batch idx: 143 done; train loss: 2.95634672511369e-05; avg train loss: 0.5903661292686088\n",
      "batch idx: 144 done; train loss: 0.0005233110277913511; avg train loss: 0.5862982477634997\n",
      "batch idx: 145 done; train loss: 0.00013910756388213485; avg train loss: 0.5822834591319954\n",
      "batch idx: 146 done; train loss: 0.0006117141456343234; avg train loss: 0.5783265084858297\n",
      "batch idx: 147 done; train loss: 0.06948505342006683; avg train loss: 0.5748883905461962\n",
      "batch idx: 148 done; train loss: 8.22030258178711; avg train loss: 0.626199895185397\n",
      "batch idx: 149 done; train loss: 0.0006068295333534479; avg train loss: 0.6220292747477166\n",
      "batch idx: 150 done; train loss: 0.010758493095636368; avg train loss: 0.6179811238758486\n",
      "batch idx: 151 done; train loss: 0.00022003613412380219; avg train loss: 0.6139169061933373\n",
      "batch idx: 152 done; train loss: 0.001281393808312714; avg train loss: 0.6099127525176181\n",
      "batch idx: 153 done; train loss: 8.868777513271198e-05; avg train loss: 0.6059528559933163\n",
      "batch idx: 154 done; train loss: 0.0025581750087440014; avg train loss: 0.6020599870837384\n",
      "batch idx: 155 done; train loss: 0.000578474544454366; avg train loss: 0.5982043363623327\n",
      "batch idx: 156 done; train loss: 0.00019071667338721454; avg train loss: 0.5943953324152693\n",
      "batch idx: 157 done; train loss: 0.00017915551143232733; avg train loss: 0.5906344705361312\n",
      "batch idx: 158 done; train loss: 0.0005189026123844087; avg train loss: 0.5869230518699441\n",
      "batch idx: 159 done; train loss: 0.002542122732847929; avg train loss: 0.5832706710628373\n",
      "batch idx: 160 done; train loss: 1.6927575416048057e-05; avg train loss: 0.5796479770039091\n",
      "batch idx: 161 done; train loss: 6.496695277746767e-05; avg train loss: 0.5760703041023589\n",
      "batch idx: 162 done; train loss: 0.002927187131717801; avg train loss: 0.5725540886608212\n",
      "Current learning rate: 5e-06\n",
      "\n",
      "Epoch 8/10\n",
      "Train - Loss: 0.4404, Acc: 0.9080, F1: 0.8966, Recall: 0.8125, AUC: 0.9779\n",
      "Val   - Loss: 1.9362, Acc: 0.6182, F1: 0.1600, Recall: 0.1176, AUC: 0.5542\n",
      "batch idx: 0 done; train loss: 0.010047690942883492; avg train loss: 0.010047690942883492\n",
      "batch idx: 1 done; train loss: 0.0019945267122238874; avg train loss: 0.0060211088275536895\n",
      "batch idx: 2 done; train loss: 0.0023914568591862917; avg train loss: 0.00481122483809789\n",
      "batch idx: 3 done; train loss: 1.5020257706055418e-05; avg train loss: 0.0036121736929999315\n",
      "batch idx: 4 done; train loss: 7.435062408447266; avg train loss: 1.489902220643853\n",
      "batch idx: 5 done; train loss: 0.0004832768754567951; avg train loss: 1.241665730015787\n",
      "batch idx: 6 done; train loss: 0.00026890001026913524; avg train loss: 1.0643233257292846\n",
      "batch idx: 7 done; train loss: 7.456770420074463; avg train loss: 1.8633792125224318\n",
      "batch idx: 8 done; train loss: 3.7431014789035544e-05; avg train loss: 1.6563412367993604\n",
      "batch idx: 9 done; train loss: 7.484043121337891; avg train loss: 2.2391114252532134\n",
      "batch idx: 10 done; train loss: 0.0005428509321063757; avg train loss: 2.035605191224022\n",
      "batch idx: 11 done; train loss: 0.001543165068142116; avg train loss: 1.8661000223776985\n",
      "batch idx: 12 done; train loss: 0.00022349244682118297; avg train loss: 1.7225710585368619\n",
      "batch idx: 13 done; train loss: 0.00028772983932867646; avg train loss: 1.5995508207727522\n",
      "batch idx: 14 done; train loss: 0.0001227780303452164; avg train loss: 1.4929222845899253\n",
      "batch idx: 15 done; train loss: 0.0004976941272616386; avg train loss: 1.3996457476860087\n",
      "batch idx: 16 done; train loss: 0.00016246906307060272; avg train loss: 1.3173232018846595\n",
      "batch idx: 17 done; train loss: 0.0019884591456502676; avg train loss: 1.24424904951027\n",
      "batch idx: 18 done; train loss: 6.186770770000294e-05; avg train loss: 1.1787655136259243\n",
      "batch idx: 19 done; train loss: 0.0003383779258001596; avg train loss: 1.119844156840918\n",
      "batch idx: 20 done; train loss: 0.003147174371406436; avg train loss: 1.0666681100566555\n",
      "batch idx: 21 done; train loss: 0.0003137096355203539; avg train loss: 1.0181974554920585\n",
      "batch idx: 22 done; train loss: 0.0003955773718189448; avg train loss: 0.9739451999216133\n",
      "batch idx: 23 done; train loss: 0.0013073477894067764; avg train loss: 0.933418622749438\n",
      "batch idx: 24 done; train loss: 0.00039319414645433426; avg train loss: 0.8960976056053187\n",
      "batch idx: 25 done; train loss: 0.0004259632551111281; avg train loss: 0.8616486962841569\n",
      "batch idx: 26 done; train loss: 0.0004817279113922268; avg train loss: 0.8297536233814619\n",
      "batch idx: 27 done; train loss: 0.00011646069469861686; avg train loss: 0.8001237247140774\n",
      "batch idx: 28 done; train loss: 0.0005738280597142875; avg train loss: 0.7725530386225476\n",
      "batch idx: 29 done; train loss: 0.0019576449412852526; avg train loss: 0.7468665254998389\n",
      "batch idx: 30 done; train loss: 0.007371725048869848; avg train loss: 0.7230118545175496\n",
      "batch idx: 31 done; train loss: 0.00038342276820912957; avg train loss: 0.7004297160253827\n",
      "batch idx: 32 done; train loss: 0.0002932118659373373; avg train loss: 0.6792134583235814\n",
      "batch idx: 33 done; train loss: 0.0002356490003876388; avg train loss: 0.6592435227552521\n",
      "batch idx: 34 done; train loss: 7.429153919219971; avg train loss: 0.8526695340828155\n",
      "batch idx: 35 done; train loss: 0.001505433232523501; avg train loss: 0.8290260868369741\n",
      "batch idx: 36 done; train loss: 8.193991661071777; avg train loss: 1.0280792104649417\n",
      "batch idx: 37 done; train loss: 0.0007968349382281303; avg train loss: 1.0010454637405546\n",
      "batch idx: 38 done; train loss: 0.033045634627342224; avg train loss: 0.9762249553017542\n",
      "batch idx: 39 done; train loss: 0.00016723664884921163; avg train loss: 0.9518235123354316\n",
      "batch idx: 40 done; train loss: 0.00026341783814132214; avg train loss: 0.9286147295428148\n",
      "batch idx: 41 done; train loss: 0.0001174142598756589; avg train loss: 0.9065076506075067\n",
      "batch idx: 42 done; train loss: 0.0006320862448774278; avg train loss: 0.8854407770176781\n",
      "batch idx: 43 done; train loss: 0.00010156115604331717; avg train loss: 0.865319431202641\n",
      "batch idx: 44 done; train loss: 0.0019301610300317407; avg train loss: 0.846133002976583\n",
      "batch idx: 45 done; train loss: 0.00016878610767889768; avg train loss: 0.8277424765229111\n",
      "batch idx: 46 done; train loss: 0.001320919836871326; avg train loss: 0.8101590391466124\n",
      "batch idx: 47 done; train loss: 0.00042655906872823834; avg train loss: 0.7932896124783232\n",
      "batch idx: 48 done; train loss: 2.622600959512056e-06; avg train loss: 0.7771000820726627\n",
      "batch idx: 49 done; train loss: 0.00011491115583339706; avg train loss: 0.7615603786543261\n",
      "batch idx: 50 done; train loss: 0.0014666287461295724; avg train loss: 0.7466565796365183\n",
      "batch idx: 51 done; train loss: 0.002959160367026925; avg train loss: 0.7323547061890281\n",
      "batch idx: 52 done; train loss: 0.009521075524389744; avg train loss: 0.7187163357991293\n",
      "batch idx: 53 done; train loss: 0.00037722624256275594; avg train loss: 0.7054137596962299\n",
      "batch idx: 54 done; train loss: 0.0005206898204050958; avg train loss: 0.692597522062124\n",
      "batch idx: 55 done; train loss: 0.0014607959892600775; avg train loss: 0.6802557948108229\n",
      "batch idx: 56 done; train loss: 0.0008077934035100043; avg train loss: 0.6683356544352559\n",
      "batch idx: 57 done; train loss: 1.0640435218811035; avg train loss: 0.6751582038739774\n",
      "batch idx: 58 done; train loss: 0.00012396997772157192; avg train loss: 0.663716945672346\n",
      "batch idx: 59 done; train loss: 0.000271879427600652; avg train loss: 0.6526595279016002\n",
      "batch idx: 60 done; train loss: 0.0001599660754436627; avg train loss: 0.6419628137733026\n",
      "batch idx: 61 done; train loss: 0.00030048147891648114; avg train loss: 0.6316134213169415\n",
      "batch idx: 62 done; train loss: 2.1172146797180176; avg train loss: 0.6551943936725142\n",
      "batch idx: 63 done; train loss: 0.0027707540430128574; avg train loss: 0.6450002743033032\n",
      "batch idx: 64 done; train loss: 0.0902126282453537; avg train loss: 0.6364650797485655\n",
      "batch idx: 65 done; train loss: 0.006850328762084246; avg train loss: 0.6269254623093764\n",
      "batch idx: 66 done; train loss: 0.009823078289628029; avg train loss: 0.6177149789657981\n",
      "batch idx: 67 done; train loss: 2.2479257583618164; avg train loss: 0.6416886668980925\n",
      "batch idx: 68 done; train loss: 0.0001292145170737058; avg train loss: 0.6323907038201066\n",
      "batch idx: 69 done; train loss: 0.002515130676329136; avg train loss: 0.6233924813466242\n",
      "batch idx: 70 done; train loss: 0.00010430268594063818; avg train loss: 0.6146137746049244\n",
      "batch idx: 71 done; train loss: 0.00011300401820335537; avg train loss: 0.6060790416801088\n",
      "batch idx: 72 done; train loss: 0.0006333967321552336; avg train loss: 0.5977852657219177\n",
      "batch idx: 73 done; train loss: 0.0006254147156141698; avg train loss: 0.5897155380056163\n",
      "batch idx: 74 done; train loss: 0.0004911408759653568; avg train loss: 0.5818592127105543\n",
      "batch idx: 75 done; train loss: 0.00010322991875000298; avg train loss: 0.5742045287264516\n",
      "batch idx: 76 done; train loss: 0.0024546037893742323; avg train loss: 0.56677920502597\n",
      "batch idx: 77 done; train loss: 0.0001006075763143599; avg train loss: 0.5595140948022566\n",
      "batch idx: 78 done; train loss: 2.3899190425872803; avg train loss: 0.5826837776856112\n",
      "batch idx: 79 done; train loss: 0.0018449680646881461; avg train loss: 0.5754232925653497\n",
      "batch idx: 80 done; train loss: 3.6954811548639555e-06; avg train loss: 0.568319346922335\n",
      "batch idx: 81 done; train loss: 0.0002851079625543207; avg train loss: 0.5613921001057522\n",
      "batch idx: 82 done; train loss: 2.622600959512056e-06; avg train loss: 0.5546283714611162\n",
      "batch idx: 83 done; train loss: 0.08068318665027618; avg train loss: 0.5489861668800348\n",
      "batch idx: 84 done; train loss: 0.006720795761793852; avg train loss: 0.5426065742786437\n",
      "batch idx: 85 done; train loss: 0.0005078217945992947; avg train loss: 0.5363031004125501\n",
      "batch idx: 86 done; train loss: 0.00939448643475771; avg train loss: 0.5302466795622307\n",
      "batch idx: 87 done; train loss: 0.0004543225804809481; avg train loss: 0.5242263118692563\n",
      "batch idx: 88 done; train loss: 0.012690619565546513; avg train loss: 0.5184787198209\n",
      "batch idx: 89 done; train loss: 2.5029828548431396; avg train loss: 0.5405287657655915\n",
      "batch idx: 90 done; train loss: 0.0005752577562816441; avg train loss: 0.5345952107325223\n",
      "batch idx: 91 done; train loss: 1.1181734800338745; avg train loss: 0.5409384527901456\n",
      "batch idx: 92 done; train loss: 0.00011955977242905647; avg train loss: 0.5351231958759766\n",
      "batch idx: 93 done; train loss: 0.06748343259096146; avg train loss: 0.5301483047771999\n",
      "batch idx: 94 done; train loss: 0.0011344670783728361; avg train loss: 0.5245797380645806\n",
      "batch idx: 95 done; train loss: 8.928377064876258e-05; avg train loss: 0.5191162958323522\n",
      "batch idx: 96 done; train loss: 1.1052775382995605; avg train loss: 0.5251591952392306\n",
      "batch idx: 97 done; train loss: 0.00011753345461329445; avg train loss: 0.5198016272618365\n",
      "batch idx: 98 done; train loss: 0.0002954761730507016; avg train loss: 0.5145540903821518\n",
      "batch idx: 99 done; train loss: 1.308833360671997; avg train loss: 0.5224968830850503\n",
      "batch idx: 100 done; train loss: 0.0008200620068237185; avg train loss: 0.5173317660446718\n",
      "batch idx: 101 done; train loss: 0.00033468366018496454; avg train loss: 0.5122631671977651\n",
      "batch idx: 102 done; train loss: 0.012680732645094395; avg train loss: 0.5074128522991954\n",
      "batch idx: 103 done; train loss: 0.0025754161179065704; avg train loss: 0.5025586461820677\n",
      "batch idx: 104 done; train loss: 0.00020394629973452538; avg train loss: 0.4977743157069978\n",
      "batch idx: 105 done; train loss: 0.002148107625544071; avg train loss: 0.4930985967628332\n",
      "batch idx: 106 done; train loss: 0.000569300667848438; avg train loss: 0.4884955192292352\n",
      "batch idx: 107 done; train loss: 0.0008503158460371196; avg train loss: 0.4839802858645759\n",
      "batch idx: 108 done; train loss: 8.4638240878121e-06; avg train loss: 0.4795401774054889\n",
      "batch idx: 109 done; train loss: 3.4927710657939315e-05; avg train loss: 0.47518103877189954\n",
      "batch idx: 110 done; train loss: 0.0006392342620529234; avg train loss: 0.47090588737991895\n",
      "batch idx: 111 done; train loss: 0.029984133318066597; avg train loss: 0.46696908600436665\n",
      "batch idx: 112 done; train loss: 0.00283316383138299; avg train loss: 0.46286168846301284\n",
      "batch idx: 113 done; train loss: 0.00016509123088326305; avg train loss: 0.45880294638202923\n",
      "batch idx: 114 done; train loss: 2.622600959512056e-06; avg train loss: 0.45481337834915037\n",
      "batch idx: 115 done; train loss: 0.0001161031104857102; avg train loss: 0.45089357425226534\n",
      "batch idx: 116 done; train loss: 0.0024148847442120314; avg train loss: 0.4470604230598888\n",
      "batch idx: 117 done; train loss: 1.5003025531768799; avg train loss: 0.4559862038235921\n",
      "batch idx: 118 done; train loss: 3.814689989667386e-06; avg train loss: 0.4521544190409568\n",
      "batch idx: 119 done; train loss: 0.00010358751023886725; avg train loss: 0.4483873287782008\n",
      "batch idx: 120 done; train loss: 0.008296784944832325; avg train loss: 0.44475021684569366\n",
      "batch idx: 121 done; train loss: 0.0008338788175024092; avg train loss: 0.44111155833726584\n",
      "batch idx: 122 done; train loss: 0.002589208772405982; avg train loss: 0.43754633598308\n",
      "batch idx: 123 done; train loss: 2.4671096801757812; avg train loss: 0.4539137823072147\n",
      "batch idx: 124 done; train loss: 0.0005078217945992947; avg train loss: 0.45028653462311374\n",
      "batch idx: 125 done; train loss: 7.489093780517578; avg train loss: 0.5061500841937048\n",
      "batch idx: 126 done; train loss: 0.00302797625772655; avg train loss: 0.5021884927926341\n",
      "batch idx: 127 done; train loss: 1.2687263488769531; avg train loss: 0.5081770697932928\n",
      "batch idx: 128 done; train loss: 6.69933797325939e-05; avg train loss: 0.5042382319916373\n",
      "batch idx: 129 done; train loss: 0.0009787060553207994; avg train loss: 0.5003670048690503\n",
      "batch idx: 130 done; train loss: 0.00010835537250386551; avg train loss: 0.4965482365522827\n",
      "batch idx: 131 done; train loss: 0.000533795915544033; avg train loss: 0.4927905513959438\n",
      "batch idx: 132 done; train loss: 7.986703712958843e-05; avg train loss: 0.48908595978422337\n",
      "batch idx: 133 done; train loss: 0.008288745768368244; avg train loss: 0.4854979208736573\n",
      "batch idx: 134 done; train loss: 8.725739462533966e-05; avg train loss: 0.48190228632936816\n",
      "batch idx: 135 done; train loss: 0.004837357439100742; avg train loss: 0.4783944559698809\n",
      "batch idx: 136 done; train loss: 0.0013760393485426903; avg train loss: 0.4749125697171704\n",
      "batch idx: 137 done; train loss: 0.00013529339048545808; avg train loss: 0.47147215467132486\n",
      "batch idx: 138 done; train loss: 2.90866428258596e-05; avg train loss: 0.4680804779229184\n",
      "batch idx: 139 done; train loss: 0.0019005347276106477; avg train loss: 0.46475062118580907\n",
      "batch idx: 140 done; train loss: 0.027312804013490677; avg train loss: 0.4616482253193387\n",
      "batch idx: 141 done; train loss: 7.045020902296528e-05; avg train loss: 0.4583976776072942\n",
      "batch idx: 142 done; train loss: 3.45700973412022e-05; avg train loss: 0.4551923411911407\n",
      "batch idx: 143 done; train loss: 6.16293036728166e-05; avg train loss: 0.45203171124747776\n",
      "batch idx: 144 done; train loss: 0.0003163314249832183; avg train loss: 0.4489164327659433\n",
      "batch idx: 145 done; train loss: 7.152531907195225e-06; avg train loss: 0.4458417116684499\n",
      "batch idx: 146 done; train loss: 1.6093124941107817e-05; avg train loss: 0.44280888433141924\n",
      "batch idx: 147 done; train loss: 0.0024096521083265543; avg train loss: 0.4398332138434254\n",
      "batch idx: 148 done; train loss: 0.001978108659386635; avg train loss: 0.4368945889764184\n",
      "batch idx: 149 done; train loss: 0.00032240914879366755; avg train loss: 0.4339841077775676\n",
      "batch idx: 150 done; train loss: 0.004611215554177761; avg train loss: 0.43114057868999545\n",
      "batch idx: 151 done; train loss: 0.0006136203301139176; avg train loss: 0.4283081644902594\n",
      "batch idx: 152 done; train loss: 0.00013374387344811112; avg train loss: 0.4255096388653129\n",
      "batch idx: 153 done; train loss: 6.250936031341553; avg train loss: 0.4633370829723015\n",
      "batch idx: 154 done; train loss: 6.460934673668817e-05; avg train loss: 0.46034822830374944\n",
      "batch idx: 155 done; train loss: 0.0005840741214342415; avg train loss: 0.45740102218719614\n",
      "batch idx: 156 done; train loss: 0.00026782741770148277; avg train loss: 0.4544893457873904\n",
      "batch idx: 157 done; train loss: 0.016391268000006676; avg train loss: 0.4517165731431665\n",
      "batch idx: 158 done; train loss: 0.0002735478919930756; avg train loss: 0.4488773088334107\n",
      "batch idx: 159 done; train loss: 0.0008249455713666975; avg train loss: 0.4460769815630229\n",
      "batch idx: 160 done; train loss: 0.00029404606902971864; avg train loss: 0.4433081434543646\n",
      "batch idx: 161 done; train loss: 0.0002131234941771254; avg train loss: 0.4405729890101659\n",
      "batch idx: 162 done; train loss: 0.00013851160474587232; avg train loss: 0.43787093700154367\n",
      "Current learning rate: 5e-06\n",
      "\n",
      "Epoch 9/10\n",
      "Train - Loss: 0.5002, Acc: 0.8896, F1: 0.8929, Recall: 0.8152, AUC: 0.9754\n",
      "Val   - Loss: 1.9432, Acc: 0.6182, F1: 0.1600, Recall: 0.1176, AUC: 0.5526\n",
      "batch idx: 0 done; train loss: 0.00011014331539627165; avg train loss: 0.00011014331539627165\n",
      "batch idx: 1 done; train loss: 0.002092554699629545; avg train loss: 0.0011013490075129084\n",
      "batch idx: 2 done; train loss: 0.001881259260699153; avg train loss: 0.0013613190919083233\n",
      "batch idx: 3 done; train loss: 0.0005725175142288208; avg train loss: 0.0011641186974884477\n",
      "batch idx: 4 done; train loss: 0.0003364712174516171; avg train loss: 0.0009985892014810816\n",
      "batch idx: 5 done; train loss: 0.00023529145983047783; avg train loss: 0.0008713729112059809\n",
      "batch idx: 6 done; train loss: 0.00048268112004734576; avg train loss: 0.0008158455124690331\n",
      "batch idx: 7 done; train loss: 0.0005696581210941076; avg train loss: 0.0007850720885471674\n",
      "batch idx: 8 done; train loss: 0.0003093002596870065; avg train loss: 0.0007322085520071494\n",
      "batch idx: 9 done; train loss: 6.285468578338623; avg train loss: 0.6292058455306687\n",
      "batch idx: 10 done; train loss: 0.00026544384309090674; avg train loss: 0.5720294453772525\n",
      "batch idx: 11 done; train loss: 0.00031251792097464204; avg train loss: 0.5243863680892294\n",
      "batch idx: 12 done; train loss: 0.001141730579547584; avg train loss: 0.48413678058848464\n",
      "batch idx: 13 done; train loss: 0.0006957017467357218; avg train loss: 0.44960527495693114\n",
      "batch idx: 14 done; train loss: 0.0005079409456811845; avg train loss: 0.4196654526895145\n",
      "batch idx: 15 done; train loss: 0.00020895205670967698; avg train loss: 0.3934494213999642\n",
      "batch idx: 16 done; train loss: 0.00035339308669790626; avg train loss: 0.37032612561683087\n",
      "batch idx: 17 done; train loss: 1.556875467300415; avg train loss: 0.4362455334881411\n",
      "batch idx: 18 done; train loss: 0.0004189328756183386; avg train loss: 0.4133072913506399\n",
      "batch idx: 19 done; train loss: 0.00016675988445058465; avg train loss: 0.39265026477733045\n",
      "batch idx: 20 done; train loss: 0.00029881304362788796; avg train loss: 0.3739668623138208\n",
      "batch idx: 21 done; train loss: 0.00011789103882620111; avg train loss: 0.3569737272558665\n",
      "batch idx: 22 done; train loss: 0.00027783826226368546; avg train loss: 0.3414652103431012\n",
      "batch idx: 23 done; train loss: 5.721882189391181e-05; avg train loss: 0.3272398773630509\n",
      "batch idx: 24 done; train loss: 0.0024044194724410772; avg train loss: 0.3142464590474265\n",
      "batch idx: 25 done; train loss: 0.0003302744007669389; avg train loss: 0.30217275963793955\n",
      "batch idx: 26 done; train loss: 0.06883583962917328; avg train loss: 0.2935306514894667\n",
      "batch idx: 27 done; train loss: 0.0024503227323293686; avg train loss: 0.28313492546242613\n",
      "batch idx: 28 done; train loss: 0.07858211547136307; avg train loss: 0.2760813802903205\n",
      "batch idx: 29 done; train loss: 2.50339189733495e-06; avg train loss: 0.26687875106037307\n",
      "batch idx: 30 done; train loss: 0.00030357998912222683; avg train loss: 0.2582795519935585\n",
      "batch idx: 31 done; train loss: 0.0014563917648047209; avg train loss: 0.25025382823640996\n",
      "batch idx: 32 done; train loss: 1.847726889536716e-05; avg train loss: 0.24267093881315194\n",
      "batch idx: 33 done; train loss: 0.0009915679693222046; avg train loss: 0.23556272202362755\n",
      "batch idx: 34 done; train loss: 0.0022553978487849236; avg train loss: 0.2288967984757749\n",
      "batch idx: 35 done; train loss: 0.00011407678539399058; avg train loss: 0.22254172287326432\n",
      "batch idx: 36 done; train loss: 7.4866108894348145; avg train loss: 0.41886791656411704\n",
      "batch idx: 37 done; train loss: 0.0002798642381094396; avg train loss: 0.4078524415029063\n",
      "batch idx: 38 done; train loss: 2.4737186431884766; avg train loss: 0.46082336975125426\n",
      "batch idx: 39 done; train loss: 0.0002330270071979612; avg train loss: 0.44930861118265286\n",
      "batch idx: 40 done; train loss: 0.0004848258395213634; avg train loss: 0.4383616895889179\n",
      "batch idx: 41 done; train loss: 6.687417771900073e-05; avg train loss: 0.4279260987457941\n",
      "batch idx: 42 done; train loss: 2.288792165927589e-05; avg train loss: 0.41797486128476774\n",
      "batch idx: 43 done; train loss: 6.251231670379639; avg train loss: 0.5505488796732876\n",
      "batch idx: 44 done; train loss: 0.0005317704635672271; avg train loss: 0.5383262772464049\n",
      "batch idx: 45 done; train loss: 9.643566590966657e-05; avg train loss: 0.526625628516394\n",
      "batch idx: 46 done; train loss: 2.270498514175415; avg train loss: 0.5637293069346712\n",
      "batch idx: 47 done; train loss: 0.9831851124763489; avg train loss: 0.5724679695501228\n",
      "batch idx: 48 done; train loss: 0.0005043664714321494; avg train loss: 0.5607952429566802\n",
      "batch idx: 49 done; train loss: 8.511180931236595e-05; avg train loss: 0.5495810403337328\n",
      "batch idx: 50 done; train loss: 1.5025914907455444; avg train loss: 0.5682675197535721\n",
      "batch idx: 51 done; train loss: 0.00017832119192462415; avg train loss: 0.5573427274735405\n",
      "batch idx: 52 done; train loss: 1.645074735279195e-05; avg train loss: 0.5468271373466314\n",
      "batch idx: 53 done; train loss: 0.0006840273272246122; avg train loss: 0.5367133760499756\n",
      "batch idx: 54 done; train loss: 2.4496889114379883; avg train loss: 0.5714947494206668\n",
      "batch idx: 55 done; train loss: 0.0031726048327982426; avg train loss: 0.5613461396958834\n",
      "batch idx: 56 done; train loss: 0.00030596344731748104; avg train loss: 0.5515033295862595\n",
      "batch idx: 57 done; train loss: 0.00012766500003635883; avg train loss: 0.5419968526106349\n",
      "batch idx: 58 done; train loss: 0.00042417587246745825; avg train loss: 0.5328176546998185\n",
      "batch idx: 59 done; train loss: 6.29437255859375; avg train loss: 0.6288435697647173\n",
      "batch idx: 60 done; train loss: 0.0004798214649781585; avg train loss: 0.6185425247106233\n",
      "batch idx: 61 done; train loss: 8.22540732769994e-06; avg train loss: 0.608566165044441\n",
      "batch idx: 62 done; train loss: 0.0008100565755739808; avg train loss: 0.5989192426877924\n",
      "batch idx: 63 done; train loss: 6.313929557800293; avg train loss: 0.6882162788614252\n",
      "batch idx: 64 done; train loss: 0.000687958556227386; avg train loss: 0.6776389200874992\n",
      "batch idx: 65 done; train loss: 1.3269145488739014; avg train loss: 0.6874764296145658\n",
      "batch idx: 66 done; train loss: 0.00046659549116156995; avg train loss: 0.677222551493321\n",
      "batch idx: 67 done; train loss: 0.0004677870310842991; avg train loss: 0.6672702755453469\n",
      "batch idx: 68 done; train loss: 0.0018130784155800939; avg train loss: 0.6576259683405676\n",
      "batch idx: 69 done; train loss: 9.77468371274881e-05; avg train loss: 0.6482327080333756\n",
      "batch idx: 70 done; train loss: 2.3830039501190186; avg train loss: 0.6726661058092298\n",
      "batch idx: 71 done; train loss: 7.795983401592821e-05; avg train loss: 0.6633246037817963\n",
      "batch idx: 72 done; train loss: 3.6954811548639555e-06; avg train loss: 0.6542380159968559\n",
      "batch idx: 73 done; train loss: 0.10817132145166397; avg train loss: 0.6468587363408399\n",
      "batch idx: 74 done; train loss: 0.00013004888023715466; avg train loss: 0.6382356871746985\n",
      "batch idx: 75 done; train loss: 0.001057304092682898; avg train loss: 0.6298517610815141\n",
      "batch idx: 76 done; train loss: 2.1735363006591797; avg train loss: 0.6498996122448604\n",
      "batch idx: 77 done; train loss: 0.0003082277253270149; avg train loss: 0.6415715175715331\n",
      "batch idx: 78 done; train loss: 0.00021479207498487085; avg train loss: 0.6334530780082855\n",
      "batch idx: 79 done; train loss: 0.0403134822845459; avg train loss: 0.6260388330617388\n",
      "batch idx: 80 done; train loss: 1.6212332411669195e-05; avg train loss: 0.6183101587317471\n",
      "batch idx: 81 done; train loss: 0.00024685196694917977; avg train loss: 0.6107728013321765\n",
      "batch idx: 82 done; train loss: 8.427741704508662e-05; avg train loss: 0.603415108272958\n",
      "batch idx: 83 done; train loss: 0.00246649538166821; avg train loss: 0.5962609581194902\n",
      "batch idx: 84 done; train loss: 6.266520023345947; avg train loss: 0.662969888298625\n",
      "batch idx: 85 done; train loss: 1.122288465499878; avg train loss: 0.6683108019870118\n",
      "batch idx: 86 done; train loss: 0.0028956886380910873; avg train loss: 0.6606623524082885\n",
      "batch idx: 87 done; train loss: 0.0001501924270996824; avg train loss: 0.6531565324085022\n",
      "batch idx: 88 done; train loss: 0.0001961992384167388; avg train loss: 0.645819899451535\n",
      "batch idx: 89 done; train loss: 0.0002648479712661356; avg train loss: 0.6386470655461987\n",
      "batch idx: 90 done; train loss: 0.00029273517429828644; avg train loss: 0.6316321827948591\n",
      "batch idx: 91 done; train loss: 0.0006860524881631136; avg train loss: 0.6247740726828298\n",
      "batch idx: 92 done; train loss: 0.0004850641416851431; avg train loss: 0.618061287644753\n",
      "batch idx: 93 done; train loss: 0.0002469711471349001; avg train loss: 0.6114887949160549\n",
      "batch idx: 94 done; train loss: 0.0004138090298511088; avg train loss: 0.6050564266435685\n",
      "batch idx: 95 done; train loss: 0.0004670721245929599; avg train loss: 0.5987586208673292\n",
      "batch idx: 96 done; train loss: 0.00029869386344216764; avg train loss: 0.592588930898217\n",
      "batch idx: 97 done; train loss: 0.0005025792634114623; avg train loss: 0.5865472334325557\n",
      "batch idx: 98 done; train loss: 0.00019488819816615433; avg train loss: 0.5806244824705922\n",
      "batch idx: 99 done; train loss: 0.0682026818394661; avg train loss: 0.5755002644642809\n",
      "batch idx: 100 done; train loss: 0.0005227153305895627; avg train loss: 0.5698074174431552\n",
      "batch idx: 101 done; train loss: 0.00017486473370809108; avg train loss: 0.5642227845734548\n",
      "batch idx: 102 done; train loss: 0.0005530973430722952; avg train loss: 0.5587502633382083\n",
      "batch idx: 103 done; train loss: 1.6212332411669195e-05; avg train loss: 0.5533778205400757\n",
      "batch idx: 104 done; train loss: 0.0005303407087922096; avg train loss: 0.5481126064464444\n",
      "batch idx: 105 done; train loss: 0.0003923600015696138; avg train loss: 0.5429454343101721\n",
      "batch idx: 106 done; train loss: 0.00010668662434909493; avg train loss: 0.5378721749860055\n",
      "batch idx: 107 done; train loss: 0.0003761537664104253; avg train loss: 0.5328953599747129\n",
      "batch idx: 108 done; train loss: 7.211902266135439e-05; avg train loss: 0.5280070733604739\n",
      "batch idx: 109 done; train loss: 3.802703940891661e-05; avg train loss: 0.5232073547575551\n",
      "batch idx: 110 done; train loss: 0.00027211778797209263; avg train loss: 0.5184962264965679\n",
      "batch idx: 111 done; train loss: 0.0002053765201708302; avg train loss: 0.5138686296217786\n",
      "batch idx: 112 done; train loss: 2.8013790142722428e-05; avg train loss: 0.509321367534773\n",
      "batch idx: 113 done; train loss: 1.883488948806189e-05; avg train loss: 0.5048538014589372\n",
      "batch idx: 114 done; train loss: 0.9867321848869324; avg train loss: 0.5090440482713545\n",
      "batch idx: 115 done; train loss: 0.00011550712952157483; avg train loss: 0.5046567332615112\n",
      "batch idx: 116 done; train loss: 0.0010451575508341193; avg train loss: 0.5003523608195396\n",
      "batch idx: 117 done; train loss: 9.417090768693015e-05; avg train loss: 0.4961128846338459\n",
      "batch idx: 118 done; train loss: 0.00010930894495686516; avg train loss: 0.4919447873591493\n",
      "batch idx: 119 done; train loss: 0.00105015910230577; avg train loss: 0.4878539987903423\n",
      "batch idx: 120 done; train loss: 8.284702198579907e-05; avg train loss: 0.48382283224680217\n",
      "batch idx: 121 done; train loss: 0.0002796259068418294; avg train loss: 0.4798593633423763\n",
      "batch idx: 122 done; train loss: 9.572047565598041e-05; avg train loss: 0.4759588459206956\n",
      "batch idx: 123 done; train loss: 0.0005857420619577169; avg train loss: 0.4721251918573187\n",
      "batch idx: 124 done; train loss: 0.001484721782617271; avg train loss: 0.4683600680967211\n",
      "batch idx: 125 done; train loss: 0.0001294529065489769; avg train loss: 0.4646439521031483\n",
      "batch idx: 126 done; train loss: 2.436887502670288; avg train loss: 0.4801734288792675\n",
      "batch idx: 127 done; train loss: 0.0006918897270224988; avg train loss: 0.4764274793546406\n",
      "batch idx: 128 done; train loss: 0.0002783149539027363; avg train loss: 0.47273640056083643\n",
      "batch idx: 129 done; train loss: 0.002209009835496545; avg train loss: 0.4691169590937184\n",
      "batch idx: 130 done; train loss: 0.0007982643437571824; avg train loss: 0.4655420072253981\n",
      "batch idx: 131 done; train loss: 0.0005575056420639157; avg train loss: 0.4620193973649183\n",
      "batch idx: 132 done; train loss: 0.000501030299346894; avg train loss: 0.45854933445465085\n",
      "batch idx: 133 done; train loss: 0.00010847456724150106; avg train loss: 0.4551281340077299\n",
      "batch idx: 134 done; train loss: 0.0003899767471011728; avg train loss: 0.4517597032132067\n",
      "batch idx: 135 done; train loss: 0.000276765669696033; avg train loss: 0.44843997573126915\n",
      "batch idx: 136 done; train loss: 0.00010358751023886725; avg train loss: 0.4451674473500937\n",
      "batch idx: 137 done; train loss: 0.0001311216183239594; avg train loss: 0.44194254643899394\n",
      "batch idx: 138 done; train loss: 0.000502817565575242; avg train loss: 0.4387667210514154\n",
      "batch idx: 139 done; train loss: 0.00010907054820563644; avg train loss: 0.4356334521192496\n",
      "batch idx: 140 done; train loss: 5.6622808187967166e-05; avg train loss: 0.43254425474824915\n",
      "batch idx: 141 done; train loss: 0.0004876854654867202; avg train loss: 0.4295016028518917\n",
      "batch idx: 142 done; train loss: 0.013673452660441399; avg train loss: 0.4265937136897137\n",
      "batch idx: 143 done; train loss: 0.0004804172203876078; avg train loss: 0.4236345935753434\n",
      "batch idx: 144 done; train loss: 0.001829261309467256; avg train loss: 0.42072559128385456\n",
      "batch idx: 145 done; train loss: 0.00019870213873218745; avg train loss: 0.41784527012532635\n",
      "batch idx: 146 done; train loss: 0.0006540066679008305; avg train loss: 0.41500723431949355\n",
      "batch idx: 147 done; train loss: 0.00011157367407577112; avg train loss: 0.41220388526107854\n",
      "batch idx: 148 done; train loss: 0.0029924397822469473; avg train loss: 0.40945749972095213\n",
      "batch idx: 149 done; train loss: 0.014278742484748363; avg train loss: 0.4068229746727108\n",
      "batch idx: 150 done; train loss: 3.886147169396281e-05; avg train loss: 0.4041290401482007\n",
      "batch idx: 151 done; train loss: 0.0002101439022226259; avg train loss: 0.40147167898868774\n",
      "batch idx: 152 done; train loss: 1.0240296125411987; avg train loss: 0.40554068509033814\n",
      "batch idx: 153 done; train loss: 0.00021514961554203182; avg train loss: 0.40290870109374854\n",
      "batch idx: 154 done; train loss: 0.0014806747203692794; avg train loss: 0.4003188428590816\n",
      "batch idx: 155 done; train loss: 1.2755313036905136e-05; avg train loss: 0.3977527781953249\n",
      "batch idx: 156 done; train loss: 0.0002574589161667973; avg train loss: 0.39522096087507547\n",
      "batch idx: 157 done; train loss: 6.425174069590867e-05; avg train loss: 0.39271996904511103\n",
      "batch idx: 158 done; train loss: 5.125986263010418e-06; avg train loss: 0.39025006437178494\n",
      "batch idx: 159 done; train loss: 0.0017088347813114524; avg train loss: 0.3878216816868445\n",
      "batch idx: 160 done; train loss: 0.07743747532367706; avg train loss: 0.38589382947340867\n",
      "batch idx: 161 done; train loss: 0.0060349623672664165; avg train loss: 0.3835490216517658\n",
      "batch idx: 162 done; train loss: 0.0002862997353076935; avg train loss: 0.38119771660933355\n",
      "Current learning rate: 5.000000000000001e-07\n",
      "\n",
      "Epoch 10/10\n",
      "Train - Loss: 0.1781, Acc: 0.9387, F1: 0.9398, Recall: 0.9070, AUC: 0.9934\n",
      "Val   - Loss: 1.9386, Acc: 0.6364, F1: 0.1667, Recall: 0.1176, AUC: 0.5526\n",
      "\n",
      "Test Results:\n",
      "Loss: 2.0871, Acc: 0.6250, F1: 0.0000, Recall: 0.0000, AUC: 0.5374\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(HDF5Dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev_normalized.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(HDF5Dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_normalized.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 76\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, test_loader, optimizer, scheduler, criterion, num_epochs, device)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     74\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m \u001b[43mplot_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mplot_metrics\u001b[0;34m(train_metrics, val_metrics, test_metrics)\u001b[0m\n\u001b[1;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m12\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metrics, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, i)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PTSDTransformer().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=2\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = create_balanced_dataloader(\"train_normalized.h5\", batch_size=1, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(HDF5Dataset(\"dev_normalized.h5\"), batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "test_loader = DataLoader(HDF5Dataset(\"test_normalized.h5\"), batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    num_epochs=10,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dda1d3-d8cb-4a89-90fa-3f738409edd4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-14T22:03:09.502576Z",
     "iopub.status.idle": "2025-05-14T22:03:09.502972Z",
     "shell.execute_reply": "2025-05-14T22:03:09.502841Z",
     "shell.execute_reply.started": "2025-05-14T22:03:09.502827Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4a6f2-dc91-4a24-a744-026e0fd5118a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-14T22:03:09.504173Z",
     "iopub.status.idle": "2025-05-14T22:03:09.504453Z",
     "shell.execute_reply": "2025-05-14T22:03:09.504347Z",
     "shell.execute_reply.started": "2025-05-14T22:03:09.504333Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898792ce-6671-46f3-858c-5db02beb2a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
