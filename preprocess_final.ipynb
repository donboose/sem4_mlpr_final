{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25878c23-deda-4250-9bdd-52893e6e0d24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:51:44.931003Z",
     "iopub.status.busy": "2025-05-14T12:51:44.930819Z",
     "iopub.status.idle": "2025-05-14T12:51:44.933460Z",
     "shell.execute_reply": "2025-05-14T12:51:44.933072Z",
     "shell.execute_reply.started": "2025-05-14T12:51:44.930988Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "import joblib\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fa1bb21-5ed4-4b9b-84fe-cce6f6309561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:51:45.176379Z",
     "iopub.status.busy": "2025-05-14T12:51:45.176106Z",
     "iopub.status.idle": "2025-05-14T12:51:45.181864Z",
     "shell.execute_reply": "2025-05-14T12:51:45.181418Z",
     "shell.execute_reply.started": "2025-05-14T12:51:45.176361Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_scalers(split=\"train\"):\n",
    "    full_path = \"../dataset/edaicwoz_labels/\" + split + \"_split.csv\"\n",
    "    split_file_df = pd.read_csv(full_path)\n",
    "    \n",
    "    scalers = {\n",
    "        \"audio_egemaps\": StandardScaler(),\n",
    "        \"audio_mfcc\": StandardScaler(),\n",
    "        \"audio_vgg16\": StandardScaler(),\n",
    "        \"audio_densenet\": StandardScaler(),\n",
    "        \"visual_of\": StandardScaler(),\n",
    "        \"visual_resnet\": StandardScaler(),\n",
    "        \"visual_vgg\": StandardScaler()\n",
    "    }\n",
    "    \n",
    "    for idx, row in enumerate(split_file_df.iterrows()):\n",
    "        p_id = row[1][\"Participant_ID\"]\n",
    "        features_dir = f\"../dataset/edaicwoz_participant/{p_id}_P/{p_id}_P/features\"\n",
    "        \n",
    "        # Load each feature and update scaler\n",
    "        egemaps = pd.read_csv(os.path.join(features_dir, f\"{p_id}_OpenSMILE2.3.0_egemaps.csv\"), sep=\";\").iloc[:, 1:].values\n",
    "        scalers[\"audio_egemaps\"].partial_fit(egemaps)\n",
    "        \n",
    "        mfcc = pd.read_csv(os.path.join(features_dir, f\"{p_id}_OpenSMILE2.3.0_mfcc.csv\"), sep=\";\").iloc[:, 1:].values\n",
    "        scalers[\"audio_mfcc\"].partial_fit(mfcc)\n",
    "        \n",
    "        vgg16 = pd.read_csv(os.path.join(features_dir, f\"{p_id}_vgg16.csv\")).iloc[:, 2:].values\n",
    "        scalers[\"audio_vgg16\"].partial_fit(vgg16)\n",
    "        \n",
    "        densenet = pd.read_csv(os.path.join(features_dir, f\"{p_id}_densenet201.csv\")).iloc[:, 2:].values\n",
    "        scalers[\"audio_densenet\"].partial_fit(densenet)\n",
    "        \n",
    "        openface_pg = pd.read_csv(os.path.join(features_dir, f\"{p_id}_OpenFace2.1.0_Pose_gaze_AUs.csv\")).select_dtypes(include=np.number).values\n",
    "        scalers[\"visual_of\"].partial_fit(openface_pg)\n",
    "        \n",
    "        resnet = loadmat(os.path.join(features_dir, f\"{p_id}_CNN_ResNet.mat\"))[\"feature\"]\n",
    "        scalers[\"visual_resnet\"].partial_fit(resnet)\n",
    "        \n",
    "        vgg = loadmat(os.path.join(features_dir, f\"{p_id}_CNN_VGG.mat\"))[\"feature\"]\n",
    "        scalers[\"visual_vgg\"].partial_fit(vgg)\n",
    "        \n",
    "        del egemaps, mfcc, vgg16, densenet, openface_pg, resnet, vgg\n",
    "        gc.collect()\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Fitting scalers: processed {idx+1} participants\")\n",
    "    \n",
    "    # Save the scalers for future use\n",
    "    with open(\"../storage/transform/feature_scalers.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scalers, f)\n",
    "        print(\"Dumped scalers\")\n",
    "    \n",
    "    return scalers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3303b0e7-805d-4559-8a09-a6ad1983d593",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:51:45.392152Z",
     "iopub.status.busy": "2025-05-14T12:51:45.391978Z",
     "iopub.status.idle": "2025-05-14T12:51:45.398515Z",
     "shell.execute_reply": "2025-05-14T12:51:45.398007Z",
     "shell.execute_reply.started": "2025-05-14T12:51:45.392138Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_normalized(split=\"train\", scalers=None):\n",
    "    # If scalers not provided, try to load them\n",
    "    if scalers is None:\n",
    "        try:\n",
    "            with open(\"../storage/transform/feature_scalers.pkl\", \"rb\") as f:\n",
    "                scalers = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise ValueError(\"No scalers provided and couldn't find saved scalers.\")\n",
    "    \n",
    "    full_path = \"../dataset/edaicwoz_labels/\" + split + \"_split.csv\"\n",
    "    split_file_df = pd.read_csv(full_path)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(\"../storage/transform\", exist_ok=True)\n",
    "    h5_filename = f\"../storage/transform/edaicwoz_{split}_normalized.h5\"\n",
    "    \n",
    "    # Remove file if it exists to avoid append issues\n",
    "    if os.path.exists(h5_filename):\n",
    "        os.remove(h5_filename)\n",
    "    \n",
    "    for idx, row in enumerate(split_file_df.iterrows()):\n",
    "        p_id = row[1][\"Participant_ID\"]\n",
    "        features_dir = f\"../dataset/edaicwoz_participant/{p_id}_P/{p_id}_P/features\"\n",
    "        \n",
    "        # Load features\n",
    "        egemaps = pd.read_csv(os.path.join(features_dir, f\"{p_id}_OpenSMILE2.3.0_egemaps.csv\"), sep=\";\").iloc[:, 1:].values\n",
    "        mfcc = pd.read_csv(os.path.join(features_dir, f\"{p_id}_OpenSMILE2.3.0_mfcc.csv\"), sep=\";\").iloc[:, 1:].values\n",
    "        vgg16 = pd.read_csv(os.path.join(features_dir, f\"{p_id}_vgg16.csv\")).iloc[:, 2:].values\n",
    "        densenet = pd.read_csv(os.path.join(features_dir, f\"{p_id}_densenet201.csv\")).iloc[:, 2:].values\n",
    "        openface_pg = pd.read_csv(os.path.join(features_dir, f\"{p_id}_OpenFace2.1.0_Pose_gaze_AUs.csv\")).select_dtypes(include=np.number).values\n",
    "        resnet = loadmat(os.path.join(features_dir, f\"{p_id}_CNN_ResNet.mat\"))[\"feature\"]\n",
    "        vgg = loadmat(os.path.join(features_dir, f\"{p_id}_CNN_VGG.mat\"))[\"feature\"]\n",
    "        \n",
    "        ptsd_binary = row[1][\"PCL-C (PTSD)\"]\n",
    "        \n",
    "        # Normalize using scalers\n",
    "        egemaps_norm = scalers[\"audio_egemaps\"].transform(egemaps)\n",
    "        mfcc_norm = scalers[\"audio_mfcc\"].transform(mfcc)\n",
    "        vgg16_norm = scalers[\"audio_vgg16\"].transform(vgg16)\n",
    "        densenet_norm = scalers[\"audio_densenet\"].transform(densenet)\n",
    "        openface_pg_norm = scalers[\"visual_of\"].transform(openface_pg)\n",
    "        resnet_norm = scalers[\"visual_resnet\"].transform(resnet)\n",
    "        vgg_norm = scalers[\"visual_vgg\"].transform(vgg)\n",
    "        \n",
    "        features = {\n",
    "            \"info\": np.array([p_id, ptsd_binary]),\n",
    "            \"audio_egemaps\": egemaps_norm,\n",
    "            \"audio_mfcc\": mfcc_norm,\n",
    "            \"audio_vgg16\": vgg16_norm,\n",
    "            \"audio_densenet\": densenet_norm,\n",
    "            \"visual_of\": openface_pg_norm,\n",
    "            \"visual_resnet\": resnet_norm,\n",
    "            \"visual_vgg\": vgg_norm\n",
    "        }\n",
    "        \n",
    "        # Store normalized features\n",
    "        with h5py.File(h5_filename, 'a') as f:\n",
    "            grp = f.create_group(str(p_id))\n",
    "            for key, data in features.items():\n",
    "                grp.create_dataset(key, data=data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        del egemaps, mfcc, vgg16, densenet, openface_pg, resnet, vgg\n",
    "        del egemaps_norm, mfcc_norm, vgg16_norm, densenet_norm, openface_pg_norm, resnet_norm, vgg_norm\n",
    "        del features\n",
    "        gc.collect()\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Stored normalized data for {idx+1} participants\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1360ea9-2600-4aa3-9d8a-0f15439e533f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:51:45.641632Z",
     "iopub.status.busy": "2025-05-14T12:51:45.641305Z",
     "iopub.status.idle": "2025-05-14T13:17:26.542502Z",
     "shell.execute_reply": "2025-05-14T13:17:26.541298Z",
     "shell.execute_reply.started": "2025-05-14T12:51:45.641614Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting scalers on training data...\n",
      "Fitting scalers: processed 1 participants\n",
      "Fitting scalers: processed 11 participants\n",
      "Fitting scalers: processed 21 participants\n",
      "Fitting scalers: processed 31 participants\n",
      "Fitting scalers: processed 41 participants\n",
      "Fitting scalers: processed 51 participants\n",
      "Fitting scalers: processed 61 participants\n",
      "Fitting scalers: processed 71 participants\n",
      "Fitting scalers: processed 81 participants\n",
      "Fitting scalers: processed 91 participants\n",
      "Fitting scalers: processed 101 participants\n",
      "Fitting scalers: processed 111 participants\n",
      "Fitting scalers: processed 121 participants\n",
      "Fitting scalers: processed 131 participants\n",
      "Fitting scalers: processed 141 participants\n",
      "Fitting scalers: processed 151 participants\n",
      "Fitting scalers: processed 161 participants\n",
      "Dumped scalers\n",
      "Normalizing training data...\n",
      "Stored normalized data for 1 participants\n",
      "Stored normalized data for 11 participants\n",
      "Stored normalized data for 21 participants\n",
      "Stored normalized data for 31 participants\n",
      "Stored normalized data for 41 participants\n",
      "Stored normalized data for 51 participants\n",
      "Stored normalized data for 61 participants\n",
      "Stored normalized data for 71 participants\n",
      "Stored normalized data for 81 participants\n",
      "Stored normalized data for 91 participants\n",
      "Stored normalized data for 101 participants\n",
      "Stored normalized data for 111 participants\n",
      "Stored normalized data for 121 participants\n",
      "Stored normalized data for 131 participants\n",
      "Stored normalized data for 141 participants\n",
      "Stored normalized data for 151 participants\n",
      "Stored normalized data for 161 participants\n",
      "Normalizing validation data...\n",
      "Stored normalized data for 1 participants\n",
      "Stored normalized data for 11 participants\n",
      "Stored normalized data for 21 participants\n",
      "Stored normalized data for 31 participants\n",
      "Stored normalized data for 41 participants\n",
      "Stored normalized data for 51 participants\n",
      "Normalizing test data...\n",
      "Stored normalized data for 1 participants\n",
      "Stored normalized data for 11 participants\n",
      "Stored normalized data for 21 participants\n",
      "Stored normalized data for 31 participants\n",
      "Stored normalized data for 41 participants\n",
      "Stored normalized data for 51 participants\n",
      "All data normalized and saved!\n"
     ]
    }
   ],
   "source": [
    "# First fit scalers using training data only\n",
    "print(\"Fitting scalers on training data...\")\n",
    "scalers = fit_scalers(split=\"train\")\n",
    "\n",
    "# Now apply normalization to all splits\n",
    "print(\"Normalizing training data...\")\n",
    "get_data_normalized(split=\"train\", scalers=scalers)\n",
    "\n",
    "print(\"Normalizing validation data...\")\n",
    "get_data_normalized(split=\"dev\", scalers=scalers)\n",
    "\n",
    "print(\"Normalizing test data...\")\n",
    "get_data_normalized(split=\"test\", scalers=scalers)\n",
    "\n",
    "print(\"All data normalized and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f645ad7-cb2c-493e-8b8d-5b5a82a6b366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
